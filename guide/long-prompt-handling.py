# ë‹¤ì–‘í•œ ê¸´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ë°©ì‹ êµ¬í˜„

class PromptProcessor:
    """ê¸´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡œì„¸ì„œ"""
    
    @staticmethod
    def method_1_truncate(prompt: str, tokenizer, max_tokens: int = 77):
        """ë°©ë²• 1: ë‹¨ìˆœ ìë¥´ê¸° (ê¸°ë³¸)"""
        tokens = tokenizer.encode(prompt)
        if len(tokens) > max_tokens:
            print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ {max_tokens} í† í°ì„ ì´ˆê³¼. ìë™ìœ¼ë¡œ ì˜ë¦½ë‹ˆë‹¤.")
            tokens = tokens[:max_tokens]
        return tokenizer.decode(tokens)
    
    @staticmethod
    def method_2_emphasis_parsing(prompt: str, tokenizer, max_tokens: int = 77):
        """ë°©ë²• 2: ê°•ì¡° êµ¬ë¬¸ íŒŒì‹± (A1111 ìŠ¤íƒ€ì¼)"""
        # (word:1.5) í˜•íƒœì˜ ê°•ì¡° êµ¬ë¬¸ ì²˜ë¦¬
        import re
        
        # ê°•ì¡° êµ¬ë¬¸ ì¶”ì¶œ
        emphasis_pattern = r'\(([^:]+):([0-9.]+)\)'
        emphasized_words = re.findall(emphasis_pattern, prompt)
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        emphasized_words.sort(key=lambda x: float(x[1]), reverse=True)
        
        # ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ìš°ì„  ë°°ì¹˜
        important_parts = [word for word, weight in emphasized_words[:20]]
        base_prompt = re.sub(emphasis_pattern, r'\1', prompt)
        
        # ì¬êµ¬ì„±
        reconstructed = ' '.join(important_parts) + ' ' + base_prompt
        return PromptProcessor.method_1_truncate(reconstructed, tokenizer, max_tokens)
    
    @staticmethod
    def method_3_semantic_chunking(prompt: str, tokenizer, max_tokens: int = 77):
        """ë°©ë²• 3: ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = prompt.split('. ')
        
        current_tokens = 0
        selected_sentences = []
        
        for sentence in sentences:
            sentence_tokens = len(tokenizer.encode(sentence))
            if current_tokens + sentence_tokens <= max_tokens:
                selected_sentences.append(sentence)
                current_tokens += sentence_tokens
            else:
                break
        
        return '. '.join(selected_sentences)
    
    @staticmethod
    def method_4_keyword_extraction(prompt: str, tokenizer, max_tokens: int = 77):
        """ë°©ë²• 4: í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ì‹"""
        # ì¤‘ìš” í‚¤ì›Œë“œ íŒ¨í„´
        important_patterns = [
            r'\b(?:masterpiece|best quality|high quality|detailed|realistic|8k|4k)\b',
            r'\b(?:beautiful|gorgeous|stunning|amazing|perfect)\b',
            r'\b\w+(?:_\w+)*\b',  # ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ì—°ê²°ëœ ë‹¨ì–´ (ë³´í†µ ì¤‘ìš”)
        ]
        
        keywords = []
        for pattern in important_patterns:
            keywords.extend(re.findall(pattern, prompt, re.IGNORECASE))
        
        # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œ ìœ ì§€
        seen = set()
        unique_keywords = []
        for k in keywords:
            if k.lower() not in seen:
                seen.add(k.lower())
                unique_keywords.append(k)
        
        # ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ì¶”ê°€
        remaining_text = prompt
        for keyword in unique_keywords:
            remaining_text = remaining_text.replace(keyword, '', 1)
        
        # ì¬êµ¬ì„±
        result = ' '.join(unique_keywords[:30]) + ' ' + remaining_text.strip()
        return PromptProcessor.method_1_truncate(result, tokenizer, max_tokens)
    
    @staticmethod
    def method_5_prompt_weighting(prompt: str, model_type: str = 'SD15'):
        """ë°©ë²• 5: í”„ë¡¬í”„íŠ¸ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ (ComfyUI ìŠ¤íƒ€ì¼)"""
        if model_type == 'SDXL':
            # SDXL: ë‘ ê°œì˜ ì¸ì½”ë” í™œìš©
            return {
                'clip_l': prompt[:500],  # ì£¼ìš” ë‚´ìš©
                'clip_g': prompt[500:1000]  # ì¶”ê°€ ë””í…Œì¼
            }
        else:
            # SD1.5: ë‹¨ì¼ ì¸ì½”ë”
            return {'clip': prompt[:350]}  # ì•½ 77 í† í°

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ
class EnhancedPromptEncoder:
    """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”"""
    
    def __init__(self, pipeline, method='semantic'):
        self.pipeline = pipeline
        self.method = method
        self.tokenizer = pipeline.tokenizer
        
    def encode_long_prompt(self, prompt: str, negative_prompt: str = ""):
        """ê¸´ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©"""
        
        # ì„ íƒí•œ ë°©ë²•ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
        if self.method == 'truncate':
            processed_prompt = PromptProcessor.method_1_truncate(
                prompt, self.tokenizer
            )
        elif self.method == 'emphasis':
            processed_prompt = PromptProcessor.method_2_emphasis_parsing(
                prompt, self.tokenizer
            )
        elif self.method == 'semantic':
            processed_prompt = PromptProcessor.method_3_semantic_chunking(
                prompt, self.tokenizer
            )
        elif self.method == 'keyword':
            processed_prompt = PromptProcessor.method_4_keyword_extraction(
                prompt, self.tokenizer
            )
        else:
            processed_prompt = prompt
        
        print(f"ğŸ“ ì›ë³¸ ê¸¸ì´: {len(prompt)} â†’ ì²˜ë¦¬ í›„: {len(processed_prompt)}")
        
        # í† í°í™” ë° ì¸ì½”ë”©
        text_inputs = self.tokenizer(
            processed_prompt,
            padding="max_length",
            max_length=self.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        )
        
        # ë„¤ê±°í‹°ë¸Œë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        if negative_prompt:
            negative_processed = PromptProcessor.method_1_truncate(
                negative_prompt, self.tokenizer
            )
            uncond_inputs = self.tokenizer(
                negative_processed,
                padding="max_length",
                max_length=self.tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt",
            )
        
        return text_inputs, uncond_inputs

# UIì— ì˜µì…˜ ì¶”ê°€
def create_prompt_options_ui():
    """í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì˜µì…˜ UI"""
    with ui.card().classes('w-full bg-gray-800 p-3'):
        ui.label('ğŸ“ ê¸´ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬').classes('text-lg font-bold mb-2')
        
        prompt_method = ui.select(
            label='ì²˜ë¦¬ ë°©ì‹',
            options=[
                {'label': 'ë‹¨ìˆœ ìë¥´ê¸° (ê¸°ë³¸)', 'value': 'truncate'},
                {'label': 'ê°•ì¡° êµ¬ë¬¸ ìš°ì„ ', 'value': 'emphasis'},
                {'label': 'ì˜ë¯¸ ë‹¨ìœ„ ìœ ì§€', 'value': 'semantic'},
                {'label': 'í‚¤ì›Œë“œ ì¶”ì¶œ', 'value': 'keyword'},
            ],
            value='semantic'
        ).classes('w-full')
        
        # í† í° ì¹´ìš´í„°
        with ui.row().classes('w-full items-center gap-2 mt-2'):
            ui.label('í˜„ì¬ í† í° ìˆ˜:').classes('text-sm')
            token_count_label = ui.label('0 / 77').classes(
                'text-sm font-bold px-2 py-1 bg-blue-600 rounded'
            )
        
        # í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°
        with ui.expansion('ì²˜ë¦¬ëœ í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°', icon='visibility').classes('w-full mt-2'):
            preview_area = ui.textarea(
                label='ì²˜ë¦¬ ê²°ê³¼',
                value='',
                readonly=True
            ).classes('w-full text-xs font-mono')
        
        return prompt_method, token_count_label, preview_area