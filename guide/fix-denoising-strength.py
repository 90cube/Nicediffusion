# img2img.pyì˜ generate ë©”ì„œë“œ ìˆ˜ì •

async def generate(self, params: Img2ImgParams) -> List[Any]:
    """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰ (ì˜¬ë°”ë¥¸ denoising êµ¬í˜„)"""
    print(f"ğŸ¨ Img2Img ìƒì„± ì‹œì‘ - Seed: {params.seed}, Strength: {params.strength}")
    print(f"ğŸ”§ Steps: {params.steps}, Denoising Steps: {int(params.steps * params.strength)}")
    
    if params.init_image is None:
        print("âŒ ì´ˆê¸° ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return []
    
    # íŒŒë¼ë¯¸í„° ê²€ì¦
    init_image = self._validate_init_image(params.init_image, params.width, params.height)
    strength = self._validate_strength(params.strength)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    from ..scheduler_manager import SchedulerManager
    SchedulerManager.apply_scheduler_to_pipeline(
        self.pipeline, 
        params.sampler, 
        params.scheduler
    )
    
    # ìƒì„±ê¸° ì„¤ì •
    generator = torch.Generator(device=self.device)
    if params.seed > 0:
        generator.manual_seed(params.seed)
    
    def _generate():
        """ì‹¤ì œ ìƒì„± ë¡œì§ (ì˜¬ë°”ë¥¸ A1111 ìŠ¤íƒ€ì¼)"""
        
        # ë°©ë²• 1: StableDiffusionImg2ImgPipelineì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        if hasattr(self.pipeline, '__class__') and 'Img2Img' in self.pipeline.__class__.__name__:
            # ì „ìš© img2img íŒŒì´í”„ë¼ì¸
            print("âœ… ì „ìš© Img2Img íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
            result = self.pipeline(
                prompt=params.prompt,
                negative_prompt=params.negative_prompt,
                image=init_image,
                strength=strength,  # íŒŒì´í”„ë¼ì¸ì´ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬
                num_inference_steps=params.steps,
                guidance_scale=params.cfg_scale,
                generator=generator,
                num_images_per_prompt=params.batch_size,
            )
            return result.images
        
        # ë°©ë²• 2: ì¼ë°˜ StableDiffusionPipelineì„ img2imgë¡œ ì‚¬ìš©
        print("âš ï¸ ì¼ë°˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ img2img êµ¬í˜„")
        
        # 1. ì´ë¯¸ì§€ë¥¼ latentë¡œ ì¸ì½”ë”©
        init_latent = self._encode_image(init_image)
        
        # 2. ìŠ¤ì¼€ì¤„ëŸ¬ì˜ timesteps ì„¤ì •
        self.pipeline.scheduler.set_timesteps(params.steps, device=self.device)
        timesteps = self.pipeline.scheduler.timesteps
        
        # 3. strengthì— ë”°ë¥¸ ì‹œì‘ timestep ê³„ì‚° (ì¤‘ìš”!)
        # strength = 1.0: ëª¨ë“  ìŠ¤í… ì‹¤í–‰ (ì™„ì „ ì¬ìƒì„±)
        # strength = 0.0: ìŠ¤í… ì‹¤í–‰ ì•ˆí•¨ (ì›ë³¸ ìœ ì§€)
        init_timestep = int(params.steps * (1 - strength))
        init_timestep = min(init_timestep, params.steps - 1)
        
        # ì‹¤ì œ ì‚¬ìš©í•  timesteps
        timesteps = timesteps[init_timestep:]
        num_inference_steps = len(timesteps)
        
        print(f"ğŸ“Š Denoising ì •ë³´:")
        print(f"  - ì „ì²´ ìŠ¤í…: {params.steps}")
        print(f"  - ì‹œì‘ ìŠ¤í…: {init_timestep}")
        print(f"  - ì‹¤í–‰ ìŠ¤í…: {num_inference_steps}")
        print(f"  - Strength: {strength}")
        
        # 4. ì´ˆê¸° ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = torch.randn_like(init_latent, generator=generator)
        
        # ì²« ë²ˆì§¸ timestepì—ì„œ ë…¸ì´ì¦ˆ ì¶”ê°€
        if len(timesteps) > 0:
            init_latent = self.pipeline.scheduler.add_noise(
                init_latent, 
                noise, 
                timesteps[0]
            )
        
        # 5. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        text_embeddings = self._get_text_embeddings(params.prompt, params.negative_prompt)
        
        # 6. Diffusion ë£¨í”„ ì‹¤í–‰
        latents = init_latent
        
        for i, t in enumerate(timesteps):
            # CFGë¥¼ ìœ„í•œ latent ë³µì œ
            latent_model_input = torch.cat([latents] * 2)
            
            # ë…¸ì´ì¦ˆ ì˜ˆì¸¡
            with torch.no_grad():
                noise_pred = self.pipeline.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=text_embeddings,
                ).sample
            
            # CFG ì ìš©
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + params.cfg_scale * (noise_pred_text - noise_pred_uncond)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
            latents = self.pipeline.scheduler.step(noise_pred, t, latents).prev_sample
            
            # ì§„í–‰ë¥  í‘œì‹œ (ì„ íƒì‚¬í•­)
            if i % 5 == 0:
                print(f"  ìŠ¤í… {i+1}/{num_inference_steps} ì™„ë£Œ...")
        
        # 7. latentë¥¼ ì´ë¯¸ì§€ë¡œ ë””ì½”ë”©
        with torch.no_grad():
            latents = 1 / self.pipeline.vae.config.scaling_factor * latents
            image = self.pipeline.vae.decode(latents).sample
            
        # 8. í›„ì²˜ë¦¬
        image = (image / 2 + 0.5).clamp(0, 1)
        image = image.cpu().permute(0, 2, 3, 1).float().numpy()
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        if image.shape[0] == 1:
            image = image[0]
            image = Image.fromarray((image * 255).astype('uint8'))
            return [image]
        else:
            images = []
            for i in range(image.shape[0]):
                img = Image.fromarray((image[i] * 255).astype('uint8'))
                images.append(img)
            return images
    
    def _get_text_embeddings(self, prompt: str, negative_prompt: str):
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        # í”„ë¡¬í”„íŠ¸ í† í°í™”
        text_input = self.pipeline.tokenizer(
            [prompt],
            padding="max_length",
            max_length=self.pipeline.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        )
        
        # ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ í† í°í™”
        uncond_input = self.pipeline.tokenizer(
            [negative_prompt],
            padding="max_length",
            max_length=self.pipeline.tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        )
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            text_embeddings = self.pipeline.text_encoder(
                text_input.input_ids.to(self.device)
            )[0]
            uncond_embeddings = self.pipeline.text_encoder(
                uncond_input.input_ids.to(self.device)
            )[0]
        
        # CFGë¥¼ ìœ„í•´ concat
        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
        return text_embeddings
    
    # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ìˆ˜í–‰
    generated_images = await asyncio.to_thread(_generate)
    
    print(f"âœ… Img2Img ì™„ë£Œ: {len(generated_images)}ê°œ ì´ë¯¸ì§€")
    return generated_images