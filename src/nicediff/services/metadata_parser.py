# íŒŒì¼ ê²½ë¡œ: src/nicediff/services/metadata_parser.py (ìˆ˜ì • ì œì•ˆ)

from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from PIL import Image
import json
import re
import struct # structëŠ” ë‚´ì¥ ëª¨ë“ˆì´ë¯€ë¡œ pip ì„¤ì¹˜ ë¶ˆí•„ìš” (ì˜¤ë¥˜ê°€ ë‚¬ë˜ ë¶€ë¶„)
from ..core.logger import (
    debug, info, warning, error, success, failure, warning_emoji, 
    info_emoji, debug_emoji, process_emoji, model_emoji, image_emoji, ui_emoji
)

class MetadataParser:
    @staticmethod
    def normalize_sampler_name(name: str) -> str:
        """ìƒ˜í”ŒëŸ¬ ì´ë¦„ì„ ë‚´ë¶€ í‘œì¤€ ë¬¸ìì—´ë¡œ ì •ê·œí™”"""
        if not isinstance(name, str):
            name = str(name)
        name = name.lower().replace('-', '_').replace(' ', '_')
        mapping = {
            'euler_a': ['euler_a', 'euler a', 'eulera', 'euler-a', 'euler_a'],
            'euler': ['euler', 'euler_'],
            'dpmpp_2m': ['dpmpp_2m', 'dpm++_2m', 'dpm++ 2m', 'dpmpp-2m'],
            'dpmpp_sde_gpu': ['dpmpp_sde_gpu', 'dpm++_sde_gpu', 'dpm++ sde gpu', 'dpmpp-sde-gpu'],
            'dpmpp_2m_sde_gpu': ['dpmpp_2m_sde_gpu', 'dpm++_2m_sde_gpu', 'dpm++ 2m sde gpu'],
            'ddim': ['ddim'],
            'ddpm': ['ddpm'],
            'heun': ['heun'],
            'dpm_fast': ['dpm_fast', 'dpm fast', 'dpm-fast'],
            'dpm_adaptive': ['dpm_adaptive', 'dpm adaptive', 'dpm-adaptive'],
            'lms': ['lms'],
            'dpm_solver': ['dpm_solver', 'dpm solver', 'dpm-solver'],
            'dpm_solver_stochastic': ['dpm_solver_stochastic', 'dpm solver stochastic'],
            'k_dpm_2': ['k_dpm_2', 'k dpm 2', 'k-dpm-2'],
            'k_dpm_2_a': ['k_dpm_2_a', 'k dpm 2 a', 'k-dpm-2-a'],
            'k_euler': ['k_euler', 'k euler', 'k-euler'],
            'k_euler_a': ['k_euler_a', 'k euler a', 'k-euler-a'],
            'k_heun': ['k_heun', 'k heun', 'k-heun'],
            'k_lms': ['k_lms', 'k lms', 'k-lms'],
        }
        
        for standard, variants in mapping.items():
            if name in variants:
                return standard
        
        # ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° ì›ë³¸ ë°˜í™˜ (ê²½ê³  ì—†ì´)
        return name

    @staticmethod
    def normalize_scheduler_name(name: str) -> str:
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì´ë¦„ì„ ë‚´ë¶€ í‘œì¤€ ë¬¸ìì—´ë¡œ ì •ê·œí™”"""
        if not isinstance(name, str):
            name = str(name)
        name = name.lower().replace('-', '_').replace(' ', '_')
        mapping = {
            'karras': ['karras'],
            'exponential': ['exponential'],
            'sgm_uniform': ['sgm_uniform', 'sgm uniform', 'sgm-uniform'],
            'simple_linear': ['simple_linear', 'simple linear', 'simple-linear'],
            'scaled_linear': ['scaled_linear', 'scaled linear', 'scaled-linear'],
            'cosine': ['cosine'],
            'cosine_beta': ['cosine_beta', 'cosine beta', 'cosine-beta'],
            'linear': ['linear'],
            'linear_beta': ['linear_beta', 'linear beta', 'linear-beta'],
        }
        
        for standard, variants in mapping.items():
            if name in variants:
                return standard
        
        return name

    @staticmethod
    def extract_sampler_from_value(value) -> str:
        """ê°’ì—ì„œ ìƒ˜í”ŒëŸ¬ ì´ë¦„ì„ ì¶”ì¶œí•˜ê³  ì •ê·œí™”"""
        if isinstance(value, dict):
            # dict í˜•íƒœ: {'value': 1, 'label': 'euler_a'} ë˜ëŠ” {'label': 'Euler a'}
            if 'label' in value:
                return MetadataParser.normalize_sampler_name(value['label'])
            elif 'value' in value:
                return MetadataParser.normalize_sampler_name(str(value['value']))
        elif isinstance(value, str):
            return MetadataParser.normalize_sampler_name(value)
        else:
            return MetadataParser.normalize_sampler_name(str(value))
        
        # ê¸°ë³¸ê°’ ë°˜í™˜ (ëª¨ë“  ê²½ë¡œì—ì„œ ë°˜í™˜ ë³´ì¥)
        return "euler_a"

    @staticmethod
    def extract_scheduler_from_value(value) -> str:
        """ê°’ì—ì„œ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ë¦„ì„ ì¶”ì¶œí•˜ê³  ì •ê·œí™”"""
        if isinstance(value, dict):
            if 'label' in value:
                return MetadataParser.normalize_scheduler_name(value['label'])
            elif 'value' in value:
                return MetadataParser.normalize_scheduler_name(str(value['value']))
        elif isinstance(value, str):
            return MetadataParser.normalize_scheduler_name(value)
        else:
            return MetadataParser.normalize_scheduler_name(str(value))
        
        # ê¸°ë³¸ê°’ ë°˜í™˜ (ëª¨ë“  ê²½ë¡œì—ì„œ ë°˜í™˜ ë³´ì¥)
        return "karras"

    @staticmethod
    def extract_from_png(image_path: Path) -> Dict[str, Any]:
        try:
            with Image.open(image_path) as img:
                raw_metadata = ""
                if 'parameters' in img.info:
                    raw_metadata = img.info['parameters']
                else:
                    for value in img.info.values():
                        if isinstance(value, str) and 'Steps:' in value:
                            raw_metadata = value
                            break
                if raw_metadata:
                    return MetadataParser._parse_automatic1111_format(raw_metadata)
        except Exception as e:
            info(f"PNG ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return {}

    @staticmethod
    def _parse_automatic1111_format(text: str) -> Dict[str, Any]:
        """[ìˆ˜ì •] AUTOMATIC1111 í˜•ì‹ íŒŒì‹± ë¡œì§ ê°œì„  (JSON í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬ ì§€ì›)"""
        result = {'prompt': '', 'negative_prompt': '', 'parameters': {}}
        
        # íŒŒë¼ë¯¸í„° ë¸”ë¡ì˜ ì‹œì‘ì ì„ ì°¾ìŒ (Steps: ê°€ ê°€ì¥ ì¼ë°˜ì )
        param_match = re.search(r'Steps:\s*\d+', text)
        param_start_index = param_match.start() if param_match else -1
        
        # íŒŒë¼ë¯¸í„° ë¸”ë¡ê³¼ í”„ë¡¬í”„íŠ¸ ë¸”ë¡ì„ ë¶„ë¦¬
        param_text = text[param_start_index:] if param_start_index != -1 else ""
        prompt_block = text[:param_start_index].strip() if param_start_index != -1 else text.strip()
        
        # í”„ë¡¬í”„íŠ¸ ë¸”ë¡ì´ JSON í˜•ì‹ì¸ì§€ í™•ì¸
        prompt_is_json = False
        prompt_json = None
        try:
            prompt_json = json.loads(prompt_block)
            if isinstance(prompt_json, dict) and 'prompt' in prompt_json:
                prompt_is_json = True
        except Exception:
            pass
        
        if prompt_is_json and prompt_json is not None:
            # JSON ë‚´ë¶€ì—ì„œ ê°’ ì¶”ì¶œ
            result['prompt'] = prompt_json.get('prompt', '')
            # negativeprompt ë˜ëŠ” negative_prompt ëª¨ë‘ ì§€ì›
            result['negative_prompt'] = prompt_json.get('negativeprompt', prompt_json.get('negative_prompt', ''))
            # íŒŒë¼ë¯¸í„°ë„ ê°™ì´ ì¶”ì¶œ
            # í•´ìƒë„, cfg, steps ë“±ë„ parametersì— ë„£ì–´ì¤Œ
            param_keys = [
                ('resolution', 'resolution'),
                ('guidancescale', 'cfg_scale'),
                ('guidance_scale', 'cfg_scale'),
                ('numinferencesteps', 'steps'),
                ('num_inference_steps', 'steps'),
                ('seed', 'seed'),
                ('sampler', 'sampler'),
                ('Model', 'model'),
                ('stylepreset', 'style_preset'),
                ('style_preset', 'style_preset'),
            ]
            for src_key, dst_key in param_keys:
                if src_key in prompt_json:
                    result['parameters'][dst_key] = prompt_json[src_key]
            # í•´ìƒë„ ë¶„ë¦¬
            if 'resolution' in prompt_json and 'x' in str(prompt_json['resolution']):
                w, h = str(prompt_json['resolution']).split('x')
                result['parameters']['width'] = int(w.strip())
                result['parameters']['height'] = int(h.strip())
            # upscaler ë“± ê¸°íƒ€ ì •ë³´ë„ í•„ìš”ì‹œ ì¶”ê°€ ê°€ëŠ¥
        else:
            # í”„ë¡¬í”„íŠ¸ ë¸”ë¡ì—ì„œ ê¸ì •/ë¶€ì • í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬
            neg_prompt_match = re.search(r'Negative prompt:\s*', prompt_block, re.IGNORECASE)
            if neg_prompt_match:
                result['prompt'] = prompt_block[:neg_prompt_match.start()].strip()
                result['negative_prompt'] = prompt_block[neg_prompt_match.end():].strip()
            else:
                result['prompt'] = prompt_block

        # íŒŒë¼ë¯¸í„° íŒŒì‹± (ê¸°ì¡´ ë¡œì§ê³¼ ìœ ì‚¬)
        if param_text:
            patterns = {
                'steps': r'Steps:\s*(\d+)',
                'sampler': r'Sampler:\s*([^,]+)',
                'scheduler': r'Scheduler:\s*([^,]+)',
                'cfg_scale': r'CFG scale:\s*([\d\.]+)',
                'seed': r'Seed:\s*(\d+)',
                'size': r'Size:\s*(\d+x\d+)',
                'model': r'Model:\s*([^,]+)',
            }
            for key, pattern in patterns.items():
                match = re.search(pattern, param_text, re.IGNORECASE)
                if match:
                    value_str = match.group(1).strip()
                    try:
                        if key == 'size':
                            w, h = value_str.split('x')
                            result['parameters']['width'] = int(w)
                            result['parameters']['height'] = int(h)
                        elif key in ['steps', 'seed']:
                            result['parameters'][key] = int(value_str)
                        elif key == 'cfg_scale':
                            result['parameters'][key] = float(value_str)
                        elif key == 'sampler':
                            result['parameters'][key] = MetadataParser.extract_sampler_from_value(value_str)
                        elif key == 'scheduler':
                            result['parameters'][key] = MetadataParser.extract_scheduler_from_value(value_str)
                        else:
                            result['parameters'][key] = value_str
                    except (ValueError, IndexError):
                            info(f"ê²½ê³ : íŒŒë¼ë¯¸í„° '{key}'ì˜ ê°’ '{value_str}'ì„ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            result['parameters'][key] = value_str
        return result
    
    @staticmethod
    def extract_from_safetensors(model_path: Path) -> Dict[str, Any]:
        """Safetensors íŒŒì¼ì—ì„œ __metadata__ ë¸”ë¡ ì§ì ‘ ì¶”ì¶œ ë° ì›Œí¬í”Œë¡œìš° í”„ë¡¬í”„íŠ¸ íŒŒì‹±"""
        try:
            with open(model_path, 'rb') as f:
                header_size_bytes = f.read(8)
                header_size = struct.unpack('<Q', header_size_bytes)[0]
                
                if header_size > 0:
                    json_data = f.read(header_size)
                    header_dict = json.loads(json_data)
                    
                    if '__metadata__' in header_dict:
                        metadata = header_dict['__metadata__']
                        
                        # ìƒì„¸ ë””ë²„ê¹…
                        #info(r"ğŸ“‹ Safetensors ë©”íƒ€ë°ì´í„° ë°œê²¬:")
                        #for key, value in list(metadata.items())[:10]:
                        #    info(f"   - {key}: {str(value)[:100]}...")
                        
                        # 'prompt' í‚¤ê°€ ComfyUI ì›Œí¬í”Œë¡œìš° JSONì¸ì§€ í™•ì¸í•˜ê³  íŒŒì‹±
                        if 'prompt' in metadata and isinstance(metadata['prompt'], str):
                            try:
                                # JSON ë¬¸ìì—´ì¼ ê°€ëŠ¥ì„± í™•ì¸
                                prompt_content = json.loads(metadata['prompt'])
                                # ComfyUI ì›Œí¬í”Œë¡œìš° JSONìœ¼ë¡œ ê°„ì£¼í•˜ê³  íŒŒì‹±
                                parsed_prompts = MetadataParser._parse_comfyui_workflow_json(prompt_content)
                                if parsed_prompts['prompt']:
                                    metadata['prompt'] = parsed_prompts['prompt']
                                if parsed_prompts['negative_prompt']:
                                    metadata['negative_prompt'] = parsed_prompts['negative_prompt']
                                # 'parameters'ë„ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•˜ë©´ ì¶”ê°€
                                if parsed_prompts['parameters']:
                                    metadata['parameters'] = {**metadata.get('parameters', {}), **parsed_prompts['parameters']}
                                
                            except json.JSONDecodeError:
                                # JSONì´ ì•„ë‹ˆë©´ ì¼ë°˜ ë¬¸ìì—´ í”„ë¡¬í”„íŠ¸ë¡œ ê°„ì£¼
                                pass
                            except Exception as e:
                                info(f"ê²½ê³ : ComfyUI ì›Œí¬í”Œë¡œìš° í”„ë¡¬í”„íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
                                pass # íŒŒì‹± ì‹¤íŒ¨í•´ë„ ì›ë˜ ë©”íƒ€ë°ì´í„° ì‚¬ìš©

                        return metadata
                    else:
                        info(r"ì—†ìŒ")
                        
        except Exception as e:
            info(f"Safetensors ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜ ({model_path.name}): {e}")
        return {}
    
    @staticmethod
    def _parse_comfyui_workflow_json(workflow_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ComfyUI ì›Œí¬í”Œë¡œìš° JSON ë°ì´í„°ì—ì„œ ê¸ì •/ë¶€ì • í”„ë¡¬í”„íŠ¸ ë° íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ê°€ì¥ ì¼ë°˜ì ì¸ "CLIPTextEncode" ë…¸ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.
        """
        extracted = {'prompt': '', 'negative_prompt': '', 'parameters': {}}
        nodes = workflow_data.get('nodes', [])

        # ê¸ì • ë° ë¶€ì • í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ ì‹œë„
        # CLIPTextEncode ë…¸ë“œë¥¼ ì°¾ì•„ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
        for node in nodes:
            if node.get('class_type') == 'CLIPTextEncode':
                node_id = str(node.get('id'))
                if 'inputs' in node and 'text' in node['inputs']:
                    # ê¸ì • í”„ë¡¬í”„íŠ¸ (ëŒ€ë¶€ë¶„ì˜ ê²½ìš° CLIPTextEncode ë…¸ë“œëŠ” ê¸ì • í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©ë¨)
                    if not extracted['prompt']: # ì²« ë²ˆì§¸ ê¸ì • í”„ë¡¬í”„íŠ¸ë§Œ ê°€ì ¸ì˜´
                        extracted['prompt'] = node['inputs']['text']
                        #info(f"ğŸ” ComfyUI ì›Œí¬í”Œë¡œìš°ì—ì„œ ê¸ì • í”„ë¡¬í”„íŠ¸ ì¶”ì¶œë¨ (Node {node_id}): {extracted['prompt'][:50]}...")
                
                # ë¶€ì • í”„ë¡¬í”„íŠ¸ëŠ” ë³´í†µ ë³„ë„ì˜ CLIPTextEncode ë…¸ë“œë‚˜ ë‹¤ë¥¸ íŠ¹ì • ë…¸ë“œì— ì—°ê²°ë  ìˆ˜ ìˆìŒ
                # ëª…ì‹œì ì¸ 'negative' í‚¤ê°€ ì—†ìœ¼ë¯€ë¡œ heuristicì´ í•„ìš”í•¨
                # ì˜ˆë¥¼ ë“¤ì–´, promptê°€ ë¹„ì–´ìˆê³  text_g/text_lì´ ìˆëŠ” ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ (ê³ ê¸‰ ì¼€ì´ìŠ¤)
                
                # ì›Œí¬í”Œë¡œìš° ë©”íƒ€ë°ì´í„°ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì‹œë„ (ì˜ˆ: KSampler ë…¸ë“œ)
                if node.get('class_type') == 'KSampler':
                    if 'inputs' in node:
                        if 'steps' in node['inputs']:
                            extracted['parameters']['steps'] = node['inputs']['steps']
                        if 'cfg' in node['inputs']: # ComfyUIì—ì„œëŠ” 'cfg'ë¡œ í‘œê¸°ë  ìˆ˜ ìˆìŒ
                            extracted['parameters']['cfg_scale'] = node['inputs']['cfg']
                        if 'sampler_name' in node['inputs']:
                            extracted['parameters']['sampler'] = MetadataParser.extract_sampler_from_value(node['inputs']['sampler_name'])
                        if 'scheduler' in node['inputs']:
                            extracted['parameters']['scheduler'] = MetadataParser.extract_scheduler_from_value(node['inputs']['scheduler'])
                        if 'seed' in node['inputs']:
                            extracted['parameters']['seed'] = node['inputs']['seed']
                
                # í¬ê¸° ì •ë³´ (EmptyLatentImage ë…¸ë“œ)
                if node.get('class_type') == 'EmptyLatentImage':
                    if 'inputs' in node:
                        if 'width' in node['inputs']:
                            extracted['parameters']['width'] = node['inputs']['width']
                        if 'height' in node['inputs']:
                            extracted['parameters']['height'] = node['inputs']['height']
            
            # TODO: LoRA, ControlNet ë“± ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ ê°€ëŠ¥
            # (ì´ ë¶€ë¶„ì€ í•„ìš”ì— ë”°ë¼ í™•ì¥)

        # 'prompt' í‚¤ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆë‹¤ë©´, 'workflow' ìµœìƒìœ„ í‚¤ì˜ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ë„ ìˆìŒ (ëœ ì¼ë°˜ì )
        if not extracted['prompt'] and 'prompt' in workflow_data and isinstance(workflow_data['prompt'], str):
            extracted['prompt'] = workflow_data['prompt']

        # 'workflow' í‚¤ì— ì§ì ‘ 'extra_pnginfo'ê°€ ìˆëŠ” ê²½ìš°
        if 'extra_pnginfo' in workflow_data and 'prompt' in workflow_data['extra_pnginfo']:
            extracted['prompt'] = workflow_data['extra_pnginfo']['prompt']
        if 'extra_pnginfo' in workflow_data and 'negative_prompt' in workflow_data['extra_pnginfo']:
            extracted['negative_prompt'] = workflow_data['extra_pnginfo']['negative_prompt']

        return extracted
    
    @staticmethod
    def detect_model_type(model_path: Path) -> Tuple[str, Optional[str]]:
        """
        ëª¨ë¸ íƒ€ì…ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.
        Returns: (model_type, base_model) - ì˜ˆ: ('SDXL', 'sdxl_1_0')
        """
        debug_emoji(f"ëª¨ë¸ íƒ€ì… ê°ì§€ ì¤‘: {model_path.name}")
        
        # 1. Safetensors ë©”íƒ€ë°ì´í„°ì—ì„œ í™•ì¸
        if model_path.suffix == '.safetensors':
            metadata = MetadataParser.extract_from_safetensors(model_path) # ìˆ˜ì •ëœ extract_from_safetensors í˜¸ì¶œ
            
            # Civitai í˜•ì‹ ë©”íƒ€ë°ì´í„°
            if 'ss_base_model_version' in metadata:
                base_version = metadata['ss_base_model_version']
                #success(f"ss_base_model_version ë°œê²¬: {base_version}")
                
                if 'xl' in base_version.lower() or 'sdxl' in base_version.lower():
                    return 'SDXL', base_version
                elif 'sd3' in base_version.lower():
                    return 'SD3', base_version
                else:
                    return 'SD15', base_version
            
            # ComfyUI/A1111 í˜•ì‹
            if 'modelspec.architecture' in metadata:
                arch = metadata['modelspec.architecture']
                #success(f"modelspec.architecture ë°œê²¬: {arch}")
                
                if 'xl' in arch.lower():
                    return 'SDXL', arch
                elif 'sd3' in arch.lower():
                    return 'SD3', arch
                else:
                    return 'SD15', arch
            
            # ê¸°íƒ€ ë©”íƒ€ë°ì´í„° í‚¤ í™•ì¸
            for key, value in metadata.items():
                if isinstance(value, str):
                    value_lower = value.lower()
                    if 'sdxl' in value_lower or 'xl' in value_lower:
                        #success(f"SDXL íŒíŠ¸ ë°œê²¬: {key}={value[:50]}...")
                        return 'SDXL', value
                    elif 'sd3' in value_lower:
                        #success(f"SD3 íŒíŠ¸ ë°œê²¬: {key}={value[:50]}...")
                        return 'SD3', value
        
        # 2. íŒŒì¼ ê²½ë¡œì—ì„œ ì¶”ì¸¡ (í´ë”ëª… ê¸°ë°˜)
        path_parts = [part.lower() for part in model_path.parts]
        
        # SDXL í´ë”ì— ìˆìœ¼ë©´ SDXLë¡œ ê°„ì£¼
        if 'sdxl' in path_parts or 'xl' in path_parts:
            info(f"ğŸ“ ê²½ë¡œì—ì„œ SDXL í´ë” ë°œê²¬: {model_path}")
            return 'SDXL', None
        
        # 3. íŒŒì¼ëª…ì—ì„œ ì¶”ì¸¡
        filename_lower = model_path.name.lower()
        
        # SDXL í‚¤ì›Œë“œ
        sdxl_keywords = ['sdxl', 'xl', 'illustrious', 'pony', 'animagine', 'juggernaut']
        for keyword in sdxl_keywords:
            if keyword in filename_lower:
                #info(f"ğŸ“ íŒŒì¼ëª…ì—ì„œ SDXL í‚¤ì›Œë“œ ë°œê²¬: {keyword}")
                return 'SDXL', None
        
        # SD3 í‚¤ì›Œë“œ
        sd3_keywords = ['sd3', 'stable-diffusion-3']
        for keyword in sd3_keywords:
            if keyword in filename_lower:
                #info(f"ğŸ“ íŒŒì¼ëª…ì—ì„œ SD3 í‚¤ì›Œë“œ ë°œê²¬: {keyword}")
                return 'SD3', None
        
        # ê¸°ë³¸ê°’ì€ SD1.5
        #info_emoji(r"íŠ¹ë³„í•œ í‘œì‹œê°€ ì—†ì–´ SD1.5ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
        return 'SD15', None
    
    @staticmethod
    def get_model_info(model_path: Path) -> Dict[str, Any]:
        """ëª¨ë¸ íŒŒì¼ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        model_info = {
            'model_type': 'SD15',  # ê¸°ë³¸ê°’
            'base_model': 'SD1.5',  # ê¸°ë³¸ê°’
            'metadata': {}
        }
        
        try:
            # safetensors íŒŒì¼ì¸ ê²½ìš° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            if model_path.suffix.lower() == '.safetensors':
                metadata = MetadataParser.extract_from_safetensors(model_path)
                model_info['metadata'] = metadata
                
                # ëª¨ë¸ íƒ€ì… ê°ì§€
                model_type, _ = MetadataParser.detect_model_type(model_path)
                model_info['model_type'] = model_type
                
                # base_model ì„¤ì •
                if model_type == 'SDXL':
                    model_info['base_model'] = 'SDXL'
                else:
                    model_info['base_model'] = 'SD1.5'
                    
        except Exception as e:
            info(f"ëª¨ë¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({model_path.name}): {e}")
        
        return model_info
    
    @staticmethod
    def get_lora_info(lora_path: Path) -> Dict[str, Any]:
        """LoRA íŒŒì¼ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        lora_info = {
            'base_model': 'SD1.5',  # ê¸°ë³¸ê°’
            'metadata': {}
        }
        
        try:
            # safetensors íŒŒì¼ì¸ ê²½ìš° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            if lora_path.suffix.lower() == '.safetensors':
                metadata = MetadataParser.extract_from_safetensors(lora_path)
                lora_info['metadata'] = metadata
                
                # LoRA ê¸°ë³¸ ëª¨ë¸ ì •ë³´ ì¶”ì¶œ
                if 'ss_base_model_version' in metadata:
                    base_model = metadata['ss_base_model_version']
                    if 'xl' in base_model.lower() or 'sdxl' in base_model.lower():
                        lora_info['base_model'] = 'SDXL'
                    else:
                        lora_info['base_model'] = 'SD1.5'
                elif 'ss_sd_model_name' in metadata:
                    # SD ëª¨ë¸ëª…ì—ì„œ ì¶”ì •
                    model_name = metadata['ss_sd_model_name'].lower()
                    if 'xl' in model_name or 'sdxl' in model_name:
                        lora_info['base_model'] = 'SDXL'
                    else:
                        lora_info['base_model'] = 'SD1.5'
                        
        except Exception as e:
            info(f"LoRA ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({lora_path.name}): {e}")
        
        return lora_info