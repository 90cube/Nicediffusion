from ..core.logger import (
    debug, info, warning, error, success, failure, warning_emoji, 
    info_emoji, debug_emoji, process_emoji, model_emoji, image_emoji, ui_emoji
)
"""
ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Í¥ÄÎ¶¨ ÏÑúÎπÑÏä§
Ïª§Ïä§ÌÖÄ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÌååÏùºÎì§ÏùÑ Ïä§Ï∫îÌïòÍ≥† Î°úÎìúÌïòÎäî Í∏∞Îä•
"""

import os
from pathlib import Path
from typing import Dict, List, Optional, Any
from transformers import CLIPTokenizer, CLIPTokenizerFast


class TokenizerManager:
    """ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self, tokenizers_path: str):
        self.tokenizers_path = Path(tokenizers_path)
        self.available_tokenizers: Dict[str, Dict[str, Any]] = {}
        self.loaded_tokenizers: Dict[str, CLIPTokenizer] = {}
        self.current_tokenizer: Optional[CLIPTokenizer] = None
        
    def scan_tokenizers(self) -> Dict[str, Dict[str, Any]]:
        """ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÎîîÎ†âÌÜ†Î¶¨ Ïä§Ï∫î"""
        if not self.tokenizers_path.exists():
            warning_emoji(f"ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Í≤ΩÎ°úÍ∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {self.tokenizers_path}")
            return {}
        
        self.available_tokenizers.clear()
        
        for tokenizer_dir in self.tokenizers_path.iterdir():
            if not tokenizer_dir.is_dir():
                continue
                
            tokenizer_name = tokenizer_dir.name
            tokenizer_info = self._validate_tokenizer_directory(tokenizer_dir)
            
            if tokenizer_info:
                self.available_tokenizers[tokenizer_name] = tokenizer_info
                success(f"ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞úÍ≤¨: {tokenizer_name}")
            else:
                warning_emoji(f"Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä: {tokenizer_name}")
        
        info(f"üìã Ï¥ù {len(self.available_tokenizers)}Í∞úÏùò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞úÍ≤¨")
        return self.available_tokenizers
    
    def _validate_tokenizer_directory(self, tokenizer_dir: Path) -> Optional[Dict[str, Any]]:
        """ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÎîîÎ†âÌÜ†Î¶¨ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨"""
        required_files = ['tokenizer_config.json', 'vocab.json']
        optional_files = ['special_tokens_map.json', 'merges.txt']
        
        # ÌïÑÏàò ÌååÏùº ÌôïÏù∏
        for required_file in required_files:
            if not (tokenizer_dir / required_file).exists():
                return None
        
        # ÌååÏùº Ï†ïÎ≥¥ ÏàòÏßë
        tokenizer_info = {
            'name': tokenizer_dir.name,
            'path': str(tokenizer_dir),
            'files': {},
            'has_merges': False,
            'has_special_tokens': False
        }
        
        # ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
        for file_name in required_files + optional_files:
            file_path = tokenizer_dir / file_name
            if file_path.exists():
                tokenizer_info['files'][file_name] = {
                    'path': str(file_path),
                    'size': file_path.stat().st_size
                }
                
                if file_name == 'merges.txt':
                    tokenizer_info['has_merges'] = True
                elif file_name == 'special_tokens_map.json':
                    tokenizer_info['has_special_tokens'] = True
        
        return tokenizer_info
    
    def load_tokenizer(self, tokenizer_name: str) -> Optional[CLIPTokenizer]:
        """ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú"""
        if tokenizer_name in self.loaded_tokenizers:
            success(f"Ïù¥ÎØ∏ Î°úÎìúÎêú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÇ¨Ïö©: {tokenizer_name}")
            return self.loaded_tokenizers[tokenizer_name]
        
        if tokenizer_name not in self.available_tokenizers:
            failure(f"ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {tokenizer_name}")
            return None
        
        tokenizer_info = self.available_tokenizers[tokenizer_name]
        tokenizer_path = Path(tokenizer_info['path'])
        
        try:
            process_emoji(f"ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú Ï§ë: {tokenizer_name}")
            
            # CLIPTokenizer Î°úÎìú
            tokenizer = CLIPTokenizer.from_pretrained(
                tokenizer_path,
                local_files_only=True
            )
            
            # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÑ§Ï†ï
            tokenizer.padding_side = "right"
            tokenizer.truncation_side = "right"
            
            self.loaded_tokenizers[tokenizer_name] = tokenizer
            self.current_tokenizer = tokenizer
            
            success(f"ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú ÏôÑÎ£å: {tokenizer_name}")
            info(f"   - Î™®Îç∏ ÏµúÎåÄ Í∏∏Ïù¥: {tokenizer.model_max_length}")
            info(f"   - Ïñ¥Ìúò ÌÅ¨Í∏∞: {tokenizer.vocab_size}")
            
            return tokenizer
            
        except Exception as e:
            failure(f"ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú Ïã§Ìå®: {tokenizer_name}")
            info(f"   Ïò§Î•ò: {e}")
            return None
    
    def get_current_tokenizer(self) -> Optional[CLIPTokenizer]:
        """ÌòÑÏû¨ Î°úÎìúÎêú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞òÌôò"""
        return self.current_tokenizer
    
    def get_tokenizer_info(self, tokenizer_name: str) -> Optional[Dict[str, Any]]:
        """ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ï†ïÎ≥¥ Î∞òÌôò"""
        return self.available_tokenizers.get(tokenizer_name)
    
    def list_available_tokenizers(self) -> List[str]:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î™©Î°ù Î∞òÌôò"""
        return list(self.available_tokenizers.keys())
    
    def unload_tokenizer(self, tokenizer_name: str):
        """ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ïñ∏Î°úÎìú"""
        if tokenizer_name in self.loaded_tokenizers:
            del self.loaded_tokenizers[tokenizer_name]
            if self.current_tokenizer and tokenizer_name in str(self.current_tokenizer):
                self.current_tokenizer = None
            success(f"ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ïñ∏Î°úÎìú: {tokenizer_name}")
    
    def unload_all_tokenizers(self):
        """Î™®Îì† ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ïñ∏Î°úÎìú"""
        self.loaded_tokenizers.clear()
        self.current_tokenizer = None
        success(r"Î™®Îì† ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ïñ∏Î°úÎìú ÏôÑÎ£å")
    
    def get_tokenizer_stats(self, tokenizer_name: str) -> Optional[Dict[str, Any]]:
        """ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÌÜµÍ≥Ñ Ï†ïÎ≥¥"""
        tokenizer = self.loaded_tokenizers.get(tokenizer_name)
        if not tokenizer:
            return None
        
        return {
            'name': tokenizer_name,
            'model_max_length': tokenizer.model_max_length,
            'vocab_size': tokenizer.vocab_size,
            'pad_token': tokenizer.pad_token,
            'eos_token': tokenizer.eos_token,
            'unk_token': tokenizer.unk_token,
            'bos_token': tokenizer.bos_token,
            'pad_token_id': tokenizer.pad_token_id,
            'eos_token_id': tokenizer.eos_token_id,
            'unk_token_id': tokenizer.unk_token_id,
            'bos_token_id': tokenizer.bos_token_id,
        } 