# src/nicediff/core/state_manager.py
# (ë„ë©”ì¸ ì£¼ë„ ì„¤ê³„ ì›ì¹™ì— ë”°ë¼ ì •ë¦¬ëœ ë²„ì „)

import asyncio
import json
try:
    import tomllib
except ImportError:
    import tomli as tomllib
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import torch

from ..services.model_scanner import ModelScanner
from ..services.metadata_parser import MetadataParser
from ..services.tokenizer_manager import TokenizerManager
from ..domains.generation.model_definitions.generation_params import GenerationParams
from ..domains.generation.model_definitions.history_item import HistoryItem
from ..domains.generation.services.model_loader import ModelLoader
from ..domains.generation.services.image_saver import ImageSaver
from ..domains.generation.strategies.basic_strategy import BasicGenerationStrategy
from ..domains.generation.processors.prompt_processor import PromptProcessor
from ..services.long_prompt_handler import LongPromptHandler


class StateManager:
    """ì¤‘ì•™ ìƒíƒœ ê´€ë¦¬ìž (ë„ë©”ì¸ ì£¼ë„ ì„¤ê³„ ì›ì¹™ ì ìš©)"""
    
    def __init__(self):
        self._state: Dict[str, Any] = {
            'current_model_info': None,
            'current_vae_path': 'baked_in',  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë‚´ìž¥ VAE ì‚¬ìš©
            'current_loras': [],
            'current_params': GenerationParams(),
            'available_checkpoints': {},
            'available_vae': {},
            'available_loras': {},
            'history': [],
            'is_generating': False,
            'is_loading_model': False,
            'model_type': 'SD15',
            'is_xl_model': False,
            'sd_model': 'SD15',  # ì¶”ê°€: UIì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ íƒ€ìž…
            'status_message': 'ì¤€ë¹„ ì¤‘...',
            'infinite_mode': False,  # ë¬´í•œ ìƒì„± ëª¨ë“œ
            'current_mode': 'txt2img',  # í˜„ìž¬ ìƒì„± ëª¨ë“œ (txt2img, img2img, inpaint, upscale)
            
            # ì´ë¯¸ì§€ ìƒíƒœ ë¶„ë¦¬ (ê°œì„ ì•ˆ 5 ì ìš©)
            'init_image': None,  # ì—…ë¡œë“œëœ ì›ë³¸ ì´ë¯¸ì§€ (ì˜êµ¬ ë³´ì¡´)
            'generated_images': [],  # ìƒì„±ëœ ê²°ê³¼ ì´ë¯¸ì§€ë“¤ (ë…ë¦½ ê´€ë¦¬)
            'current_display_image': None,  # í˜„ìž¬ í‘œì‹œ ì¤‘ì¸ ì´ë¯¸ì§€
        }
        
        # í¬ê¸° ì¼ì¹˜ í† ê¸€ ê¸°ë³¸ê°’ ì„¤ì •
        self._state['current_params'].size_match_enabled = False
        self._observers: Dict[str, List[Callable]] = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.stop_generation_flag = asyncio.Event()
        
        # ë„ë©”ì¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.model_loader = ModelLoader(self.device)
        self.image_saver = ImageSaver()
        self.tokenizer_manager = None  # initializeì—ì„œ ì„¤ì •
        self.prompt_processor = PromptProcessor('SD15')  # ê¸°ë³¸ê°’ìœ¼ë¡œ SD15
        self.long_prompt_handler = None  # initializeì—ì„œ ì„¤ì •
        
        print(f"ðŸ–¥ï¸ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
    
    async def initialize(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ëª¨ë¸ ìŠ¤ìº” ì‹œìž‘"""
        config_path = Path("config.toml")
        if await asyncio.to_thread(config_path.exists):
            with open(config_path, "rb") as f:
                self.config = tomllib.load(f)
        else:
            # config.tomlì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            self.config = {
                'paths': {
                    'checkpoints': 'models/checkpoints',
                    'vae': 'models/vae',
                    'loras': 'models/loras'
                }
            }
            print("âš ï¸ config.tomlì´ ì—†ì–´ ê¸°ë³¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # í† í¬ë‚˜ì´ì € ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.tokenizer_manager = TokenizerManager(self.config.get('paths', {}).get('tokenizers', 'models/tokenizers'))
        
        # NOTE: ëª¨ë¸ ìŠ¤ìº”ì€ ì˜¤ëž˜ ê±¸ë¦´ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ë°±ê·¸ë¼ìš´ë“œ ìž‘ì—…ìœ¼ë¡œ ì‹¤í–‰
        asyncio.create_task(self._scan_models())
    
    async def _scan_models(self):
        """ModelScannerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ìŠ¤ìº”í•˜ê³ , í‘œì¤€í™”ëœ í‚¤ë¡œ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.set('status_message', 'ëª¨ë¸ ìŠ¤ìº” ì¤‘...')
        paths_config = self.config.get('paths', {})
        scanner = ModelScanner(paths_config=paths_config)
        all_models_data = await scanner.scan_all_models()
        
        # ì´ì œ ìŠ¤ìºë„ˆê°€ 'checkpoints' í‚¤ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, ì´ ì½”ë“œê°€ ì •ìƒ ìž‘ë™í•©ë‹ˆë‹¤.
        self.set('available_checkpoints', all_models_data.get('checkpoints', {}))
        self.set('available_vae', all_models_data.get('vae', {}))
        self.set('available_loras', all_models_data.get('loras', {}))        

        # í† í¬ë‚˜ì´ì € ìŠ¤ìº”
        if self.tokenizer_manager:
            self.tokenizer_manager.scan_tokenizers()
        
        self.set('status_message', 'ì¤€ë¹„ ì™„ë£Œ')
        print("âœ… StateManager: ìŠ¤ìº” ê²°ê³¼ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        
        # LoRA ëª©ë¡ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ë°œìƒ
        loras_data = all_models_data.get('loras', {})
        self._notify('loras_updated', loras_data)
        print(f"ðŸ“¢ LoRA ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ë°œìƒ: {sum(len(items) for items in loras_data.values())}ê°œ LoRA")

    # --- ëª¨ë¸ ì„ íƒ ë° ë¡œë”© ---
    async def select_model(self, model_info: Dict[str, Any]):
        """1ë‹¨ê³„: GPU ë¡œë”© ì—†ì´, ë©”íƒ€ë°ì´í„° í‘œì‹œë¥¼ ìœ„í•´ ëª¨ë¸ì„ 'ì„ íƒ'ë§Œ í•©ë‹ˆë‹¤."""
        print(f"ëª¨ë¸ ì„ íƒë¨ (ë¯¸ë¦¬ë³´ê¸°ìš©): {model_info['name']}")
        self.set('current_model_info', model_info)
        self.set('sd_model_type', model_info.get('model_type', 'SD15'))
        
        # ì„ íƒ ì´ë²¤íŠ¸ ë°œìƒ
        self._notify('model_selection_changed', model_info)

    async def load_model_pipeline(self, model_info: Dict[str, Any]) -> bool:
        """[ìµœì¢… ìˆ˜ì •] ëª¨ë¸ ë¡œë”©ì˜ ëª¨ë“  ê³¼ì •ì„ ì±…ìž„ì§€ëŠ” ì¤‘ì•™ ì²˜ë¦¬ ë©”ì„œë“œ"""
    
        # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì´ë©´ ì¤‘ë‹¨
        current_model_info = self.get('current_model_info')
        if (current_model_info and 
            current_model_info.get('path') == model_info.get('path') and 
            self.model_loader.get_current_pipeline() is not None):
            self._notify_user(f"'{model_info['name']}' ëª¨ë¸ì€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìžˆìŠµë‹ˆë‹¤.", 'info')
            return True

        try:
            self.set('is_loading_model', True)
            self._notify('model_loading_started', {'name': model_info['name']})
            
            # ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
            self.model_loader.unload_model()
            
            # ë„ë©”ì¸ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¡œë“œ
            await self.model_loader.load_model(model_info)
            self.set('current_model_info', model_info)
            
            # VAE ìžë™ ì„ íƒì€ ì‚¬ìš©ìžê°€ 'Automatic'ì„ ì„ íƒí–ˆì„ ë•Œë§Œ ì‹¤í–‰
            current_vae_path = self.get('current_vae_path')
            if current_vae_path is None:
                await self._auto_select_vae(model_info)
            
            # Top_barì—ì„œëŠ” íŒŒë¼ë¯¸í„° ìžë™ ì ìš© ê¸ˆì§€ - í”„ë¡¬í”„íŠ¸ íŒ¨ë„ê³¼ íŒŒë¼ë¯¸í„° íŒ¨ë„ì—ì„œë§Œ ì„¤ì •
            print(f"â„¹ï¸ íŒŒë¼ë¯¸í„° ìžë™ ì ìš© ë¹„í™œì„±í™” (Top_barì—ì„œëŠ” ì²´í¬í¬ì¸íŠ¸ì™€ VAEë§Œ ì„ íƒ)")
            
            # ëª¨ë¸ íƒ€ìž…ì— ë”°ë¼ PromptProcessorì™€ LongPromptHandler ì—…ë°ì´íŠ¸
            model_type = model_info.get('model_type', 'SD15')
            self.prompt_processor = PromptProcessor(model_type)
            
            # LongPromptHandler ì´ˆê¸°í™” (í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œëœ í›„)
            pipeline = self.model_loader.get_current_pipeline()
            if pipeline and hasattr(pipeline, 'tokenizer'):
                max_tokens = 225 if model_type == 'SDXL' else 77
                self.long_prompt_handler = LongPromptHandler(pipeline.tokenizer, max_tokens)
                print(f"âœ… LongPromptHandler ì´ˆê¸°í™”: {model_type} ëª¨ë“œ (ìµœëŒ€ {max_tokens} í† í°)")
            
            print(f"âœ… PromptProcessor ì—…ë°ì´íŠ¸: {model_type} ëª¨ë“œ (ìµœëŒ€ {self.prompt_processor.max_tokens} í† í°)")
            
            # ëª¨ë¸ ë¡œë”© ì™„ë£Œ ì•Œë¦¼ (ì„ íƒ ì•Œë¦¼ì€ ì´ë¯¸ select_modelì—ì„œ ë°œìƒí–ˆìœ¼ë¯€ë¡œ ìƒëžµ)
            self._notify('model_loaded', model_info)
            
            self._notify('model_loading_finished', {'success': True, 'model_info': model_info})
            self._notify_user(f"'{model_info['name']}' ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.", 'positive')
            return True
        
        except Exception as e:
            import traceback
            traceback.print_exc()
            error_msg = f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            self._notify_user(error_msg, 'negative')
            self._notify('model_loading_finished', {'success': False, 'error': str(e)})
            return False
        
        finally:
            self.set('is_loading_model', False)

    async def _load_model_heavy_work(self, model_info: Dict[str, Any]):
        """ì‹¤ì œ ëª¨ë¸ ë¡œë”© ìž‘ì—… (Heavy I/O)"""
        def _load():
            model_path = model_info['path']
            model_type = model_info.get('model_type', 'SD15')
            
            print(f"ðŸ” ëª¨ë¸ íƒ€ìž…: {model_type}, ê²½ë¡œ: {model_path}")
            
            # ëª¨ë¸ íƒ€ìž…ì— ë”°ë¼ ì ì ˆí•œ íŒŒì´í”„ë¼ì¸ ì„ íƒ
            if model_type == 'SDXL':
                from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline
                pipeline = StableDiffusionXLPipeline.from_single_file(
                    model_path,
                    torch_dtype=torch.float16,
                    use_safetensors=True,
                    safety_checker=None
                )
            else:
                # SD15 ëª¨ë¸
                from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipeline
                pipeline = StableDiffusionPipeline.from_single_file(
                    model_path,
                    torch_dtype=torch.float16,
                    use_safetensors=True,
                    safety_checker=None,
                    requires_safety_checker=False
                )
            
            # GPUë¡œ ì´ë™
            pipeline = pipeline.to(self.device)
            
            # SD15 íŠ¹í™” ì„¤ì •
            if model_type == 'SD15':
                # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ì„¤ì •
                if hasattr(pipeline, 'enable_attention_slicing'):
                    pipeline.enable_attention_slicing(1)
                if hasattr(pipeline, 'enable_vae_slicing'):
                    pipeline.enable_vae_slicing()
                
                # PyTorch 2.0+ SDPA ìµœì í™” (xformers ëŒ€ì‹  ì‚¬ìš©)
                if hasattr(pipeline, 'enable_model_cpu_offload'):
                    pipeline.enable_model_cpu_offload()
                
                # xformersëŠ” ì„ íƒì  ê¸°ëŠ¥ì´ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                try:
                    if hasattr(pipeline, 'enable_xformers_memory_efficient_attention'):
                        pipeline.enable_xformers_memory_efficient_attention()
                        print("âœ… xformers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ í™œì„±í™”")
                except (ModuleNotFoundError, AttributeError) as e:
                    print(f"âš ï¸ xformers ë¯¸ì‚¬ìš©: {e}")
                    print("âœ… PyTorch 2.0+ SDPA ì‚¬ìš© ì¤‘")
                
                # SD15 í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ ì¶”ê°€ ì„¤ì •
                if hasattr(pipeline, 'scheduler') and hasattr(pipeline.scheduler, 'config'):
                    # Karras ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™”
                    pipeline.scheduler.config.use_karras_sigmas = True
                    pipeline.scheduler.config.karras_rho = 7.0
                    
                    # SD15ì—ì„œ ë” ì•ˆì •ì ì¸ ìƒì„±
                    if hasattr(pipeline.scheduler.config, 'beta_start'):
                        pipeline.scheduler.config.beta_start = 0.00085
                    if hasattr(pipeline.scheduler.config, 'beta_end'):
                        pipeline.scheduler.config.beta_end = 0.012
                
                # ëª¨ë¸ ì •ë°€ë„ ìµœì í™”
                if hasattr(pipeline, 'text_encoder'):
                    pipeline.text_encoder = pipeline.text_encoder.to(torch.float16)
                if hasattr(pipeline, 'vae'):
                    pipeline.vae = pipeline.vae.to(torch.float16)
                if hasattr(pipeline, 'unet'):
                    pipeline.unet = pipeline.unet.to(torch.float16)
                
                print("âœ… SD15 í’ˆì§ˆ ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")
            else:
                # SDXL ì„¤ì •
                if hasattr(pipeline, 'enable_attention_slicing'):
                    pipeline.enable_attention_slicing()
                if hasattr(pipeline, 'enable_vae_slicing'):
                    pipeline.enable_vae_slicing()
            
            return pipeline
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¡œë”© ìˆ˜í–‰
        self.pipeline = await asyncio.to_thread(_load)
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_info['name']}")

    async def _unload_current_model(self):
        """ê¸°ì¡´ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ"""
        if self.pipeline is not None:
            print(f"ðŸ—‘ï¸ ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
            try:
                # float16 ëª¨ë¸ì€ CPUë¡œ ì´ë™í•˜ì§€ ì•Šê³  ì§ì ‘ ì‚­ì œ
                del self.pipeline
                self.pipeline = None
                
                # CUDA ìºì‹œ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                self.pipeline = None

    async def _auto_select_vae(self, model_info: Dict[str, Any]):
        """A1111 ë°©ì‹ì˜ VAE ìžë™ ì„ íƒ ë¡œì§"""
        print(f"ðŸ” VAE ìžë™ ì„ íƒ í”„ë¡œì„¸ìŠ¤ ì‹œìž‘...")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ê¶Œìž¥ VAE ì°¾ê¸°
        metadata = model_info.get('metadata', {})
        recommended_vae = metadata.get('recommended_vae')
        
        if recommended_vae:
            # ì¶”ì²œ VAEê°€ ìžˆìœ¼ë©´ ë¡œë“œ ì‹œë„
            available_vae = self.get('available_vae', {})
            for folder_vae in available_vae.values():
                for vae_info in folder_vae:
                    if recommended_vae.lower() in vae_info['name'].lower():
                        print(f"âœ… ì¶”ì²œ VAE ë°œê²¬: {vae_info['name']}")
                        await self.load_vae(vae_info['path'])
                        return
        
        # SD15 ëª¨ë¸ì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” VAEë“¤ì„ ìžë™ìœ¼ë¡œ ì°¾ì•„ì„œ ì ìš©
        model_type = model_info.get('model_type', 'SD15')
        if model_type == 'SD15':
            available_vae = self.get('available_vae', {})
            preferred_vae_names = [
                'vaeFtMse840000EmaPruned',  # ê°€ìž¥ ì¼ë°˜ì ì¸ SD15 VAE
                'vae-ft-mse-840000-ema-pruned',
                'vaeFtMse840k',
                'vae-ft-mse-840k',
                'vaeRealanimark',
                'vaeRealanimark_vaeRealanimark'
            ]
            
            for preferred_name in preferred_vae_names:
                for folder_vae in available_vae.values():
                    for vae_info in folder_vae:
                        if preferred_name.lower() in vae_info['name'].lower():
                            print(f"âœ… SD15 ê¶Œìž¥ VAE ë°œê²¬: {vae_info['name']}")
                            await self.load_vae(vae_info['path'])
                            return
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ë‚´ìž¥ VAE ì‚¬ìš©
        print("â„¹ï¸ ì²´í¬í¬ì¸íŠ¸ ë‚´ìž¥ VAE ì‚¬ìš© (ë³„ë„ VAE ì—†ìŒ)")
        self.set('current_vae_path', 'baked_in')
        self._notify('vae_auto_selected', 'baked_in')

    async def load_vae(self, vae_path: str):
        """VAE íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì— ì ìš©"""
        if not self.model_loader.get_current_pipeline():
            self._notify_user('ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.', 'warning')
            return False
        
        self.set('is_loading_model', True) # VAE ë¡œë”©ë„ ë¡œë”© ìƒíƒœë¡œ ê°„ì£¼
        self._notify_user(f"VAE '{Path(vae_path).name}' ë¡œë“œ ì¤‘...", 'info')
        
        try:
            # ë„ë©”ì¸ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ VAE ë¡œë“œ
            success = await self.model_loader.load_vae(vae_path)
            
            if success:
                self.set('current_vae_path', vae_path)
                self._notify_user(f"VAE '{Path(vae_path).name}' ì ìš© ì™„ë£Œ!", 'positive')
                return True
            else:
                self._notify_user(f"VAE ë¡œë“œ ì‹¤íŒ¨", 'negative')
                return False
        except Exception as e:
            print(f"âŒ VAE ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._notify_user(f"VAE ë¡œë“œ ì‹¤íŒ¨: {e}", 'negative')
            return False
        finally:
            self.set('is_loading_model', False)

    async def generate_image(self):
        """
        ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰ (ê°€ì´ë“œ 2ë‹¨ê³„ ì •ì±… ì ìš©)
        - ìƒì„± íŒŒë¼ë¯¸í„°ëŠ” ë°˜ë“œì‹œ current_params(íŒŒë¼ë¯¸í„°/í”„ë¡¬í”„íŠ¸ íŒ¨ë„)ì—ì„œë§Œ ìˆ˜ì§‘
        - ëª¨ë¸/LoRA/í”„ë¦¬ë·° ë“± ì™¸ë¶€ ìƒíƒœëŠ” ìƒì„± íŒŒë¼ë¯¸í„°ì— ì§ì ‘ í¬í•¨í•˜ì§€ ì•ŠìŒ
        - í˜¹ì‹œë¼ë„ ì™¸ë¶€ ê°’ì´ params_dictì— ë“¤ì–´ê°€ë©´ ê²½ê³  ë¡œê·¸ ì¶œë ¥ ë° ë¬´ì‹œ
        """
        if self.get('is_generating'):
            self._notify_user('ì´ë¯¸ ìƒì„± ì¤‘ìž…ë‹ˆë‹¤.', 'warning')
            return
        
        if not self.model_loader.get_current_pipeline():
            self._notify_user('ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'warning')
            return
        
        self.stop_generation_flag.clear()
        self.set('is_generating', True)
        
        try:
            pipeline = self.model_loader.get_current_pipeline()
            strategy = BasicGenerationStrategy(pipeline, self.device, state=self)
            
            # [ì •ì±…] ì˜¤ì§ current_paramsì—ì„œë§Œ ìƒì„± íŒŒë¼ë¯¸í„° ìˆ˜ì§‘
            params = self.get('current_params')
            
            # ëžœë¤ ì‹œë“œ ì²˜ë¦¬ (íŒŒë¼ë¯¸í„° íŒ¨ë„ì˜ ì‹œë“œ ê³ ì • ìƒíƒœ í™•ì¸)
            seed = params.seed
            if hasattr(params, 'seed_pinned') and not params.seed_pinned:
                # ëžœë¤ ì‹œë“œ ëª¨ë“œ: ìƒˆë¡œìš´ ëžœë¤ ì‹œë“œ ìƒì„±
                import random
                seed = random.randint(1, 2147483647)
                print(f"ðŸŽ² ëžœë¤ ì‹œë“œ ëª¨ë“œ: ìƒˆë¡œìš´ ì‹œë“œ ìƒì„± - {seed}")
            else:
                # ì‹œë“œ ê³ ì • ëª¨ë“œ: ê¸°ì¡´ ì‹œë“œ ì‚¬ìš©
                print(f"ðŸ”’ ì‹œë“œ ê³ ì • ëª¨ë“œ: ê¸°ì¡´ ì‹œë“œ ì‚¬ìš© - {seed}")
            
            params_dict = {
                'prompt': params.prompt,
                'negative_prompt': params.negative_prompt,
                'width': params.width,
                'height': params.height,
                'steps': params.steps,
                'cfg_scale': params.cfg_scale,
                'seed': seed,  # ì²˜ë¦¬ëœ ì‹œë“œ ì‚¬ìš©
                'sampler': params.sampler,
                'scheduler': params.scheduler,
                'batch_size': params.batch_size,
                'clip_skip': getattr(params, 'clip_skip', 1),
            }
            # [ë°©ì–´] ì™¸ë¶€ ìƒíƒœê°€ params_dictì— ì„žì´ë©´ ê²½ê³ 
            for forbidden in ['current_model_info', 'current_loras', 'current_vae_path', 'preview', 'preview_image']:
                if forbidden in params_dict:
                    print(f"âš ï¸ ê²½ê³ : ìƒì„± íŒŒë¼ë¯¸í„°ì— ì™¸ë¶€ ìƒíƒœ({forbidden})ê°€ í¬í•¨ë˜ì–´ ìžˆìŒ. ë¬´ì‹œí•©ë‹ˆë‹¤.")
                    params_dict.pop(forbidden)
            
            current_mode = self.get('current_mode', 'txt2img')
            if current_mode in ['img2img', 'inpaint', 'upscale']:
                params_dict['img2img_mode'] = True
                strength = getattr(params, 'strength', 0.8)
                params_dict['strength'] = strength
                size_match_enabled = getattr(params, 'size_match_enabled', False)
                params_dict['size_match_enabled'] = size_match_enabled
                init_image = self.get('init_image')
                if init_image is None:
                    uploaded_image = self.get('uploaded_image')
                    if uploaded_image is not None:
                        from PIL import Image
                        import numpy as np
                        if isinstance(uploaded_image, np.ndarray):
                            init_image = Image.fromarray(uploaded_image.astype('uint8'))
                            self.set('init_image', init_image)
                params_dict['init_image'] = init_image
                print(f"ðŸ” i2i ëª¨ë“œ íŒŒë¼ë¯¸í„°:")
                print(f"  - init_image: {init_image}")
                print(f"  - strength: {strength}")
                print(f"  - size_match_enabled: {size_match_enabled}")
                print(f"  - size: {params.width}x{params.height}")
                if init_image is None:
                    self._notify_user('ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.', 'warning')
                    self.set('is_generating', False)
                    return
            else:
                params_dict['img2img_mode'] = False
                print("ðŸŽ¨ txt2img ëª¨ë“œ í™œì„±í™”")
            
            # ëª¨ë¸ ì •ë³´ëŠ” íŒŒì´í”„ë¼ì¸ì—ë§Œ ì˜í–¥, ìƒì„± íŒŒë¼ë¯¸í„°ì—ëŠ” ì§ì ‘ í¬í•¨í•˜ì§€ ì•ŠìŒ
            model_info = self.get('current_model_info', {})
            
            self._notify('generation_started', {
                'mode': current_mode,
                'params': params_dict
            })
            
            result = await strategy.execute(params_dict, model_info)
            
            # 1ë‹¨ê³„: ìƒì„± ì™„ë£Œ í›„ ì´ë¯¸ì§€ê°€ StateManagerì— ì €ìž¥ë˜ëŠ”ì§€ í™•ì¸
            print("=" * 80)
            print("ðŸ” [ë””ë²„ê¹… 1ë‹¨ê³„] ìƒì„± ì™„ë£Œ í›„ ì´ë¯¸ì§€ ì €ìž¥ í™•ì¸")
            print("=" * 80)
            
            if result.success and result.images:
                print(f"âœ… ìƒì„± ì„±ê³µ: {len(result.images)}ê°œ ì´ë¯¸ì§€")
                print(f"   - result.images íƒ€ìž…: {type(result.images)}")
                print(f"   - result.images[0] íƒ€ìž…: {type(result.images[0])}")
                print(f"   - result.images[0] í¬ê¸°: {result.images[0].size if hasattr(result.images[0], 'size') else 'N/A'}")
                
                # ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì €ìž¥ (ê°œì„ ì•ˆ 5 ì ìš©)
                generated_images = result.images
                self.set_generated_images(generated_images)
                print(f"âœ… StateManagerì— generated_images ë…ë¦½ ì €ìž¥ ì™„ë£Œ")
                print(f"   - ì €ìž¥ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(generated_images)}")
                
                # ì›ë³¸ ì´ë¯¸ì§€ ë³´ì¡´ í™•ì¸ ë° ê°•í™”
                self.preserve_init_image()
                self.ensure_image_state_preservation()
                
                # ê° ì´ë¯¸ì§€ë³„ í›„ì²˜ë¦¬
                for i, image in enumerate(generated_images):
                    print(f"   - ì´ë¯¸ì§€ {i+1} í›„ì²˜ë¦¬ ì‹œìž‘")
                    await self.finish_generation(image, params, params_dict['seed'])
                    print(f"   - ì´ë¯¸ì§€ {i+1} í›„ì²˜ë¦¬ ì™„ë£Œ")
                
                self._notify_user(f'{len(result.images)}ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!', 'positive')
            else:
                error_msg = ', '.join(result.errors) if result.errors else 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'
                print(f"âŒ ìƒì„± ì‹¤íŒ¨: {error_msg}")
                self._notify_user(f'ìƒì„± ì‹¤íŒ¨: {error_msg}', 'negative')
                generated_images = []
            
        except Exception as e:
            print(f"âŒ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            self._notify_user(f'ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}', 'negative')
            generated_images = []
            
        finally:
            self.set('is_generating', False)
            
            # 2ë‹¨ê³„: 'generation_completed' ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ëŠ”ì§€ í™•ì¸
            print("=" * 80)
            print("ðŸ” [ë””ë²„ê¹… 2ë‹¨ê³„] generation_completed ì´ë²¤íŠ¸ ë°œìƒ í™•ì¸")
            print("=" * 80)
            
            if generated_images:
                print(f"âœ… generation_completed ì´ë²¤íŠ¸ ë°œìƒ (ì´ë¯¸ì§€ {len(generated_images)}ê°œ í¬í•¨)")
                print(f"   - ì´ë²¤íŠ¸ ë°ì´í„°: {{'images': {len(generated_images)}ê°œ ì´ë¯¸ì§€}}")
                self._notify('generation_completed', {'images': generated_images})
            else:
                print(f"âš ï¸ generation_completed ì´ë²¤íŠ¸ ë°œìƒ (ì´ë¯¸ì§€ ì—†ìŒ)")
                self._notify('generation_completed', {})
            
            print("=" * 80)
            print("ðŸŽ‰ StateManager ë””ë²„ê¹… ì™„ë£Œ")
            print("=" * 80)

    async def finish_generation(self, image, params: GenerationParams, seed: int):
        """ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ í›„ì²˜ë¦¬ (ë„ë©”ì¸ ì„œë¹„ìŠ¤ ì‚¬ìš©)"""
        try:
            # ë„ë©”ì¸ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì €ìž¥
            model_name = self.get('current_model_info')['name']
            save_result = await self.image_saver.save_generated_image(image, params, seed, model_name)
            
            # ì´ë²¤íŠ¸ ë°œìƒ
            self._notify('image_generated', {
                'image_path': save_result['image_path'],
                'thumbnail_path': save_result['thumbnail_path'],
                'params': params,
                'seed': seed
            })
            
            # ížˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            history_item = HistoryItem(
                image_path=save_result['image_path'],
                thumbnail_path=save_result['thumbnail_path'],
                params=params,
                model=model_name,
                vae=self.get('current_vae_path'),
                loras=self.get('current_loras', [])
            )
            self._add_to_history(history_item.to_dict())
            
            print(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ: 1ê°œ ì´ë¯¸ì§€ ì €ìž¥")
            
        except Exception as e:
            print(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    def _build_metadata_string(self, params: GenerationParams, seed: int) -> str:
        """PNG ë©”íƒ€ë°ì´í„° ë¬¸ìžì—´ ìƒì„±"""
        model_name = self.get('current_model_info', {}).get('name', 'Unknown')
        vae_path = self.get('current_vae_path')
        
        # VAE ì •ë³´ ì¶”ê°€
        vae_info = ""
        if vae_path and vae_path != 'baked_in':
            vae_name = Path(vae_path).name
            vae_info = f", VAE: {vae_name}"
        
        return f"{params.prompt}\n\nNegative prompt: {params.negative_prompt}\nSteps: {params.steps}, Sampler: {params.sampler}, CFG scale: {params.cfg_scale}, Seed: {seed}, Size: {params.width}x{params.height}, Model: {model_name}{vae_info}"

    def _create_pnginfo(self, metadata: str) -> Any:
        """PNG ë©”íƒ€ë°ì´í„° ê°ì²´ ìƒì„±"""
        pnginfo = PngImagePlugin.PngInfo()
        pnginfo.add_text("parameters", metadata)
        return pnginfo

    async def start_infinite_generation(self):
        """ë¬´í•œ ìƒì„± ëª¨ë“œ ì‹œìž‘"""
        if not self.model_loader.get_current_pipeline():
            self._notify_user('ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'warning')
            return
        
        self.set('infinite_mode', True)
        self._notify_user('ë¬´í•œ ìƒì„± ëª¨ë“œê°€ ì‹œìž‘ë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')
        asyncio.create_task(self._infinite_generation_loop())

    async def _infinite_generation_loop(self):
        """ë¬´í•œ ìƒì„± ë£¨í”„"""
        while self.get('infinite_mode') and not self.stop_generation_flag.is_set():
            try:
                await self._generate_single_image()
                await asyncio.sleep(2)  # 2ì´ˆ ëŒ€ê¸°
            except Exception as e:
                print(f"âŒ ë¬´í•œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(5)  # ì˜¤ë¥˜ ì‹œ 5ì´ˆ ëŒ€ê¸°

    async def _generate_single_image(self):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ìƒì„± (ë¬´í•œ ëª¨ë“œìš©)"""
        if not self.model_loader.get_current_pipeline():
            return
        
        params = self.get('current_params')
        
        def _generate():
            return self.model_loader.get_current_pipeline()(
                prompt=params.prompt,
                negative_prompt=params.negative_prompt,
                width=params.width,
                height=params.height,
                num_inference_steps=params.steps,
                guidance_scale=params.cfg_scale,
                generator=torch.Generator(device=self.device).manual_seed(params.seed)
            )
        
        try:
            result = await asyncio.to_thread(_generate)
            if result.images:
                await self.finish_generation(result.images[0], params, params.seed)
        except Exception as e:
            print(f"âŒ ë‹¨ì¼ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")

    async def stop_infinite_generation(self):
        """ë¬´í•œ ìƒì„± ëª¨ë“œ ì¤‘ì§€"""
        self.set('infinite_mode', False)
        self._notify_user('ë¬´í•œ ìƒì„± ëª¨ë“œê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')

    async def stop_generation(self):
        """ìƒì„± ì¤‘ì§€"""
        self.stop_generation_flag.set()
        self.set('is_generating', False)
        self._notify_user('ìƒì„±ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')

    def apply_params_from_metadata(self, model_info: Dict[str, Any], include_prompts: bool = False):
        """ë©”íƒ€ë°ì´í„°ì—ì„œ íŒŒë¼ë¯¸í„° ì ìš© (ë” ì´ìƒ ìžë™ìœ¼ë¡œ í˜¸ì¶œë˜ì§€ ì•ŠìŒ)"""
        def get_first(*keys, default=None):
            # ìµœìƒìœ„ â†’ parameters ë‚´ë¶€ ìˆœì„œë¡œ ê°’ ë°˜í™˜
            for key in keys:
                if key in model_info:
                    return model_info[key]
                if 'parameters' in model_info and key in model_info['parameters']:
                    return model_info['parameters'][key]
            return default
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        width = get_first('width', 'Width', default=512)
        height = get_first('height', 'Height', default=512)
        steps = get_first('steps', 'Steps', default=20)
        cfg_scale = get_first('cfg_scale', 'CFG Scale', default=7.0)
        sampler = get_first('sampler', 'Sampler', default='dpmpp_2m')
        scheduler = get_first('scheduler', 'Scheduler', default='karras')
        seed = get_first('seed', 'Seed', default=-1)
        
        # í˜„ìž¬ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        current_params = self.get('current_params')
        
        # í”„ë¡¬í”„íŠ¸ í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬
        if include_prompts:
            prompt = get_first('prompt', 'Prompt', default='')
            negative_prompt = get_first('negative_prompt', 'Negative prompt', default='')
            current_params.prompt = prompt
            current_params.negative_prompt = negative_prompt
            print(f"â„¹ï¸ í”„ë¡¬í”„íŠ¸ëŠ” ì œì™¸í•˜ê³  íŒŒë¼ë¯¸í„°ë§Œ ì ìš©í•©ë‹ˆë‹¤.")
        
        # ê° íŒŒë¼ë¯¸í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì´ë²¤íŠ¸ ë°œìƒ
        if width is not None:
            self.update_param('width', int(width))
        if height is not None:
            self.update_param('height', int(height))
        if steps is not None:
            self.update_param('steps', int(steps))
        if cfg_scale is not None:
            self.update_param('cfg_scale', float(cfg_scale))
        if sampler:
            self.update_param('sampler', str(sampler))
        if scheduler:
            self.update_param('scheduler', str(scheduler))
        
        # ì‹œë“œ ì²˜ë¦¬
        if seed is not None and seed != -1:
            self.update_param('seed', int(seed))
            print(f"âœ… Seed {seed} ë°œê²¬, ëžœë¤ ì‹œë“œë¡œ ì„¤ì •: -1")
        else:
            self.update_param('seed', -1)
        
        print(f"âœ… ë©”íƒ€ë°ì´í„°ì—ì„œ íŒŒë¼ë¯¸í„°ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def restore_from_history(self, history_id: str):
        """ížˆìŠ¤í† ë¦¬ì—ì„œ íŒŒë¼ë¯¸í„° ë³µì›"""
        history = self.get('history', [])
        for item in history:
            if item.get('id') == history_id:
                # íŒŒë¼ë¯¸í„° ë³µì›
                params = item.get('params', {})
                current_params = self.get('current_params')
                
                # GenerationParams ê°ì²´ë¡œ ë³€í™˜
                if isinstance(params, dict):
                    for key, value in params.items():
                        if hasattr(current_params, key):
                            setattr(current_params, key, value)
                elif hasattr(params, '__dict__'):
                    # ì´ë¯¸ GenerationParams ê°ì²´ì¸ ê²½ìš°
                    for key, value in params.__dict__.items():
                        if hasattr(current_params, key):
                            setattr(current_params, key, value)
                
                # VAE ë³µì›
                vae = item.get('vae')
                if vae and vae != 'baked_in':
                    self.set('current_vae_path', vae)
                
                # LoRA ë³µì›
                loras = item.get('loras', [])
                if loras:
                    self.set('current_loras', loras)
                
                # ëª¨ë¸ ë³µì› (ê°€ëŠ¥í•œ ê²½ìš°)
                model_name = item.get('model')
                if model_name:
                    available_checkpoints = self.get('available_checkpoints', {})
                    for folder_models in available_checkpoints.values():
                        for model_info in folder_models:
                            if model_info['name'] == model_name:
                                asyncio.create_task(self.select_model(model_info))
                                break
                
                self._notify('params_restored', params)
                self._notify_user('ížˆìŠ¤í† ë¦¬ì—ì„œ íŒŒë¼ë¯¸í„°ê°€ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤.', 'positive')
                return
        
        self._notify_user('ížˆìŠ¤í† ë¦¬ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'negative')

    async def get_vae_options_list(self) -> List[str]:
        """VAE ì˜µì…˜ ëª©ë¡ ë°˜í™˜"""
        available_vae = self.get('available_vae', {})
        options = ['baked_in']  # ê¸°ë³¸ê°’
        
        for folder_vae in available_vae.values():
            for vae_info in folder_vae:
                options.append(vae_info['name'])
        
        return options

    def find_vae_by_name(self, vae_name: str) -> Optional[str]:
        """VAE ì´ë¦„ìœ¼ë¡œ ê²½ë¡œ ì°¾ê¸°"""
        if vae_name == 'baked_in':
            return 'baked_in'
        
        available_vae = self.get('available_vae', {})
        for folder_vae in available_vae.values():
            for vae_info in folder_vae:
                if vae_info['name'] == vae_name:
                    return vae_info['path']
        
        return None

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ëª¨ë¸ ì–¸ë¡œë“œ
            self.model_loader.unload_model()
            
            # CUDA ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # --- ìƒíƒœ ê´€ë¦¬ ë©”ì„œë“œë“¤ ---
    def get(self, key: str, default: Any = None) -> Any:
        """ìƒíƒœ ê°’ ì¡°íšŒ"""
        return self._state.get(key, default)

    def set(self, key: str, value: Any):
        """ìƒíƒœ ê°’ ì„¤ì •"""
        self._state[key] = value
        self._notify(f'{key}_changed', value)
    
    def set_init_image(self, image):
        """img2imgìš© ì›ë³¸ ì´ë¯¸ì§€ ì„¤ì • (ì˜êµ¬ ë³´ì¡´) - ë¬´í•œ ìž¬ê·€ ë°©ì§€"""
        print(f"ðŸ” set_init_image í˜¸ì¶œ: ì´ë¯¸ì§€ íƒ€ìž…={type(image)}")
        
        # ë¬´í•œ ìž¬ê·€ ë°©ì§€ í”Œëž˜ê·¸ í™•ì¸
        if hasattr(self, '_setting_init_image') and self._setting_init_image:
            print(f"âš ï¸ set_init_image ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€ë¨")
            return
        
        try:
            self._setting_init_image = True
            
            if image is not None:
                print(f"ðŸ” ì´ë¯¸ì§€ ì •ë³´: í¬ê¸°={image.size}, ëª¨ë“œ={image.mode}")
                # ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì˜êµ¬ ë³´ì¡´
                self._state['init_image'] = image
                print(f"âœ… init_image ì˜êµ¬ ë³´ì¡´ ì™„ë£Œ: {image.size}")
                
                # ì €ìž¥ í›„ í™•ì¸
                saved_image = self._state.get('init_image')
                print(f"ðŸ” ì €ìž¥ í›„ í™•ì¸: íƒ€ìž…={type(saved_image)}, í¬ê¸°={saved_image.size if saved_image else 'None'}")
                
                # ì´ë²¤íŠ¸ ë°œìƒ ì œí•œ (ë¬´í•œ ìž¬ê·€ ë°©ì§€)
                if not hasattr(self, '_init_image_event_sent'):
                    self._notify('init_image_changed', {'status': 'success', 'size': image.size})
                    self._init_image_event_sent = True
                    print(f"âœ… init_image_changed ì´ë²¤íŠ¸ ë°œìƒ")
            else:
                print(f"âš ï¸ init_imageê°€ Noneìœ¼ë¡œ ì„¤ì •ë¨")
                self._state['init_image'] = None
                
                # ì´ë²¤íŠ¸ ë°œìƒ ì œí•œ
                if not hasattr(self, '_init_image_event_sent'):
                    self._notify('init_image_changed', {'status': 'cleared'})
                    self._init_image_event_sent = True
                    print(f"âœ… init_image_changed ì´ë²¤íŠ¸ ë°œìƒ (cleared)")
                    
        except Exception as e:
            print(f"âŒ set_init_image ì˜¤ë¥˜: {e}")
        finally:
            self._setting_init_image = False
    
    def set_generated_images(self, images):
        """ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ ì„¤ì • (ë…ë¦½ ê´€ë¦¬) - ë¬´í•œ ìž¬ê·€ ë°©ì§€"""
        print(f"ðŸ” set_generated_images í˜¸ì¶œ: ì´ë¯¸ì§€ ê°œìˆ˜={len(images) if images else 0}")
        
        # ë¬´í•œ ìž¬ê·€ ë°©ì§€ í”Œëž˜ê·¸ í™•ì¸
        if hasattr(self, '_setting_generated_images') and self._setting_generated_images:
            print(f"âš ï¸ set_generated_images ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€ë¨")
            return
        
        try:
            self._setting_generated_images = True
            
            if images:
                self._state['generated_images'] = images
                print(f"âœ… generated_images ë…ë¦½ ì €ìž¥ ì™„ë£Œ: {len(images)}ê°œ")
                
                # ì´ë²¤íŠ¸ ë°œìƒ ì œí•œ
                if not hasattr(self, '_generated_images_event_sent'):
                    self._notify('generated_images_changed', {'count': len(images)})
                    self._generated_images_event_sent = True
                    print(f"âœ… generated_images_changed ì´ë²¤íŠ¸ ë°œìƒ")
            else:
                self._state['generated_images'] = []
                
                # ì´ë²¤íŠ¸ ë°œìƒ ì œí•œ
                if not hasattr(self, '_generated_images_event_sent'):
                    self._notify('generated_images_changed', {'count': 0})
                    self._generated_images_event_sent = True
                    print(f"âœ… generated_images_changed ì´ë²¤íŠ¸ ë°œìƒ (cleared)")
                    
        except Exception as e:
            print(f"âŒ set_generated_images ì˜¤ë¥˜: {e}")
        finally:
            self._setting_generated_images = False
    
    def get_init_image(self):
        """ì—…ë¡œë“œëœ ì›ë³¸ ì´ë¯¸ì§€ ì¡°íšŒ"""
        return self._state.get('init_image')
    
    def get_generated_images(self):
        """ìƒì„±ëœ ê²°ê³¼ ì´ë¯¸ì§€ë“¤ ì¡°íšŒ"""
        return self._state.get('generated_images', [])
    
    def clear_generated_images(self):
        """ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ ì´ˆê¸°í™” - ë¬´í•œ ìž¬ê·€ ë°©ì§€"""
        if not hasattr(self, '_clearing_generated_images') or not self._clearing_generated_images:
            try:
                self._clearing_generated_images = True
                self._state['generated_images'] = []
                
                # ì´ë²¤íŠ¸ í”Œëž˜ê·¸ ì´ˆê¸°í™”
                if hasattr(self, '_generated_images_event_sent'):
                    delattr(self, '_generated_images_event_sent')
                
                self._notify('generated_images_changed', {'count': 0})
                print(f"âœ… generated_images ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ clear_generated_images ì˜¤ë¥˜: {e}")
            finally:
                self._clearing_generated_images = False
    
    def preserve_init_image(self):
        """ì›ë³¸ ì´ë¯¸ì§€ ë³´ì¡´ í™•ì¸ - ìƒì„± ê³¼ì •ì—ì„œ í˜¸ì¶œ"""
        init_image = self.get('init_image')
        if init_image:
            print(f"âœ… ì›ë³¸ ì´ë¯¸ì§€ ë³´ì¡´ í™•ì¸: {init_image.size}")
            return True
        else:
            print(f"âš ï¸ ì›ë³¸ ì´ë¯¸ì§€ê°€ ì—†ìŒ - ë³´ì¡´í•  ì´ë¯¸ì§€ ì—†ìŒ")
            return False
    
    def ensure_image_state_preservation(self):
        """ì´ë¯¸ì§€ ìƒíƒœ ë³´ì¡´ ê°•í™” - ìƒì„± ì „í›„ í˜¸ì¶œ"""
        try:
            print(f"ðŸ”„ ì´ë¯¸ì§€ ìƒíƒœ ë³´ì¡´ ê°•í™” ì‹œìž‘")
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë³´ì¡´ í™•ì¸
            init_image = self.get('init_image')
            if init_image:
                print(f"âœ… ì›ë³¸ ì´ë¯¸ì§€ ë³´ì¡´ë¨: {init_image.size}")
            else:
                print(f"â„¹ï¸ ì›ë³¸ ì´ë¯¸ì§€ ì—†ìŒ")
            
            # ìƒì„±ëœ ì´ë¯¸ì§€ í™•ì¸
            generated_images = self.get('generated_images', [])
            if generated_images:
                print(f"âœ… ìƒì„±ëœ ì´ë¯¸ì§€ ë³´ì¡´ë¨: {len(generated_images)}ê°œ")
            else:
                print(f"â„¹ï¸ ìƒì„±ëœ ì´ë¯¸ì§€ ì—†ìŒ")
            
            # ì´ë²¤íŠ¸ í”Œëž˜ê·¸ ì´ˆê¸°í™” (ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë°œìƒ í—ˆìš©)
            self.reset_image_events()
            
            print(f"âœ… ì´ë¯¸ì§€ ìƒíƒœ ë³´ì¡´ ê°•í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ìƒíƒœ ë³´ì¡´ ê°•í™” ì¤‘ ì˜¤ë¥˜: {e}")
    
    def reset_image_events(self):
        """ì´ë¯¸ì§€ ê´€ë ¨ ì´ë²¤íŠ¸ í”Œëž˜ê·¸ ì´ˆê¸°í™”"""
        if hasattr(self, '_init_image_event_sent'):
            delattr(self, '_init_image_event_sent')
        if hasattr(self, '_generated_images_event_sent'):
            delattr(self, '_generated_images_event_sent')
        print(f"âœ… ì´ë¯¸ì§€ ì´ë²¤íŠ¸ í”Œëž˜ê·¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def update_param(self, param_name: str, value: Any):
        """íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        current_params = self.get('current_params')
        if hasattr(current_params, param_name):
            setattr(current_params, param_name, value)
            print(f"âœ… {param_name} ì ìš©: {value}")
            self._notify('params_updated', {param_name: value})

    def update_prompt(self, positive_prompt: str, negative_prompt: str):
        """í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸"""
        current_params = self.get('current_params')
        current_params.prompt = positive_prompt
        current_params.negative_prompt = negative_prompt
        self._notify('prompt_updated', {'positive': positive_prompt, 'negative': negative_prompt})

    # --- ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ ---
    def subscribe(self, event: str, callback: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        if event not in self._observers:
            self._observers[event] = []
        self._observers[event].append(callback)

    def unsubscribe(self, event: str, callback: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë… í•´ì œ (ì•ˆì „í•œ ì œê±°)"""
        if event in self._observers:
            try:
                if callback in self._observers[event]:
                    self._observers[event].remove(callback)
                    print(f"âœ… ì´ë²¤íŠ¸ '{event}' êµ¬ë… í•´ì œ ì™„ë£Œ")
                else:
                    print(f"âš ï¸ ì´ë²¤íŠ¸ '{event}'ì—ì„œ ì½œë°±ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì´ë¯¸ í•´ì œë¨)")
            except ValueError:
                print(f"âš ï¸ ì´ë²¤íŠ¸ '{event}' êµ¬ë… í•´ì œ ì¤‘ ì˜¤ë¥˜ (ì½œë°±ì´ ë¦¬ìŠ¤íŠ¸ì— ì—†ìŒ)")
            except Exception as e:
                print(f"âŒ ì´ë²¤íŠ¸ '{event}' êµ¬ë… í•´ì œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

    def _notify(self, event: str, data: Any = None):
        """ì´ë²¤íŠ¸ ë°œìƒ"""
        if event in self._observers:
            for callback in self._observers[event]:
                try:
                    # async í•¨ìˆ˜ì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆížˆ ì²˜ë¦¬
                    if asyncio.iscoroutinefunction(callback):
                        # async í•¨ìˆ˜ëŠ” ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
                        asyncio.create_task(callback(data))
                    else:
                        # ë™ê¸° í•¨ìˆ˜ëŠ” ì§ì ‘ í˜¸ì¶œ
                        callback(data)
                except Exception as e:
                    print(f"âš ï¸ ì´ë²¤íŠ¸ ì½œë°± ì˜¤ë¥˜ ({event}): {e}")

    def _notify_user(self, message: str, notification_type: str = 'info', duration: int = 5):
        """ì‚¬ìš©ìž ì•Œë¦¼"""
        self._notify('user_notification', {
            'message': message,
            'type': notification_type,
            'duration': duration
        })

    # --- í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ (ë„ë©”ì¸ ì„œë¹„ìŠ¤ ìœ„ìž„) ---
    def copy_prompt_to_clipboard(self, positive_prompt: str = "", negative_prompt: str = ""):
        """í”„ë¡¬í”„íŠ¸ë¥¼ í´ë¦½ë³´ë“œì— ë³µì‚¬"""
        import pyperclip
        
        if positive_prompt and negative_prompt:
            full_prompt = f"{positive_prompt}\n\nNegative prompt: {negative_prompt}"
        elif positive_prompt:
            full_prompt = positive_prompt
        elif negative_prompt:
            full_prompt = f"Negative prompt: {negative_prompt}"
        else:
            return
        
        try:
            pyperclip.copy(full_prompt)
            print(f"ðŸ“‹ í´ë¦½ë³´ë“œì— ë³µì‚¬ë¨: {full_prompt[:50]}...")
            self._notify_user("í”„ë¡¬í”„íŠ¸ê°€ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.", 'positive')
        except Exception as e:
            print(f"âŒ í´ë¦½ë³´ë“œ ë³µì‚¬ ì‹¤íŒ¨: {e}")
            self._notify_user("í´ë¦½ë³´ë“œ ë³µì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", 'negative')

    def calculate_token_count(self, text: str) -> int:
        """ì‹¤ì œ CLIP í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ìˆ˜ ê³„ì‚°"""
        pipeline = self.model_loader.get_current_pipeline()
        if not text.strip() or not pipeline or not hasattr(pipeline, 'tokenizer'):
            return 0
        
        try:
            # í† í¬ë‚˜ì´ì €ë¡œ ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°
            text_inputs = pipeline.tokenizer(
                text,
                padding="longest",
                truncation=False,
                return_tensors="pt"
            )
            return len(text_inputs.input_ids[0])
        except Exception as e:
            print(f"âš ï¸ í† í° ê³„ì‚° ì˜¤ë¥˜: {e}")
            return len(text.split())  # í´ë°±: ë‹¨ì–´ ìˆ˜ë¡œ ì¶”ì •

    def analyze_prompt(self, prompt: str) -> Dict[str, Any]:
        """í”„ë¡¬í”„íŠ¸ ë¶„ì„ ë° ìµœì í™” ì œì•ˆ"""
        pipeline = self.model_loader.get_current_pipeline()
        tokenizer = getattr(pipeline, 'tokenizer', None) if pipeline else None
        analysis = self.prompt_processor.analyze_prompt(prompt, tokenizer)
        
        # PromptAnalysis ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        return {
            'token_count': analysis.token_count,
            'segments': analysis.segments,
            'weights': analysis.weights,
            'suggestions': analysis.suggestions,
            'is_optimized': analysis.is_optimized,
            'truncated_parts': analysis.truncated_parts
        }

    def add_break_keyword(self, prompt: str, position: int = None) -> str:
        """BREAK í‚¤ì›Œë“œ ì¶”ê°€"""
        if self.long_prompt_handler:
            return self.long_prompt_handler.add_break_keywords(prompt)
        return self.prompt_processor.add_break_keyword(prompt, position)
    
    def get_prompt_stats(self, prompt: str) -> Dict:
        """í”„ë¡¬í”„íŠ¸ í†µê³„ ì •ë³´ (LongPromptHandler ì‚¬ìš©)"""
        if self.long_prompt_handler:
            return self.long_prompt_handler.get_prompt_stats(prompt)
        
        # ê¸°ë³¸ í†µê³„ (LongPromptHandlerê°€ ì—†ëŠ” ê²½ìš°)
        total_tokens = self.calculate_token_count(prompt)
        return {
            'total_tokens': total_tokens,
            'max_tokens': self.prompt_processor.max_tokens,
            'chunks_count': 1,
            'is_truncated': total_tokens > self.prompt_processor.max_tokens,
            'truncation_ratio': total_tokens / self.prompt_processor.max_tokens if self.prompt_processor.max_tokens > 0 else 0,
            'chunks': [{'text': prompt, 'tokens': total_tokens, 'importance': 1.0}]
        }
    
    def optimize_long_prompt(self, prompt: str) -> str:
        """ê¸´ í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        if self.long_prompt_handler:
            return self.long_prompt_handler.optimize_prompt(prompt)
        return self.optimize_prompt(prompt)
    
    def split_long_prompt(self, prompt: str) -> List[Dict]:
        """ê¸´ í”„ë¡¬í”„íŠ¸ ë¶„í• """
        if self.long_prompt_handler:
            chunks = self.long_prompt_handler.smart_split(prompt)
            return [
                {
                    'text': chunk.text,
                    'tokens': len(chunk.tokens),
                    'importance': chunk.importance,
                    'start_pos': chunk.start_pos,
                    'end_pos': chunk.end_pos
                }
                for chunk in chunks
            ]
        return [{'text': prompt, 'tokens': self.calculate_token_count(prompt), 'importance': 1.0}]

    def apply_weight(self, keyword: str, weight: float = 1.1) -> str:
        """ê°€ì¤‘ì¹˜ ì ìš©"""
        return self.prompt_processor.apply_weight(keyword, weight)

    def optimize_prompt(self, prompt: str, target_tokens: int = 70) -> str:
        """í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        return self.prompt_processor.optimize_prompt(prompt, target_tokens)

    def _add_to_history(self, history_item: Dict[str, Any]):
        """ížˆìŠ¤í† ë¦¬ì— ì•„ì´í…œ ì¶”ê°€"""
        history = self.get('history', [])
        history.insert(0, history_item)  # ìµœì‹  í•­ëª©ì„ ë§¨ ì•žì— ì¶”ê°€
        
        # ì„¤ì •ì—ì„œ ížˆìŠ¤í† ë¦¬ ì œí•œ ê°€ì ¸ì˜¤ê¸°
        history_limit = self.config.get('ui', {}).get('history_limit', 50)
        history = history[:history_limit]  # ì œí•œëœ ê°œìˆ˜ë§Œ ìœ ì§€
        
        self.set('history', history)
        
        # ížˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ë°œìƒ
        self._notify('history_updated', history)
        
        print(f"ðŸ“‹ ížˆìŠ¤í† ë¦¬ì— ì¶”ê°€ë¨: {history_item.get('model', 'Unknown')}")
    
    async def _notify_async(self, event: str, data: Any = None):
        """ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë°œìƒ"""
        self._notify(event, data)

    def get_history(self) -> List[Dict[str, Any]]:
        """ížˆìŠ¤í† ë¦¬ ëª©ë¡ ë°˜í™˜"""
        return self.get('history', [])
    
    def clear_history(self):
        """ížˆìŠ¤í† ë¦¬ ì „ì²´ ì‚­ì œ"""
        self.set('history', [])
        self._notify('history_updated', [])
        self._notify_user('ížˆìŠ¤í† ë¦¬ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')
    
    def clear_all_history(self):
        """ì „ì²´ ížˆìŠ¤í† ë¦¬ ì‚­ì œ (ë³„ì¹­)"""
        self.clear_history()
    
    def delete_history_item(self, history_id: str):
        """ížˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì‚­ì œ"""
        history = self.get('history', [])
        history = [item for item in history if item.get('id') != history_id]
        self.set('history', history)
        self._notify('history_updated', history)
        self._notify_user('ížˆìŠ¤í† ë¦¬ ì•„ì´í…œì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')
    
    # --- LoRA ê´€ë ¨ ë©”ì„œë“œë“¤ ---
    async def load_lora(self, lora_info: Dict[str, Any], weight: float = 1.0) -> bool:
        """LoRA ë¡œë“œ"""
        try:
            success = await self.model_loader.load_lora(lora_info, weight)
            if success:
                # ë¡œë“œëœ LoRA ëª©ë¡ ì—…ë°ì´íŠ¸
                loaded_loras = self.model_loader.get_loaded_loras()
                self.set('loaded_loras', loaded_loras)
                self._notify('loras_updated', loaded_loras)
                self._notify_user(f"LoRA '{lora_info['name']}' ë¡œë“œ ì™„ë£Œ", 'positive')
                return True
            else:
                self._notify_user(f"LoRA '{lora_info['name']}' ë¡œë“œ ì‹¤íŒ¨", 'negative')
                return False
        except Exception as e:
            print(f"âŒ LoRA ë¡œë“œ ì˜¤ë¥˜: {e}")
            self._notify_user(f"LoRA ë¡œë“œ ì˜¤ë¥˜: {str(e)}", 'negative')
            return False
    
    async def unload_lora(self, lora_name: str) -> bool:
        """íŠ¹ì • LoRA ì–¸ë¡œë“œ"""
        try:
            success = await self.model_loader.unload_lora(lora_name)
            if success:
                # ë¡œë“œëœ LoRA ëª©ë¡ ì—…ë°ì´íŠ¸
                loaded_loras = self.model_loader.get_loaded_loras()
                self.set('loaded_loras', loaded_loras)
                self._notify('loras_updated', loaded_loras)
                self._notify_user(f"LoRA '{lora_name}' ì–¸ë¡œë“œ ì™„ë£Œ", 'positive')
                return True
            else:
                self._notify_user(f"LoRA '{lora_name}' ì–¸ë¡œë“œ ì‹¤íŒ¨", 'negative')
                return False
        except Exception as e:
            print(f"âŒ LoRA ì–¸ë¡œë“œ ì˜¤ë¥˜: {e}")
            self._notify_user(f"LoRA ì–¸ë¡œë“œ ì˜¤ë¥˜: {str(e)}", 'negative')
            return False
    
    async def unload_all_loras(self) -> bool:
        """ëª¨ë“  LoRA ì–¸ë¡œë“œ"""
        try:
            success = await self.model_loader.unload_all_loras()
            if success:
                # ë¡œë“œëœ LoRA ëª©ë¡ ì—…ë°ì´íŠ¸
                loaded_loras = self.model_loader.get_loaded_loras()
                self.set('loaded_loras', loaded_loras)
                self._notify('loras_updated', loaded_loras)
                self._notify_user("ëª¨ë“  LoRA ì–¸ë¡œë“œ ì™„ë£Œ", 'positive')
                return True
            else:
                self._notify_user("ëª¨ë“  LoRA ì–¸ë¡œë“œ ì‹¤íŒ¨", 'negative')
                return False
        except Exception as e:
            print(f"âŒ ëª¨ë“  LoRA ì–¸ë¡œë“œ ì˜¤ë¥˜: {e}")
            self._notify_user(f"ëª¨ë“  LoRA ì–¸ë¡œë“œ ì˜¤ë¥˜: {str(e)}", 'negative')
            return False
    
    def get_loaded_loras(self) -> List[Dict[str, Any]]:
        """ë¡œë“œëœ LoRA ëª©ë¡ ë°˜í™˜"""
        return self.model_loader.get_loaded_loras()