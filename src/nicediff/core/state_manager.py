# src/nicediff/core/state_manager.py
# (ë„ë©”ì¸ ì£¼ë„ ì„¤ê³„ ì›ì¹™ì— ë”°ë¼ ì •ë¦¬ëœ ë²„ì „)

import asyncio
import json
import tomllib
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import torch

from ..services.model_scanner import ModelScanner
from ..services.metadata_parser import MetadataParser
from ..services.tokenizer_manager import TokenizerManager
from ..domains.generation.models.generation_params import GenerationParams
from ..domains.generation.models.history_item import HistoryItem
from ..domains.generation.services.model_loader import ModelLoader
from ..domains.generation.services.image_saver import ImageSaver
from ..domains.generation.strategies.basic_strategy import BasicGenerationStrategy
from ..domains.generation.processors.prompt_processor import PromptProcessor


class StateManager:
    """ì¤‘ì•™ ìƒíƒœ ê´€ë¦¬ì (ë„ë©”ì¸ ì£¼ë„ ì„¤ê³„ ì›ì¹™ ì ìš©)"""
    
    def __init__(self):
        self._state: Dict[str, Any] = {
            'current_model_info': None,
            'current_vae_path': 'baked_in',  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë‚´ì¥ VAE ì‚¬ìš©
            'current_loras': [],
            'current_params': GenerationParams(),
            'available_checkpoints': {},
            'available_vae': {},
            'available_loras': {},
            'history': [],
            'is_generating': False,
            'is_loading_model': False,
            'model_type': 'SD15',
            'is_xl_model': False,
            'sd_model': 'SD15',  # ì¶”ê°€: UIì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ íƒ€ì…
            'status_message': 'ì¤€ë¹„ ì¤‘...',
            'infinite_mode': False,  # ë¬´í•œ ìƒì„± ëª¨ë“œ
            'current_mode': 'txt2img',  # í˜„ì¬ ìƒì„± ëª¨ë“œ (txt2img, img2img, inpaint, upscale)
        }
        
        # í¬ê¸° ì¼ì¹˜ í† ê¸€ ê¸°ë³¸ê°’ ì„¤ì •
        self._state['current_params'].size_match_enabled = False
        self._observers: Dict[str, List[Callable]] = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.stop_generation_flag = asyncio.Event()
        
        # ë„ë©”ì¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.model_loader = ModelLoader(self.device)
        self.image_saver = ImageSaver()
        self.tokenizer_manager = None  # initializeì—ì„œ ì„¤ì •
        self.prompt_processor = PromptProcessor()
        
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
    
    async def initialize(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ëª¨ë¸ ìŠ¤ìº” ì‹œì‘"""
        config_path = Path("config.toml")
        if await asyncio.to_thread(config_path.exists):
            with open(config_path, "rb") as f:
                self.config = tomllib.load(f)
        else:
            # config.tomlì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            self.config = {
                'paths': {
                    'checkpoints': 'models/checkpoints',
                    'vae': 'models/vae',
                    'loras': 'models/loras'
                }
            }
            print("âš ï¸ config.tomlì´ ì—†ì–´ ê¸°ë³¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # í† í¬ë‚˜ì´ì € ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.tokenizer_manager = TokenizerManager(self.config.get('paths', {}).get('tokenizers', 'models/tokenizers'))
        
        # NOTE: ëª¨ë¸ ìŠ¤ìº”ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì‹¤í–‰
        asyncio.create_task(self._scan_models())
    
    async def _scan_models(self):
        """ModelScannerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ìŠ¤ìº”í•˜ê³ , í‘œì¤€í™”ëœ í‚¤ë¡œ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        self.set('status_message', 'ëª¨ë¸ ìŠ¤ìº” ì¤‘...')
        paths_config = self.config.get('paths', {})
        scanner = ModelScanner(paths_config=paths_config)
        all_models_data = await scanner.scan_all_models()
        
        # ì´ì œ ìŠ¤ìºë„ˆê°€ 'checkpoints' í‚¤ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, ì´ ì½”ë“œê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.
        self.set('available_checkpoints', all_models_data.get('checkpoints', {}))
        self.set('available_vae', all_models_data.get('vae', {}))
        self.set('available_loras', all_models_data.get('loras', {}))        

        # í† í¬ë‚˜ì´ì € ìŠ¤ìº”
        if self.tokenizer_manager:
            self.tokenizer_manager.scan_tokenizers()
        
        self.set('status_message', 'ì¤€ë¹„ ì™„ë£Œ')
        print("âœ… StateManager: ìŠ¤ìº” ê²°ê³¼ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

    # --- ëª¨ë¸ ì„ íƒ ë° ë¡œë”© ---
    async def select_model(self, model_info: Dict[str, Any]):
        """1ë‹¨ê³„: GPU ë¡œë”© ì—†ì´, ë©”íƒ€ë°ì´í„° í‘œì‹œë¥¼ ìœ„í•´ ëª¨ë¸ì„ 'ì„ íƒ'ë§Œ í•©ë‹ˆë‹¤."""
        print(f"ëª¨ë¸ ì„ íƒë¨ (ë¯¸ë¦¬ë³´ê¸°ìš©): {model_info['name']}")
        self.set('current_model_info', model_info)
        self.set('sd_model_type', model_info.get('model_type', 'SD15'))
        
        # ì„ íƒ ì´ë²¤íŠ¸ ë°œìƒ
        self._notify('model_selection_changed', model_info)

    async def load_model_pipeline(self, model_info: Dict[str, Any]) -> bool:
        """[ìµœì¢… ìˆ˜ì •] ëª¨ë¸ ë¡œë”©ì˜ ëª¨ë“  ê³¼ì •ì„ ì±…ì„ì§€ëŠ” ì¤‘ì•™ ì²˜ë¦¬ ë©”ì„œë“œ"""
    
        # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì´ë©´ ì¤‘ë‹¨
        current_model_info = self.get('current_model_info')
        if (current_model_info and 
            current_model_info.get('path') == model_info.get('path') and 
            self.model_loader.get_current_pipeline() is not None):
            self._notify_user(f"'{model_info['name']}' ëª¨ë¸ì€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.", 'info')
            return True

        try:
            self.set('is_loading_model', True)
            self._notify('model_loading_started', {'name': model_info['name']})
            
            # ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
            self.model_loader.unload_model()
            
            # ë„ë©”ì¸ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë¡œë“œ
            await self.model_loader.load_model(model_info)
            self.set('current_model_info', model_info)
            
            # VAE ìë™ ì„ íƒì€ ì‚¬ìš©ìê°€ 'Automatic'ì„ ì„ íƒí–ˆì„ ë•Œë§Œ ì‹¤í–‰
            current_vae_path = self.get('current_vae_path')
            if current_vae_path is None:
                await self._auto_select_vae(model_info)
            
            # Top_barì—ì„œëŠ” íŒŒë¼ë¯¸í„° ìë™ ì ìš© ê¸ˆì§€ - í”„ë¡¬í”„íŠ¸ íŒ¨ë„ê³¼ íŒŒë¼ë¯¸í„° íŒ¨ë„ì—ì„œë§Œ ì„¤ì •
            print(f"â„¹ï¸ íŒŒë¼ë¯¸í„° ìë™ ì ìš© ë¹„í™œì„±í™” (Top_barì—ì„œëŠ” ì²´í¬í¬ì¸íŠ¸ì™€ VAEë§Œ ì„ íƒ)")
            
            # ëª¨ë¸ ë¡œë”© ì™„ë£Œ ì•Œë¦¼ (ì„ íƒ ì•Œë¦¼ì€ ì´ë¯¸ select_modelì—ì„œ ë°œìƒí–ˆìœ¼ë¯€ë¡œ ìƒëµ)
            self._notify('model_loaded', model_info)
            
            self._notify('model_loading_finished', {'success': True, 'model_info': model_info})
            self._notify_user(f"'{model_info['name']}' ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.", 'positive')
            return True
        
        except Exception as e:
            import traceback
            traceback.print_exc()
            error_msg = f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            self._notify_user(error_msg, 'negative')
            self._notify('model_loading_finished', {'success': False, 'error': str(e)})
            return False
        
        finally:
            self.set('is_loading_model', False)

    async def _load_model_heavy_work(self, model_info: Dict[str, Any]):
        """ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‘ì—… (Heavy I/O)"""
        def _load():
            model_path = model_info['path']
            model_type = model_info.get('model_type', 'SD15')
            
            print(f"ğŸ” ëª¨ë¸ íƒ€ì…: {model_type}, ê²½ë¡œ: {model_path}")
            
            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ íŒŒì´í”„ë¼ì¸ ì„ íƒ
            if model_type == 'SDXL':
                from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline
                pipeline = StableDiffusionXLPipeline.from_single_file(
                    model_path,
                    torch_dtype=torch.float16,
                    use_safetensors=True,
                    safety_checker=None
                )
            else:
                # SD15 ëª¨ë¸
                from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipeline
                pipeline = StableDiffusionPipeline.from_single_file(
                    model_path,
                    torch_dtype=torch.float16,
                    use_safetensors=True,
                    safety_checker=None,
                    requires_safety_checker=False
                )
            
            # GPUë¡œ ì´ë™
            pipeline = pipeline.to(self.device)
            
            # SD15 íŠ¹í™” ì„¤ì •
            if model_type == 'SD15':
                # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ì„¤ì •
                if hasattr(pipeline, 'enable_attention_slicing'):
                    pipeline.enable_attention_slicing(1)
                if hasattr(pipeline, 'enable_vae_slicing'):
                    pipeline.enable_vae_slicing()
                
                # PyTorch 2.0+ SDPA ìµœì í™” (xformers ëŒ€ì‹  ì‚¬ìš©)
                if hasattr(pipeline, 'enable_model_cpu_offload'):
                    pipeline.enable_model_cpu_offload()
                
                # xformersëŠ” ì„ íƒì  ê¸°ëŠ¥ì´ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                try:
                    if hasattr(pipeline, 'enable_xformers_memory_efficient_attention'):
                        pipeline.enable_xformers_memory_efficient_attention()
                        print("âœ… xformers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ í™œì„±í™”")
                except (ModuleNotFoundError, AttributeError) as e:
                    print(f"âš ï¸ xformers ë¯¸ì‚¬ìš©: {e}")
                    print("âœ… PyTorch 2.0+ SDPA ì‚¬ìš© ì¤‘")
                
                # SD15 í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ ì¶”ê°€ ì„¤ì •
                if hasattr(pipeline, 'scheduler') and hasattr(pipeline.scheduler, 'config'):
                    # Karras ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™”
                    pipeline.scheduler.config.use_karras_sigmas = True
                    pipeline.scheduler.config.karras_rho = 7.0
                    
                    # SD15ì—ì„œ ë” ì•ˆì •ì ì¸ ìƒì„±
                    if hasattr(pipeline.scheduler.config, 'beta_start'):
                        pipeline.scheduler.config.beta_start = 0.00085
                    if hasattr(pipeline.scheduler.config, 'beta_end'):
                        pipeline.scheduler.config.beta_end = 0.012
                
                # ëª¨ë¸ ì •ë°€ë„ ìµœì í™”
                if hasattr(pipeline, 'text_encoder'):
                    pipeline.text_encoder = pipeline.text_encoder.to(torch.float16)
                if hasattr(pipeline, 'vae'):
                    pipeline.vae = pipeline.vae.to(torch.float16)
                if hasattr(pipeline, 'unet'):
                    pipeline.unet = pipeline.unet.to(torch.float16)
                
                print("âœ… SD15 í’ˆì§ˆ ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")
            else:
                # SDXL ì„¤ì •
                if hasattr(pipeline, 'enable_attention_slicing'):
                    pipeline.enable_attention_slicing()
                if hasattr(pipeline, 'enable_vae_slicing'):
                    pipeline.enable_vae_slicing()
            
            return pipeline
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¡œë”© ìˆ˜í–‰
        self.pipeline = await asyncio.to_thread(_load)
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_info['name']}")

    async def _unload_current_model(self):
        """ê¸°ì¡´ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ"""
        if self.pipeline is not None:
            print(f"ğŸ—‘ï¸ ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
            try:
                # float16 ëª¨ë¸ì€ CPUë¡œ ì´ë™í•˜ì§€ ì•Šê³  ì§ì ‘ ì‚­ì œ
                del self.pipeline
                self.pipeline = None
                
                # CUDA ìºì‹œ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                self.pipeline = None

    async def _auto_select_vae(self, model_info: Dict[str, Any]):
        """A1111 ë°©ì‹ì˜ VAE ìë™ ì„ íƒ ë¡œì§"""
        print(f"ğŸ” VAE ìë™ ì„ íƒ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ê¶Œì¥ VAE ì°¾ê¸°
        metadata = model_info.get('metadata', {})
        recommended_vae = metadata.get('recommended_vae')
        
        if recommended_vae:
            # ì¶”ì²œ VAEê°€ ìˆìœ¼ë©´ ë¡œë“œ ì‹œë„
            available_vae = self.get('available_vae', {})
            for folder_vae in available_vae.values():
                for vae_info in folder_vae:
                    if recommended_vae.lower() in vae_info['name'].lower():
                        print(f"âœ… ì¶”ì²œ VAE ë°œê²¬: {vae_info['name']}")
                        await self.load_vae(vae_info['path'])
                        return
        
        # SD15 ëª¨ë¸ì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” VAEë“¤ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ì ìš©
        model_type = model_info.get('model_type', 'SD15')
        if model_type == 'SD15':
            available_vae = self.get('available_vae', {})
            preferred_vae_names = [
                'vaeFtMse840000EmaPruned',  # ê°€ì¥ ì¼ë°˜ì ì¸ SD15 VAE
                'vae-ft-mse-840000-ema-pruned',
                'vaeFtMse840k',
                'vae-ft-mse-840k',
                'vaeRealanimark',
                'vaeRealanimark_vaeRealanimark'
            ]
            
            for preferred_name in preferred_vae_names:
                for folder_vae in available_vae.values():
                    for vae_info in folder_vae:
                        if preferred_name.lower() in vae_info['name'].lower():
                            print(f"âœ… SD15 ê¶Œì¥ VAE ë°œê²¬: {vae_info['name']}")
                            await self.load_vae(vae_info['path'])
                            return
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ë‚´ì¥ VAE ì‚¬ìš©
        print("â„¹ï¸ ì²´í¬í¬ì¸íŠ¸ ë‚´ì¥ VAE ì‚¬ìš© (ë³„ë„ VAE ì—†ìŒ)")
        self.set('current_vae_path', 'baked_in')
        self._notify('vae_auto_selected', 'baked_in')

    async def load_vae(self, vae_path: str):
        """VAE íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì— ì ìš©"""
        if not self.model_loader.get_current_pipeline():
            self._notify_user('ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.', 'warning')
            return False
        
        self.set('is_loading_model', True) # VAE ë¡œë”©ë„ ë¡œë”© ìƒíƒœë¡œ ê°„ì£¼
        self._notify_user(f"VAE '{Path(vae_path).name}' ë¡œë“œ ì¤‘...", 'info')
        
        try:
            # ë„ë©”ì¸ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ VAE ë¡œë“œ
            success = await self.model_loader.load_vae(vae_path)
            
            if success:
                self.set('current_vae_path', vae_path)
                self._notify_user(f"VAE '{Path(vae_path).name}' ì ìš© ì™„ë£Œ!", 'positive')
                return True
            else:
                self._notify_user(f"VAE ë¡œë“œ ì‹¤íŒ¨", 'negative')
                return False
        except Exception as e:
            print(f"âŒ VAE ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._notify_user(f"VAE ë¡œë“œ ì‹¤íŒ¨: {e}", 'negative')
            return False
        finally:
            self.set('is_loading_model', False)

    async def generate_image(self):
        """ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰ (ë„ë©”ì¸ ë¡œì§ ì‚¬ìš©)"""
        if self.get('is_generating'): 
            self._notify_user('ì´ë¯¸ ìƒì„± ì¤‘ì…ë‹ˆë‹¤.', 'warning')
            return
        
        if not self.model_loader.get_current_pipeline():
            self._notify_user('ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'warning')
            return
        
        self.stop_generation_flag.clear()
        self.set('is_generating', True)
        # ìƒì„± ì‹œì‘ ì´ë²¤íŠ¸ëŠ” ì‹¤ì œ ìƒì„±ì´ ì‹œì‘ë  ë•Œë§Œ ë°œìƒ
        # self._notify('generation_started', {})  # ì—¬ê¸°ì„œëŠ” ì œê±°
        
        try:
            # ë„ë©”ì¸ ì „ëµ ì‚¬ìš©
            pipeline = self.model_loader.get_current_pipeline()
            strategy = BasicGenerationStrategy(pipeline, self.device, state=self)
            
            # íŒŒë¼ë¯¸í„° ì¤€ë¹„
            params = self.get('current_params')
            params_dict = {
                'prompt': params.prompt,
                'negative_prompt': params.negative_prompt,
                'width': params.width,
                'height': params.height,
                'steps': params.steps,
                'cfg_scale': params.cfg_scale,
                'seed': params.seed,
                'sampler': params.sampler,
                'scheduler': params.scheduler,
                'batch_size': params.batch_size,
                'clip_skip': getattr(params, 'clip_skip', 1),  # CLIP Skip ì¶”ê°€
                'strength': getattr(params, 'strength', 0.8),  # i2i Strength ì¶”ê°€
                'vae': self.get('current_vae_path'),
                'loras': self.get('current_loras')
            }
            
            # i2i ëª¨ë“œ ì²˜ë¦¬
            current_mode = self.get('current_mode', 'txt2img')
            if current_mode in ['img2img', 'inpaint', 'upscale']:
                params_dict['img2img_mode'] = True
                # init_imageëŠ” ImagePadì—ì„œ ì„¤ì •ë˜ì–´ì•¼ í•¨
                init_image = self.get('init_image')
                print(f"ğŸ” StateManagerì—ì„œ init_image í™•ì¸: {init_image}")
                if init_image:
                    print(f"âœ… init_image í™•ì¸ë¨: {type(init_image)}, {init_image.size}")
                else:
                    print(f"âŒ init_imageê°€ Noneì…ë‹ˆë‹¤!")
                    # ê¸€ë¡œë²Œ ìƒíƒœì—ì„œ ì´ë¯¸ì§€ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
                    init_image = self._state.get('init_image')
                    if init_image:
                        print(f"ğŸ”„ ê¸€ë¡œë²Œ ìƒíƒœì—ì„œ init_image ë³µêµ¬: {init_image.size}")
                    else:
                        mode_display = {
                            'img2img': 'ì´ë¯¸ì§€ â†’ ì´ë¯¸ì§€',
                            'inpaint': 'ì¸í˜ì¸íŒ…',
                            'upscale': 'ì—…ìŠ¤ì¼€ì¼'
                        }.get(current_mode, current_mode)
                        self._notify_user(f'{mode_display} ëª¨ë“œì—ì„œëŠ” ì´ˆê¸° ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ íŒ¨ë“œì—ì„œ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.', 'warning')
                        # ì¦‰ì‹œ ì¢…ë£Œí•˜ê³  finally ë¸”ë¡ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
                        self.set('is_generating', False)
                        return
                
                # ë””ë²„ê·¸: ì´ë¯¸ì§€ ì •ë³´ ì¶œë ¥
                if hasattr(init_image, 'size'):
                    print(f"ğŸ” ì „ë‹¬í•  ì´ë¯¸ì§€ í¬ê¸°: {init_image.size}, ëª¨ë“œ: {init_image.mode}")
                else:
                    print(f"âŒ ì´ë¯¸ì§€ ê°ì²´ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ: {type(init_image)}")
                
                params_dict['init_image'] = init_image
                print(f"ğŸ¨ {current_mode} ëª¨ë“œ í™œì„±í™” - Strength: {params_dict['strength']}")
            else:
                params_dict['img2img_mode'] = False
                print("ğŸ¨ txt2img ëª¨ë“œ í™œì„±í™”")
            
            # ë°˜ë³µ ìƒì„± ì²˜ë¦¬
            iterations = int(params.iterations)
            import random
            base_seed = params.seed if params.seed > 0 else random.randint(0, 2**32 - 1)
            
            for i in range(iterations):
                if self.stop_generation_flag.is_set():
                    self._notify_user('ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')
                    break
                
                # ì‹œë“œ ì—…ë°ì´íŠ¸
                current_seed = base_seed + i
                params_dict['seed'] = current_seed
                
                print(f"ğŸ¨ ìƒì„± ì‹œì‘ (Iteration {i+1}/{iterations}) - Seed: {current_seed}")
                
                # ì‹¤ì œ ìƒì„± ì‹œì‘ ì‹œì—ë§Œ ì´ë²¤íŠ¸ ë°œìƒ
                if i == 0:  # ì²« ë²ˆì§¸ ë°˜ë³µì—ì„œë§Œ
                    self._notify('generation_started', {})
                
                # ì „ëµ ì‹¤í–‰
                result = await strategy.execute(params_dict, self.get('current_model_info'))
                
                if result.success:
                    self._notify_user(f'{len(result.images)}ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!', 'positive')
                    
                    # ë„ë©”ì¸ ì „ëµì—ì„œ ì´ë¯¸ ì €ì¥ëœ ì´ë¯¸ì§€ë“¤ì˜ ì´ë²¤íŠ¸ ë°œìƒ
                    for i, post_result in enumerate(result.post_results):
                        if post_result.success:
                            # ì´ë¯¸ ì €ì¥ëœ ì´ë¯¸ì§€ì´ë¯€ë¡œ ì´ë²¤íŠ¸ë§Œ ë°œìƒ
                            self._notify('image_generated', {
                                'image_path': post_result.image_path,
                                'thumbnail_path': post_result.thumbnail_path,
                                'params': params,
                                'seed': current_seed
                            })
                            
                            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                            history_item = HistoryItem(
                                image_path=post_result.image_path,
                                thumbnail_path=post_result.thumbnail_path,
                                params=params,
                                model=self.get('current_model_info')['name'],
                                vae=self.get('current_vae_path'),
                                loras=self.get('current_loras', [])
                            )
                            self._add_to_history(history_item.to_dict())
                    
                else:
                    self._notify_user(f'ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {result.error}', 'negative')
                    break
                    
        except Exception as e:
            import traceback
            traceback.print_exc()
            self._notify_user(f'ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}', 'negative')
        finally:
            self.set('is_generating', False)
            self._notify('generation_finished', {})

    async def finish_generation(self, image, params: GenerationParams, seed: int):
        """ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ í›„ì²˜ë¦¬ (ë„ë©”ì¸ ì„œë¹„ìŠ¤ ì‚¬ìš©)"""
        try:
            # ë„ë©”ì¸ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì €ì¥
            model_name = self.get('current_model_info')['name']
            save_result = await self.image_saver.save_generated_image(image, params, seed, model_name)
            
            # ì´ë²¤íŠ¸ ë°œìƒ
            self._notify('image_generated', {
                'image_path': save_result['image_path'],
                'thumbnail_path': save_result['thumbnail_path'],
                'params': params,
                'seed': seed
            })
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            history_item = HistoryItem(
                image_path=save_result['image_path'],
                thumbnail_path=save_result['thumbnail_path'],
                params=params,
                model=model_name,
                vae=self.get('current_vae_path'),
                loras=self.get('current_loras', [])
            )
            self._add_to_history(history_item.to_dict())
            
            print(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ: 1ê°œ ì´ë¯¸ì§€ ì €ì¥")
            
        except Exception as e:
            print(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    def _build_metadata_string(self, params: GenerationParams, seed: int) -> str:
        """PNG ë©”íƒ€ë°ì´í„° ë¬¸ìì—´ ìƒì„±"""
        model_name = self.get('current_model_info', {}).get('name', 'Unknown')
        vae_path = self.get('current_vae_path')
        
        # VAE ì •ë³´ ì¶”ê°€
        vae_info = ""
        if vae_path and vae_path != 'baked_in':
            vae_name = Path(vae_path).name
            vae_info = f", VAE: {vae_name}"
        
        return f"{params.prompt}\n\nNegative prompt: {params.negative_prompt}\nSteps: {params.steps}, Sampler: {params.sampler}, CFG scale: {params.cfg_scale}, Seed: {seed}, Size: {params.width}x{params.height}, Model: {model_name}{vae_info}"

    def _create_pnginfo(self, metadata: str) -> Any:
        """PNG ë©”íƒ€ë°ì´í„° ê°ì²´ ìƒì„±"""
        pnginfo = PngImagePlugin.PngInfo()
        pnginfo.add_text("parameters", metadata)
        return pnginfo

    async def start_infinite_generation(self):
        """ë¬´í•œ ìƒì„± ëª¨ë“œ ì‹œì‘"""
        if not self.model_loader.get_current_pipeline():
            self._notify_user('ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.', 'warning')
            return
        
        self.set('infinite_mode', True)
        self._notify_user('ë¬´í•œ ìƒì„± ëª¨ë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')
        asyncio.create_task(self._infinite_generation_loop())

    async def _infinite_generation_loop(self):
        """ë¬´í•œ ìƒì„± ë£¨í”„"""
        while self.get('infinite_mode') and not self.stop_generation_flag.is_set():
            try:
                await self._generate_single_image()
                await asyncio.sleep(2)  # 2ì´ˆ ëŒ€ê¸°
            except Exception as e:
                print(f"âŒ ë¬´í•œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(5)  # ì˜¤ë¥˜ ì‹œ 5ì´ˆ ëŒ€ê¸°

    async def _generate_single_image(self):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ìƒì„± (ë¬´í•œ ëª¨ë“œìš©)"""
        if not self.model_loader.get_current_pipeline():
            return
        
        params = self.get('current_params')
        
        def _generate():
            return self.model_loader.get_current_pipeline()(
                prompt=params.prompt,
                negative_prompt=params.negative_prompt,
                width=params.width,
                height=params.height,
                num_inference_steps=params.steps,
                guidance_scale=params.cfg_scale,
                generator=torch.Generator(device=self.device).manual_seed(params.seed)
            )
        
        try:
            result = await asyncio.to_thread(_generate)
            if result.images:
                await self.finish_generation(result.images[0], params, params.seed)
        except Exception as e:
            print(f"âŒ ë‹¨ì¼ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")

    async def stop_infinite_generation(self):
        """ë¬´í•œ ìƒì„± ëª¨ë“œ ì¤‘ì§€"""
        self.set('infinite_mode', False)
        self._notify_user('ë¬´í•œ ìƒì„± ëª¨ë“œê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')

    async def stop_generation(self):
        """ìƒì„± ì¤‘ì§€"""
        self.stop_generation_flag.set()
        self.set('is_generating', False)
        self._notify_user('ìƒì„±ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')

    def apply_params_from_metadata(self, model_info: Dict[str, Any], include_prompts: bool = False):
        """ë©”íƒ€ë°ì´í„°ì—ì„œ íŒŒë¼ë¯¸í„° ì ìš© (ë” ì´ìƒ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ì§€ ì•ŠìŒ)"""
        def get_first(*keys, default=None):
            # ìµœìƒìœ„ â†’ parameters ë‚´ë¶€ ìˆœì„œë¡œ ê°’ ë°˜í™˜
            for key in keys:
                if key in model_info:
                    return model_info[key]
                if 'parameters' in model_info and key in model_info['parameters']:
                    return model_info['parameters'][key]
            return default
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        width = get_first('width', 'Width', default=512)
        height = get_first('height', 'Height', default=512)
        steps = get_first('steps', 'Steps', default=20)
        cfg_scale = get_first('cfg_scale', 'CFG Scale', default=7.0)
        sampler = get_first('sampler', 'Sampler', default='dpmpp_2m')
        scheduler = get_first('scheduler', 'Scheduler', default='karras')
        seed = get_first('seed', 'Seed', default=-1)
        
        # í˜„ì¬ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        current_params = self.get('current_params')
        
        # í”„ë¡¬í”„íŠ¸ í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬
        if include_prompts:
            prompt = get_first('prompt', 'Prompt', default='')
            negative_prompt = get_first('negative_prompt', 'Negative prompt', default='')
            current_params.prompt = prompt
            current_params.negative_prompt = negative_prompt
            print(f"â„¹ï¸ í”„ë¡¬í”„íŠ¸ëŠ” ì œì™¸í•˜ê³  íŒŒë¼ë¯¸í„°ë§Œ ì ìš©í•©ë‹ˆë‹¤.")
        
        # ê° íŒŒë¼ë¯¸í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì´ë²¤íŠ¸ ë°œìƒ
        if width is not None:
            self.update_param('width', int(width))
        if height is not None:
            self.update_param('height', int(height))
        if steps is not None:
            self.update_param('steps', int(steps))
        if cfg_scale is not None:
            self.update_param('cfg_scale', float(cfg_scale))
        if sampler:
            self.update_param('sampler', str(sampler))
        if scheduler:
            self.update_param('scheduler', str(scheduler))
        
        # ì‹œë“œ ì²˜ë¦¬
        if seed is not None and seed != -1:
            self.update_param('seed', int(seed))
            print(f"âœ… Seed {seed} ë°œê²¬, ëœë¤ ì‹œë“œë¡œ ì„¤ì •: -1")
        else:
            self.update_param('seed', -1)
        
        print(f"âœ… ë©”íƒ€ë°ì´í„°ì—ì„œ íŒŒë¼ë¯¸í„°ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def restore_from_history(self, history_id: str):
        """íˆìŠ¤í† ë¦¬ì—ì„œ íŒŒë¼ë¯¸í„° ë³µì›"""
        history = self.get('history', [])
        for item in history:
            if item.get('id') == history_id:
                # íŒŒë¼ë¯¸í„° ë³µì›
                params = item.get('params', {})
                current_params = self.get('current_params')
                
                # GenerationParams ê°ì²´ë¡œ ë³€í™˜
                if isinstance(params, dict):
                    for key, value in params.items():
                        if hasattr(current_params, key):
                            setattr(current_params, key, value)
                elif hasattr(params, '__dict__'):
                    # ì´ë¯¸ GenerationParams ê°ì²´ì¸ ê²½ìš°
                    for key, value in params.__dict__.items():
                        if hasattr(current_params, key):
                            setattr(current_params, key, value)
                
                # VAE ë³µì›
                vae = item.get('vae')
                if vae and vae != 'baked_in':
                    self.set('current_vae_path', vae)
                
                # LoRA ë³µì›
                loras = item.get('loras', [])
                if loras:
                    self.set('current_loras', loras)
                
                # ëª¨ë¸ ë³µì› (ê°€ëŠ¥í•œ ê²½ìš°)
                model_name = item.get('model')
                if model_name:
                    available_checkpoints = self.get('available_checkpoints', {})
                    for folder_models in available_checkpoints.values():
                        for model_info in folder_models:
                            if model_info['name'] == model_name:
                                asyncio.create_task(self.select_model(model_info))
                                break
                
                self._notify('params_restored', params)
                self._notify_user('íˆìŠ¤í† ë¦¬ì—ì„œ íŒŒë¼ë¯¸í„°ê°€ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤.', 'positive')
                return
        
        self._notify_user('íˆìŠ¤í† ë¦¬ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'negative')

    async def get_vae_options_list(self) -> List[str]:
        """VAE ì˜µì…˜ ëª©ë¡ ë°˜í™˜"""
        available_vae = self.get('available_vae', {})
        options = ['baked_in']  # ê¸°ë³¸ê°’
        
        for folder_vae in available_vae.values():
            for vae_info in folder_vae:
                options.append(vae_info['name'])
        
        return options

    def find_vae_by_name(self, vae_name: str) -> Optional[str]:
        """VAE ì´ë¦„ìœ¼ë¡œ ê²½ë¡œ ì°¾ê¸°"""
        if vae_name == 'baked_in':
            return 'baked_in'
        
        available_vae = self.get('available_vae', {})
        for folder_vae in available_vae.values():
            for vae_info in folder_vae:
                if vae_info['name'] == vae_name:
                    return vae_info['path']
        
        return None

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ëª¨ë¸ ì–¸ë¡œë“œ
            self.model_loader.unload_model()
            
            # CUDA ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # --- ìƒíƒœ ê´€ë¦¬ ë©”ì„œë“œë“¤ ---
    def get(self, key: str, default: Any = None) -> Any:
        """ìƒíƒœ ê°’ ì¡°íšŒ"""
        return self._state.get(key, default)

    def set(self, key: str, value: Any):
        """ìƒíƒœ ê°’ ì„¤ì •"""
        self._state[key] = value
        self._notify(f'{key}_changed', value)
    
    def set_init_image(self, image):
        """img2imgìš© ì´ë¯¸ì§€ ì„¤ì • (ë³„ë„ ë©”ì„œë“œ) - ê°œì„ ëœ ë²„ì „"""
        print(f"ğŸ” set_init_image í˜¸ì¶œ: ì´ë¯¸ì§€ íƒ€ì…={type(image)}")
        
        if image is not None:
            print(f"ğŸ” ì´ë¯¸ì§€ ì •ë³´: í¬ê¸°={image.size}, ëª¨ë“œ={image.mode}")
            # ì´ë¯¸ì§€ ê°ì²´ë¥¼ ì§ì ‘ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥ (ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ ìš°íšŒ)
            self._state['init_image'] = image
            print(f"âœ… init_image ì§ì ‘ ì €ì¥ ì™„ë£Œ: {image.size}")
            
            # ì €ì¥ í›„ í™•ì¸
            saved_image = self._state.get('init_image')
            print(f"ğŸ” ì €ì¥ í›„ í™•ì¸: íƒ€ì…={type(saved_image)}, í¬ê¸°={saved_image.size if saved_image else 'None'}")
            
            # ì´ë²¤íŠ¸ ë°œìƒ (ì´ë¯¸ì§€ ê°ì²´ ëŒ€ì‹  ì„±ê³µ ë©”ì‹œì§€ ì „ë‹¬)
            self._notify('init_image_changed', {'status': 'success', 'size': image.size})
        else:
            print(f"âš ï¸ init_imageê°€ Noneìœ¼ë¡œ ì„¤ì •ë¨")
            self._state['init_image'] = None
            self._notify('init_image_changed', {'status': 'cleared'})

    def update_param(self, param_name: str, value: Any):
        """íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸"""
        current_params = self.get('current_params')
        if hasattr(current_params, param_name):
            setattr(current_params, param_name, value)
            print(f"âœ… {param_name} ì ìš©: {value}")
            self._notify('params_updated', {param_name: value})

    def update_prompt(self, positive_prompt: str, negative_prompt: str):
        """í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸"""
        current_params = self.get('current_params')
        current_params.prompt = positive_prompt
        current_params.negative_prompt = negative_prompt
        self._notify('prompt_updated', {'positive': positive_prompt, 'negative': negative_prompt})

    # --- ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ ---
    def subscribe(self, event: str, callback: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        if event not in self._observers:
            self._observers[event] = []
        self._observers[event].append(callback)

    def unsubscribe(self, event: str, callback: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë… í•´ì œ"""
        if event in self._observers:
            self._observers[event].remove(callback)

    def _notify(self, event: str, data: Any = None):
        """ì´ë²¤íŠ¸ ë°œìƒ"""
        if event in self._observers:
            for callback in self._observers[event]:
                try:
                    # async í•¨ìˆ˜ì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆíˆ ì²˜ë¦¬
                    if asyncio.iscoroutinefunction(callback):
                        # async í•¨ìˆ˜ëŠ” ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
                        asyncio.create_task(callback(data))
                    else:
                        # ë™ê¸° í•¨ìˆ˜ëŠ” ì§ì ‘ í˜¸ì¶œ
                        callback(data)
                except Exception as e:
                    print(f"âš ï¸ ì´ë²¤íŠ¸ ì½œë°± ì˜¤ë¥˜ ({event}): {e}")

    def _notify_user(self, message: str, notification_type: str = 'info', duration: int = 5):
        """ì‚¬ìš©ì ì•Œë¦¼"""
        self._notify('user_notification', {
            'message': message,
            'type': notification_type,
            'duration': duration
        })

    # --- í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ (ë„ë©”ì¸ ì„œë¹„ìŠ¤ ìœ„ì„) ---
    def copy_prompt_to_clipboard(self, positive_prompt: str = "", negative_prompt: str = ""):
        """í”„ë¡¬í”„íŠ¸ë¥¼ í´ë¦½ë³´ë“œì— ë³µì‚¬"""
        import pyperclip
        
        if positive_prompt and negative_prompt:
            full_prompt = f"{positive_prompt}\n\nNegative prompt: {negative_prompt}"
        elif positive_prompt:
            full_prompt = positive_prompt
        elif negative_prompt:
            full_prompt = f"Negative prompt: {negative_prompt}"
        else:
            return
        
        try:
            pyperclip.copy(full_prompt)
            print(f"ğŸ“‹ í´ë¦½ë³´ë“œì— ë³µì‚¬ë¨: {full_prompt[:50]}...")
            self._notify_user("í”„ë¡¬í”„íŠ¸ê°€ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.", 'positive')
        except Exception as e:
            print(f"âŒ í´ë¦½ë³´ë“œ ë³µì‚¬ ì‹¤íŒ¨: {e}")
            self._notify_user("í´ë¦½ë³´ë“œ ë³µì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", 'negative')

    def calculate_token_count(self, text: str) -> int:
        """ì‹¤ì œ CLIP í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ìˆ˜ ê³„ì‚°"""
        pipeline = self.model_loader.get_current_pipeline()
        if not text.strip() or not pipeline or not hasattr(pipeline, 'tokenizer'):
            return 0
        
        try:
            # í† í¬ë‚˜ì´ì €ë¡œ ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°
            text_inputs = pipeline.tokenizer(
                text,
                padding="longest",
                truncation=False,
                return_tensors="pt"
            )
            return len(text_inputs.input_ids[0])
        except Exception as e:
            print(f"âš ï¸ í† í° ê³„ì‚° ì˜¤ë¥˜: {e}")
            return len(text.split())  # í´ë°±: ë‹¨ì–´ ìˆ˜ë¡œ ì¶”ì •

    def analyze_prompt(self, prompt: str) -> Dict[str, Any]:
        """í”„ë¡¬í”„íŠ¸ ë¶„ì„ ë° ìµœì í™” ì œì•ˆ"""
        pipeline = self.model_loader.get_current_pipeline()
        tokenizer = getattr(pipeline, 'tokenizer', None) if pipeline else None
        analysis = self.prompt_processor.analyze_prompt(prompt, tokenizer)
        
        # PromptAnalysis ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        return {
            'token_count': analysis.token_count,
            'segments': analysis.segments,
            'weights': analysis.weights,
            'suggestions': analysis.suggestions,
            'is_optimized': analysis.is_optimized,
            'truncated_parts': analysis.truncated_parts
        }

    def add_break_keyword(self, prompt: str, position: int = None) -> str:
        """BREAK í‚¤ì›Œë“œ ì¶”ê°€"""
        return self.prompt_processor.add_break_keyword(prompt, position)

    def apply_weight(self, keyword: str, weight: float = 1.1) -> str:
        """ê°€ì¤‘ì¹˜ ì ìš©"""
        return self.prompt_processor.apply_weight(keyword, weight)

    def optimize_prompt(self, prompt: str, target_tokens: int = 70) -> str:
        """í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        return self.prompt_processor.optimize_prompt(prompt, target_tokens)

    def _add_to_history(self, history_item: Dict[str, Any]):
        """íˆìŠ¤í† ë¦¬ì— ì•„ì´í…œ ì¶”ê°€"""
        history = self.get('history', [])
        history.insert(0, history_item)  # ìµœì‹  í•­ëª©ì„ ë§¨ ì•ì— ì¶”ê°€
        
        # ì„¤ì •ì—ì„œ íˆìŠ¤í† ë¦¬ ì œí•œ ê°€ì ¸ì˜¤ê¸°
        history_limit = self.config.get('ui', {}).get('history_limit', 50)
        history = history[:history_limit]  # ì œí•œëœ ê°œìˆ˜ë§Œ ìœ ì§€
        
        self.set('history', history)
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸ ë°œìƒ
        self._notify('history_updated', history)
        
        print(f"ğŸ“‹ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ë¨: {history_item.get('model', 'Unknown')}")
    
    async def _notify_async(self, event: str, data: Any = None):
        """ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë°œìƒ"""
        self._notify(event, data)

    def get_history(self) -> List[Dict[str, Any]]:
        """íˆìŠ¤í† ë¦¬ ëª©ë¡ ë°˜í™˜"""
        return self.get('history', [])
    
    def clear_history(self):
        """íˆìŠ¤í† ë¦¬ ì „ì²´ ì‚­ì œ"""
        self.set('history', [])
        self._notify('history_updated', [])
        self._notify_user('íˆìŠ¤í† ë¦¬ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')
    
    def clear_all_history(self):
        """ì „ì²´ íˆìŠ¤í† ë¦¬ ì‚­ì œ (ë³„ì¹­)"""
        self.clear_history()
    
    def delete_history_item(self, history_id: str):
        """íŠ¹ì • íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì‚­ì œ"""
        history = self.get('history', [])
        history = [item for item in history if item.get('id') != history_id]
        self.set('history', history)
        self._notify('history_updated', history)
        self._notify_user('íˆìŠ¤í† ë¦¬ í•­ëª©ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.', 'info')