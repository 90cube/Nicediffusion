"""
ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ ë„ë©”ì¸ ë¡œì§
UIë‚˜ Servicesì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""

import torch
import asyncio
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from PIL import Image


@dataclass
class Img2ImgParams:
    """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± íŒŒë¼ë¯¸í„°"""
    prompt: str
    negative_prompt: str
    init_image: Image.Image
    strength: float  # 0.0 ~ 1.0 (0.0: ì›ë³¸ ìœ ì§€, 1.0: ì™„ì „íˆ ìƒˆë¡œ ìƒì„±)
    width: int
    height: int
    steps: int
    cfg_scale: float
    seed: int
    sampler: str
    scheduler: str
    batch_size: int
    model_type: str = 'SD15'
    clip_skip: int = 1  # CLIP Skip ì¶”ê°€


class Img2ImgMode:
    """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ (A1111 ìŠ¤íƒ€ì¼)"""
    
    def __init__(self, pipeline: Any, device: str):
        self.pipeline = pipeline
        self.device = device
    
    def _encode_image(self, image: Image.Image) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ latent spaceë¡œ ì¸ì½”ë”© (ê°œì„ ëœ ë²„ì „)"""
        import torch  # ë©”ì„œë“œ ë‚´ë¶€ì—ì„œ torch import
        
        print(f"ğŸ” ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹œì‘: í¬ê¸°={image.size}, ëª¨ë“œ={image.mode}")
        
        # RGBë¡œ ë³€í™˜ (í•„ìˆ˜)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        with torch.no_grad():
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ - ë” ì•ˆì „í•œ ë°©ì‹
            try:
                # ë°©ë²• 1: image_processor ì‚¬ìš©
                if hasattr(self.pipeline, 'image_processor') and self.pipeline.image_processor is not None:
                    image_tensor = self.pipeline.image_processor.preprocess(image)
                # ë°©ë²• 2: feature_extractor ì‚¬ìš©
                elif hasattr(self.pipeline, 'feature_extractor') and self.pipeline.feature_extractor is not None:
                    image_tensor = self.pipeline.feature_extractor(
                        image, 
                        return_tensors="pt"
                    ).pixel_values
                # ë°©ë²• 3: ìˆ˜ë™ ì „ì²˜ë¦¬
                else:
                    import torchvision.transforms as transforms
                    transform = transforms.Compose([
                        transforms.ToTensor(),
                        transforms.Normalize([0.5], [0.5])  # -1 to 1ë¡œ ì •ê·œí™”
                    ])
                    image_tensor = transform(image).unsqueeze(0)
                    
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜, ìˆ˜ë™ ì²˜ë¦¬ë¡œ ëŒ€ì²´: {e}")
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ì§ì ‘ ë³€í™˜
                import numpy as np
                np_image = np.array(image).astype(np.float32) / 255.0
                np_image = (np_image - 0.5) / 0.5  # -1 to 1
                image_tensor = torch.from_numpy(np_image).permute(2, 0, 1).unsqueeze(0)
            
            # ë””ë°”ì´ìŠ¤ì™€ ë°ì´í„° íƒ€ì… ë§ì¶”ê¸°
            image_tensor = image_tensor.to(self.device, dtype=self.pipeline.vae.dtype)
            
            # VAE ì¸ì½”ë”©
            latent = self.pipeline.vae.encode(image_tensor).latent_dist.sample()
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            print(f"âœ… ì´ë¯¸ì§€ ì¸ì½”ë”© ì™„ë£Œ: latent shape={latent.shape}, dtype={latent.dtype}")
            return latent
    
    def _validate_init_image(self, init_image: Image.Image, target_width: int, target_height: int) -> Image.Image:
        """ì´ˆê¸° ì´ë¯¸ì§€ ê²€ì¦ ë° ë¦¬ì‚¬ì´ì¦ˆ"""
        if init_image is None:
            raise ValueError("ì´ˆê¸° ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        if init_image.size != (target_width, target_height):
            print(f"ğŸ”„ ì´ˆê¸° ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •: {init_image.size} -> ({target_width}, {target_height})")
            init_image = init_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
        
        return init_image
    
    def _validate_strength(self, strength: float) -> float:
        """ê°•ë„ ê°’ ê²€ì¦"""
        if strength < 0.0 or strength > 1.0:
            raise ValueError("ê°•ë„ ê°’ì€ 0.0 ~ 1.0 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        return strength
    
    def _apply_sd15_optimizations(self, params: Img2ImgParams):
        """SD15 ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©"""
        if params.model_type != 'SD15':
            return
            
        # SD15ì—ì„œ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì„¤ì •
        if hasattr(self.pipeline.scheduler, 'config'):
            self.pipeline.scheduler.config.use_karras_sigmas = True
            self.pipeline.scheduler.config.karras_rho = 7.0
        
        # SD15ì—ì„œ ë” ë‚˜ì€ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ë§
        if hasattr(self.pipeline.scheduler, 'set_timesteps'):
            self.pipeline.scheduler.set_timesteps(params.steps, device=self.device)
        
        # SD15ì—ì„œ ë” ì•ˆì •ì ì¸ ìƒì„±
        if hasattr(self.pipeline.scheduler, 'beta_start'):
            self.pipeline.scheduler.beta_start = 0.00085
        if hasattr(self.pipeline.scheduler, 'beta_end'):
            self.pipeline.scheduler.beta_end = 0.012
        
        # SD15ì—ì„œ ë” ì„ ëª…í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì„¤ì •
        if hasattr(self.pipeline, 'enable_attention_slicing'):
            self.pipeline.enable_attention_slicing(1)
        if hasattr(self.pipeline, 'enable_vae_slicing'):
            self.pipeline.enable_vae_slicing()
        
        # xformersëŠ” ì„ íƒì  ê¸°ëŠ¥ì´ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            if hasattr(self.pipeline, 'enable_xformers_memory_efficient_attention'):
                self.pipeline.enable_xformers_memory_efficient_attention()
        except ModuleNotFoundError:
            pass  # ì¡°ìš©íˆ ë¬´ì‹œ
    
    async def generate(self, params: Img2ImgParams) -> List[Any]:
        """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰ (A1111 ìŠ¤íƒ€ì¼)"""
        print(f"ğŸ¨ Img2Img ìƒì„± ì‹œì‘ - Seed: {params.seed}, Strength: {params.strength}")
        print(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ - Size: {params.width}x{params.height}, Batch: {params.batch_size}")
        
        # ë””ë²„ê·¸: ì´ˆê¸° ì´ë¯¸ì§€ í™•ì¸
        print(f"ğŸ” Img2Img ëª¨ë“œì—ì„œ init_image í™•ì¸: {params.init_image}")
        if params.init_image:
            print(f"ğŸ” Img2Img ëª¨ë“œì—ì„œ ì´ë¯¸ì§€ í¬ê¸°: {params.init_image.size}, ëª¨ë“œ: {params.init_image.mode}")
        else:
            print(f"âŒ Img2Img ëª¨ë“œì—ì„œ init_imageê°€ None!")
            return []
        
        # íŒŒë¼ë¯¸í„° ê²€ì¦
        init_image = self._validate_init_image(params.init_image, params.width, params.height)
        strength = self._validate_strength(params.strength)
        
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {params.prompt[:100]}...")
        print(f"ğŸš« ë¶€ì • í”„ë¡¬í”„íŠ¸: {params.negative_prompt[:100]}...")
        print(f"âš™ï¸ Steps: {params.steps}, CFG: {params.cfg_scale}, Strength: {strength}")
        
        # SD15 ìµœì í™” ì ìš©
        self._apply_sd15_optimizations(params)
        
        if params.model_type == 'SD15':
            print(f"ğŸ”§ SD15 ìƒì„± ìµœì í™” ì ìš©: Steps={params.steps}, CFG={params.cfg_scale}")
        
        # ìƒì„±ê¸° ì„¤ì •
        generator = torch.Generator(device=self.device)
        if params.seed > 0:
            generator.manual_seed(params.seed)
        
        def _generate():
            """ì‹¤ì œ ìƒì„± ë¡œì§ (ì˜¬ë°”ë¥¸ Denoising Strength êµ¬í˜„)"""
            import torch  # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ torch import
            
            print(f"ğŸ” íŒŒì´í”„ë¼ì¸ íƒ€ì…: {type(self.pipeline)}")
            
            # 1. ì´ë¯¸ì§€ â†’ latent ë³€í™˜
            init_latent = self._encode_image(init_image)
            
            # 2. ì˜¬ë°”ë¥¸ Denoising Strength êµ¬í˜„
            # ì „ì²´ ìŠ¤í… ì¤‘ ì¼ë¶€ë§Œ ì‹¤í–‰: Strength 0.7 + Steps 50 = ì‹¤ì œ 35ìŠ¤í…ë§Œ ì‹¤í–‰
            # ì²˜ìŒ 15ìŠ¤í…ì€ ê±´ë„ˆë›°ê³  ì‹œì‘
            effective_steps = int(params.steps * strength)
            skipped_steps = params.steps - effective_steps
            
            print(f"ğŸ” Denoising Strength ê³„ì‚°:")
            print(f"   - ì „ì²´ ìŠ¤í…: {params.steps}")
            print(f"   - Strength: {strength}")
            print(f"   - ì‹¤ì œ ì‹¤í–‰ ìŠ¤í…: {effective_steps}")
            print(f"   - ê±´ë„ˆë›¸ ìŠ¤í…: {skipped_steps}")
            
            # 3. íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (ì˜¬ë°”ë¥¸ strength ì ìš©)
            try:
                # ìŠ¤ì¼€ì¤„ëŸ¬ timesteps ì„¤ì •
                if hasattr(self.pipeline.scheduler, 'set_timesteps'):
                    self.pipeline.scheduler.set_timesteps(params.steps, device=self.device)
                
                # Denoising Strengthê°€ ì œëŒ€ë¡œ ì ìš©ë˜ë„ë¡ íŒŒë¼ë¯¸í„° ê²€ì¦
                print(f"ğŸ” ìµœì¢… íŒŒë¼ë¯¸í„°:")
                print(f"   - strength: {strength}")
                print(f"   - steps: {params.steps}")
                print(f"   - cfg_scale: {params.cfg_scale}")
                print(f"   - image size: {init_image.size}")
                
                # íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (strength íŒŒë¼ë¯¸í„°ê°€ ì œëŒ€ë¡œ ì „ë‹¬ë˜ëŠ”ì§€ í™•ì¸)
                result = self.pipeline(
                    prompt=params.prompt,
                    negative_prompt=params.negative_prompt,
                    image=init_image,
                    strength=strength,  # ì´ ê°’ì´ ì œëŒ€ë¡œ ì ìš©ë˜ì–´ì•¼ í•¨
                    num_inference_steps=params.steps,
                    guidance_scale=params.cfg_scale,
                    generator=generator,
                    num_images_per_prompt=params.batch_size,
                    # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
                    **({"eta": 1.0} if params.model_type == 'SD15' else {})
                )
                
                print(f"âœ… íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âš ï¸ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ì¬ì‹œë„
                result = self.pipeline(
                    prompt=params.prompt,
                    negative_prompt=params.negative_prompt,
                    image=init_image,
                    strength=strength,
                    num_inference_steps=params.steps,
                    guidance_scale=params.cfg_scale,
                    generator=generator,
                    num_images_per_prompt=1
                )
            
            # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ì—ì„œ images ë°˜í™˜
            if hasattr(result, 'images'):
                return result.images
            else:
                # result ìì²´ê°€ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                return result if isinstance(result, list) else [result]
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ìˆ˜í–‰
        generated_images = await asyncio.to_thread(_generate)
        
        print(f"âœ… ìƒì„±ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(generated_images)}")
        for i, image in enumerate(generated_images):
            print(f"âœ… ìƒì„±ëœ ì´ë¯¸ì§€ {i+1} í¬ê¸°: {image.size}")
        
        return generated_images
    
    async def upscale(self, image: Image.Image, scale_factor: float = 2.0) -> Image.Image:
        """ì´ë¯¸ì§€ ì—…ìŠ¤ì¼€ì¼ (ê°„ë‹¨í•œ êµ¬í˜„)"""
        if scale_factor <= 1.0:
            return image
        
        new_width = int(image.width * scale_factor)
        new_height = int(image.height * scale_factor)
        
        print(f"ğŸ”„ ì—…ìŠ¤ì¼€ì¼: {image.size} -> ({new_width}, {new_height})")
        
        # ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì¦ˆ
        upscaled_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        return upscaled_image
    
    async def inpaint(self, image: Image.Image, mask: Image.Image, prompt: str, 
                     negative_prompt: str = "", strength: float = 0.8) -> List[Any]:
        """ì¸í˜ì¸íŒ… (ë§ˆìŠ¤í¬ ê¸°ë°˜ ì´ë¯¸ì§€ ìˆ˜ì •)"""
        print(f"ğŸ¨ ì¸í˜ì¸íŒ… ì‹œì‘ - Strength: {strength}")
        
        # ë§ˆìŠ¤í¬ ê²€ì¦
        if mask.size != image.size:
            print(f"ğŸ”„ ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì •: {mask.size} -> {image.size}")
            mask = mask.resize(image.size, Image.Resampling.LANCZOS)
        
        # ìƒì„±ê¸° ì„¤ì •
        generator = torch.Generator(device=self.device)
        generator.manual_seed(int(torch.randint(0, 2**32 - 1, (1,)).item()))
        
        def _inpaint():
            """ì¸í˜ì¸íŒ… ìƒì„± ë¡œì§"""
            # ì¸í˜ì¸íŒ… íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (íŒŒì´í”„ë¼ì¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°)
            if hasattr(self.pipeline, 'inpaint'):
                result = self.pipeline.inpaint(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=image,
                    mask_image=mask,
                    strength=strength,
                    generator=generator,
                    num_inference_steps=20,
                    guidance_scale=7.0
                )
                return result.images if hasattr(result, 'images') else [result]
            else:
                # ì¸í˜ì¸íŒ…ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì¼ë°˜ img2imgë¡œ ëŒ€ì²´
                print("âš ï¸ ì¸í˜ì¸íŒ…ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤. ì¼ë°˜ img2imgë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                return self.pipeline(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=image,
                    strength=strength,
                    generator=generator,
                    num_inference_steps=20,
                    guidance_scale=7.0
                ).images
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ìˆ˜í–‰
        generated_images = await asyncio.to_thread(_inpaint)
        
        print(f"âœ… ì¸í˜ì¸íŒ… ì™„ë£Œ: {len(generated_images)}ê°œ ì´ë¯¸ì§€")
        return generated_images
