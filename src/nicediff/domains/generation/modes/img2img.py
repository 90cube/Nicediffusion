"""
ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ ë„ë©”ì¸ ë¡œì§
UIë‚˜ Servicesì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""

import torch
import asyncio
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from PIL import Image


@dataclass
class Img2ImgParams:
    """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± íŒŒë¼ë¯¸í„° (A1111 ìŠ¤íƒ€ì¼)"""
    prompt: str
    negative_prompt: str
    init_image: Image.Image
    strength: float  # 0.0 ~ 1.0 (0.0: ì›ë³¸ ìœ ì§€, 1.0: ì™„ì „íˆ ìƒˆë¡œ ìƒì„±)
    width: int
    height: int
    steps: int
    cfg_scale: float
    seed: int
    sampler: str
    scheduler: str
    batch_size: int
    model_type: str = 'SD15'
    clip_skip: int = 1  # CLIP Skip ì¶”ê°€
    size_match_enabled: bool = False  # í¬ê¸° ì¼ì¹˜ ëª¨ë“œ ì¶”ê°€
    
    # A1111 ì¶”ê°€ íŒŒë¼ë¯¸í„°ë“¤
    image_cfg_scale: float = 10  # ì´ë¯¸ì§€ CFG ìŠ¤ì¼€ì¼ (A1111 image_cfg_scale)
    resize_mode: int = 0  # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ (0st resize, 1: Crop and resize, 2 Resize and fill)
    mask_blur: int = 4 # ë§ˆìŠ¤í¬ ë¸”ëŸ¬ (ì¸í˜ì¸íŒ…ìš©)
    inpainting_fill: int = 1  # ì¸í˜ì¸íŒ… ì±„ìš°ê¸° ëª¨ë“œ (0: fill, 1: original, 2: latent noise,3: latent nothing)
    inpaint_full_res: bool = False  # ì „ì²´ í•´ìƒë„ ì¸í˜ì¸íŒ…
    inpaint_full_res_padding: int = 32# ì „ì²´ í•´ìƒë„ íŒ¨ë”©
    inpainting_mask_invert: int = 0  # ë§ˆìŠ¤í¬ ë°˜ì „ (0: normal, 1: invert)
    eta: float = 1.0 # ETA (ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ë§)
    tiling: bool = False  # íƒ€ì¼ë§ ëª¨ë“œ
    restore_faces: bool = False  # ì–¼êµ´ ë³µì›
    subseed: int = -1# ì„œë¸Œì‹œë“œ
    subseed_strength: float = 0# ì„œë¸Œì‹œë“œ ê°•ë„
    seed_resize_from_h: int =-1  # ì‹œë“œ ë¦¬ì‚¬ì´ì¦ˆ ë†’ì´
    seed_resize_from_w: int =-1  # ì‹œë“œ ë¦¬ì‚¬ì´ì¦ˆ ë„ˆë¹„
    sampler_index: int = 0  # ìƒ˜í”ŒëŸ¬ ì¸ë±ìŠ¤
    script_name: str = ""  # ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„
    script_args: list = field(default_factory=list)  # ìŠ¤í¬ë¦½íŠ¸ ì¸ìˆ˜
    send_images: bool = True  # ì´ë¯¸ì§€ ì „ì†¡
    save_images: bool = False  # ì´ë¯¸ì§€ ì €ì¥
    alwayson_scripts: dict = field(default_factory=dict)  # í•­ìƒ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸


class Img2ImgMode:
    """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ (A1111 ìŠ¤íƒ€ì¼)"""
    
    def __init__(self, pipeline: Any, device: str):
        self.pipeline = pipeline
        self.device = device
    
    def _encode_image(self, input_image: Image.Image) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ latent spaceë¡œ ì¸ì½”ë”© (ìƒì„¸ ë””ë²„ê¹… ë²„ì „)"""
        import torch
        import numpy as np
        
        print("=" * 80)
        print("ğŸ” [DEBUG] ì´ë¯¸ì§€ â†’ Latent ë³€í™˜ ê³¼ì • ìƒì„¸ ë¶„ì„")
        print("=" * 80)
        
        # 1ë‹¨ê³„: ì…ë ¥ ì´ë¯¸ì§€ ê²€ì¦
        print(f"ğŸ“¥ 1ë‹¨ê³„: ì…ë ¥ ì´ë¯¸ì§€ ê²€ì¦")
        print(f"   - ì´ë¯¸ì§€ íƒ€ì…: {type(input_image)}")
        print(f"   - ì´ë¯¸ì§€ í¬ê¸°: {input_image.size}")
        print(f"   - ì´ë¯¸ì§€ ëª¨ë“œ: {input_image.mode}")
        print(f"   - ì´ë¯¸ì§€ í¬ë§·: {getattr(input_image, 'format', 'Unknown')}")
        
        if input_image is None:
            raise ValueError("âŒ ì…ë ¥ ì´ë¯¸ì§€ê°€ Noneì…ë‹ˆë‹¤!")
        
        # 2ë‹¨ê³„: RGB ë³€í™˜
        print(f"\nğŸ”„ 2ë‹¨ê³„: RGB ë³€í™˜")
        original_mode = input_image.mode
        if input_image.mode != 'RGB':
            print(f"   - ì›ë³¸ ëª¨ë“œ: {original_mode} â†’ RGBë¡œ ë³€í™˜")
            input_image = input_image.convert('RGB')
        else:
            print(f"   - ì´ë¯¸ RGB ëª¨ë“œì…ë‹ˆë‹¤")
        
        print(f"   - ë³€í™˜ í›„ ëª¨ë“œ: {input_image.mode}")
        
        # 3ë‹¨ê³„: ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        print(f"\nâš™ï¸ 3ë‹¨ê³„: ì´ë¯¸ì§€ ì „ì²˜ë¦¬")
        
        with torch.no_grad():
            try:
                # ë°©ë²• 1: image_processor ì‚¬ìš© (SDXL)
                if hasattr(self.pipeline, 'image_processor') and self.pipeline.image_processor is not None:
                    print(f"   - ë°©ë²• 1: image_processor ì‚¬ìš©")
                    print(f"   - image_processor íƒ€ì…: {type(self.pipeline.image_processor)}")
                    
                    image_tensor = self.pipeline.image_processor.preprocess(input_image)
                    print(f"   âœ… image_processor ì „ì²˜ë¦¬ ì„±ê³µ")
                    
                # ë°©ë²• 2: feature_extractor ì‚¬ìš© (SD15)
                elif hasattr(self.pipeline, 'feature_extractor') and self.pipeline.feature_extractor is not None:
                    print(f"   - ë°©ë²• 2: feature_extractor ì‚¬ìš©")
                    print(f"   - feature_extractor íƒ€ì…: {type(self.pipeline.feature_extractor)}")
                    
                    result = self.pipeline.feature_extractor(input_image, return_tensors="pt")
                    image_tensor = result.pixel_values
                    print(f"   âœ… feature_extractor ì „ì²˜ë¦¬ ì„±ê³µ")
                    
                # ë°©ë²• 3: ìˆ˜ë™ ì „ì²˜ë¦¬ (fallback)
                else:
                    print(f"   - ë°©ë²• 3: ìˆ˜ë™ ì „ì²˜ë¦¬ (torchvision transforms)")
                    import torchvision.transforms as transforms
                    
                    transform = transforms.Compose([
                        transforms.ToTensor(),
                        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # -1 to 1ë¡œ ì •ê·œí™”
                    ])
                    image_tensor = transform(input_image).unsqueeze(0)
                    print(f"   âœ… ìˆ˜ë™ ì „ì²˜ë¦¬ ì„±ê³µ")
                    
            except Exception as e:
                print(f"   âŒ ì „ì²˜ë¦¬ ë°©ë²• 1-3 ì‹¤íŒ¨: {e}")
                print(f"   - ìµœí›„ì˜ ìˆ˜ë‹¨: numpy ì§ì ‘ ë³€í™˜")
                
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ì§ì ‘ ë³€í™˜
                np_image = np.array(input_image).astype(np.float32) / 255.0
                np_image = (np_image - 0.5) / 0.5  # -1 to 1
                image_tensor = torch.from_numpy(np_image).permute(2, 0, 1).unsqueeze(0)
                print(f"   âœ… numpy ì§ì ‘ ë³€í™˜ ì„±ê³µ")
            
            # 4ë‹¨ê³„: ì „ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦
            print(f"\nğŸ” 4ë‹¨ê³„: ì „ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦")
            print(f"   - tensor shape: {image_tensor.shape}")
            print(f"   - tensor dtype: {image_tensor.dtype}")
            print(f"   - tensor device: {image_tensor.device}")
            print(f"   - ê°’ì˜ ë²”ìœ„: [{image_tensor.min().item():.3f}, {image_tensor.max().item():.3f}]")
            print(f"   - í‰ê· ê°’: {image_tensor.mean().item():.3f}")
            print(f"   - í‘œì¤€í¸ì°¨: {image_tensor.std().item():.3f}")
            
            # ê°’ ë²”ìœ„ ê²€ì¦
            if image_tensor.min() < -1.1 or image_tensor.max() > 1.1:
                print(f"   âš ï¸ ê²½ê³ : ê°’ ë²”ìœ„ê°€ ì˜ˆìƒ ë²”ìœ„ [-1, 1]ì„ ë²—ì–´ë‚¨!")
            
            # 5ë‹¨ê³„: ë””ë°”ì´ìŠ¤ ë° ë°ì´í„° íƒ€ì… ë³€í™˜
            print(f"\nğŸ”„ 5ë‹¨ê³„: ë””ë°”ì´ìŠ¤ ë° ë°ì´í„° íƒ€ì… ë³€í™˜")
            print(f"   - ëŒ€ìƒ ë””ë°”ì´ìŠ¤: {self.device}")
            print(f"   - VAE dtype: {self.pipeline.vae.dtype}")
            
            image_tensor = image_tensor.to(self.device, dtype=self.pipeline.vae.dtype)
            print(f"   - ë³€í™˜ í›„ device: {image_tensor.device}")
            print(f"   - ë³€í™˜ í›„ dtype: {image_tensor.dtype}")
            
            # 6ë‹¨ê³„: VAE ì¸ì½”ë”©
            print(f"\nğŸ¨ 6ë‹¨ê³„: VAE ì¸ì½”ë”©")
            print(f"   - VAE íƒ€ì…: {type(self.pipeline.vae)}")
            print(f"   - VAE config: {self.pipeline.vae.config}")
            
            try:
                # VAE ì¸ì½”ë”©
                vae_output = self.pipeline.vae.encode(image_tensor)
                print(f"   âœ… VAE ì¸ì½”ë”© ì„±ê³µ")
                print(f"   - VAE ì¶œë ¥ íƒ€ì…: {type(vae_output)}")
                
                # latent_distì—ì„œ ìƒ˜í”Œë§
                if hasattr(vae_output, 'latent_dist'):
                    print(f"   - latent_dist ì¡´ì¬: {type(vae_output.latent_dist)}")
                    latent = vae_output.latent_dist.sample()
                    print(f"   âœ… latent_dist.sample() ì„±ê³µ")
                else:
                    print(f"   - latent_dist ì—†ìŒ, ì§ì ‘ ì‚¬ìš©")
                    latent = vae_output
                
                print(f"   - ìƒ˜í”Œë§ í›„ latent shape: {latent.shape}")
                print(f"   - ìƒ˜í”Œë§ í›„ latent dtype: {latent.dtype}")
                print(f"   - ìƒ˜í”Œë§ í›„ ê°’ ë²”ìœ„: [{latent.min().item():.3f}, {latent.max().item():.3f}]")
                
            except Exception as e:
                print(f"   âŒ VAE ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                raise
            
            # 7ë‹¨ê³„: ìŠ¤ì¼€ì¼ë§ íŒ©í„° ì ìš©
            print(f"\nğŸ“ 7ë‹¨ê³„: ìŠ¤ì¼€ì¼ë§ íŒ©í„° ì ìš©")
            scaling_factor = self.pipeline.vae.config.scaling_factor
            print(f"   - VAE scaling_factor: {scaling_factor}")
            
            latent = latent * scaling_factor
            print(f"   - ìŠ¤ì¼€ì¼ë§ í›„ latent shape: {latent.shape}")
            print(f"   - ìŠ¤ì¼€ì¼ë§ í›„ latent dtype: {latent.dtype}")
            print(f"   - ìŠ¤ì¼€ì¼ë§ í›„ ê°’ ë²”ìœ„: [{latent.min().item():.3f}, {latent.max().item():.3f}]")
            
            # 8ë‹¨ê³„: ìµœì¢… ê²€ì¦
            print(f"\nâœ… 8ë‹¨ê³„: ìµœì¢… ê²€ì¦")
            print(f"   - ìµœì¢… latent shape: {latent.shape}")
            print(f"   - ìµœì¢… latent dtype: {latent.dtype}")
            print(f"   - ìµœì¢… latent device: {latent.device}")
            print(f"   - ìµœì¢… ê°’ ë²”ìœ„: [{latent.min().item():.3f}, {latent.max().item():.3f}]")
            
            # ì˜ˆìƒ shape ê²€ì¦
            expected_channels = 4  # VAE latent channels
            expected_height = image_tensor.shape[2] * 8  # VAE downsampling factor
            expected_width = image_tensor.shape[3] * 8   # VAE downsampling factor
            
            print(f"   - ì˜ˆìƒ shape: [1, {expected_channels}, {expected_height}, {expected_width}]")
            print(f"   - ì‹¤ì œ shape: {list(latent.shape)}")
            
            if latent.shape[1] != expected_channels:
                print(f"   âš ï¸ ê²½ê³ : ì±„ë„ ìˆ˜ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„!")
            if latent.shape[2] != expected_height or latent.shape[3] != expected_width:
                print(f"   âš ï¸ ê²½ê³ : ê³µê°„ ì°¨ì›ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„!")
            
            print("=" * 80)
            print("ğŸ‰ ì´ë¯¸ì§€ â†’ Latent ë³€í™˜ ì™„ë£Œ!")
            print("=" * 80)
            
            return latent
    
    def _validate_init_image(self, init_image: Image.Image, target_width: int, target_height: int, size_match_enabled: bool = False) -> Image.Image:
        """ì´ˆê¸° ì´ë¯¸ì§€ ê²€ì¦ ë° ë¦¬ì‚¬ì´ì¦ˆ"""
        if init_image is None:
            raise ValueError("ì´ˆê¸° ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # size_match_enabledê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì›ë³¸ í¬ê¸° ìœ ì§€
        if size_match_enabled:
            print(f"âœ… í¬ê¸° ì¼ì¹˜ ëª¨ë“œ: ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ìœ ì§€ {init_image.size}")
            return init_image
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        if init_image.size != (target_width, target_height):
            print(f"ğŸ”„ ì´ˆê¸° ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •: {init_image.size} -> ({target_width}, {target_height})")
            init_image = init_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
        
        return init_image
    
    def _validate_strength(self, strength: float) -> float:
        """ê°•ë„ ê°’ ê²€ì¦"""
        if strength < 0.0 or strength > 1.0:
            raise ValueError("ê°•ë„ ê°’ì€ 0.0 ~ 1.0 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        return strength
    
    def _apply_sd15_optimizations(self, params: Img2ImgParams):
        """SD15 ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©"""
        if params.model_type != 'SD15':
            return
            
        # SD15ì—ì„œ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì„¤ì •
        if hasattr(self.pipeline.scheduler, 'config'):
            self.pipeline.scheduler.config.use_karras_sigmas = True
            self.pipeline.scheduler.config.karras_rho = 7.0
        
        # SD15ì—ì„œ ë” ë‚˜ì€ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ë§
        if hasattr(self.pipeline.scheduler, 'set_timesteps'):
            self.pipeline.scheduler.set_timesteps(params.steps, device=self.device)
        
        # SD15ì—ì„œ ë” ì•ˆì •ì ì¸ ìƒì„±
        if hasattr(self.pipeline.scheduler, 'beta_start'):
            self.pipeline.scheduler.beta_start = 0.00085
        if hasattr(self.pipeline.scheduler, 'beta_end'):
            self.pipeline.scheduler.beta_end = 0.012
        
        # SD15ì—ì„œ ë” ì„ ëª…í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì„¤ì •
        if hasattr(self.pipeline, 'enable_attention_slicing'):
            self.pipeline.enable_attention_slicing(1)
        if hasattr(self.pipeline, 'enable_vae_slicing'):
            self.pipeline.enable_vae_slicing()
        
        # xformersëŠ” ì„ íƒì  ê¸°ëŠ¥ì´ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            if hasattr(self.pipeline, 'enable_xformers_memory_efficient_attention'):
                self.pipeline.enable_xformers_memory_efficient_attention()
        except ModuleNotFoundError:
            pass  # ì¡°ìš©íˆ ë¬´ì‹œ
    
    async def generate(self, params: Img2ImgParams) -> List[Any]:
        """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰ (Strength ê°’ ìƒì„¸ ê²€ì¦)"""
        import torch
        import numpy as np
        from PIL import Image
        from skimage.metrics import structural_similarity as ssim
        from skimage.metrics import mean_squared_error as mse
        
        print("=" * 100)
        print("ğŸ” [STRENGTH ê²€ì¦] Img2Img Strength ê°’ ìƒì„¸ ë¶„ì„")
        print("=" * 100)
        
        # 1. StateManagerì—ì„œ ê°€ì ¸ì˜¨ strength ê°’ í™•ì¸
        print(f"ğŸ“Š 1ë‹¨ê³„: StateManagerì—ì„œ ê°€ì ¸ì˜¨ strength ê°’")
        print(f"   - params.strength: {params.strength}")
        print(f"   - íƒ€ì…: {type(params.strength)}")
        print(f"   - ë²”ìœ„ ê²€ì¦: {0.0 <= params.strength <= 1.0}")
        
        # íŒŒë¼ë¯¸í„° ê²€ì¦
        size_match_enabled = getattr(params, 'size_match_enabled', False)
        init_image = self._validate_init_image(params.init_image, params.width, params.height, size_match_enabled)
        strength = self._validate_strength(params.strength)
        
        print(f"   - ê²€ì¦ í›„ strength: {strength}")
        print(f"   - ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {init_image.size}")
        print(f"   - ëª©í‘œ í¬ê¸°: {params.width}x{params.height}")
        
        # ìƒì„±ê¸° ì„¤ì •
        generator = torch.Generator(device=self.device)
        if params.seed > 0:
            generator.manual_seed(params.seed)
            print(f"   - ì‹œë“œ ì„¤ì •: {params.seed}")
        else:
            print(f"   - ëœë¤ ì‹œë“œ ì‚¬ìš©")
        
        def _generate_with_strength_validation():
            """Strength ê°’ ê²€ì¦ì„ í¬í•¨í•œ ìƒì„± ë¡œì§"""
            print(f"\nğŸ” 2ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì‹œ ì „ë‹¬ë˜ëŠ” ì‹¤ì œ strength ê°’")
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ timesteps ì„¤ì • ë° ê²€ì¦
            if hasattr(self.pipeline.scheduler, 'set_timesteps'):
                self.pipeline.scheduler.set_timesteps(params.steps, device=self.device)
                print(f"   - ìŠ¤ì¼€ì¤„ëŸ¬ timesteps ì„¤ì •: {params.steps}")
                
                # 3. ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ì˜ timestep ê³„ì‚° ê²€ì¦
                print(f"\nğŸ” 3ë‹¨ê³„: ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ì˜ timestep ê³„ì‚°")
                if hasattr(self.pipeline.scheduler, 'timesteps'):
                    timesteps = self.pipeline.scheduler.timesteps
                    print(f"   - ì „ì²´ timesteps: {len(timesteps)}")
                    print(f"   - ì²« ë²ˆì§¸ timestep: {timesteps[0].item()}")
                    print(f"   - ë§ˆì§€ë§‰ timestep: {timesteps[-1].item()}")
                    
                    # Strengthì— ë”°ë¥¸ ì‹œì‘ timestep ê³„ì‚°
                    start_timestep_idx = int((1.0 - strength) * len(timesteps))
                    start_timestep = timesteps[start_timestep_idx] if start_timestep_idx < len(timesteps) else timesteps[0]
                    
                    print(f"   - Strength {strength} â†’ ì‹œì‘ timestep ì¸ë±ìŠ¤: {start_timestep_idx}")
                    print(f"   - ì‹œì‘ timestep ê°’: {start_timestep.item()}")
                    print(f"   - ê±´ë„ˆë›¸ timesteps: {start_timestep_idx}ê°œ")
                    print(f"   - ì‹¤ì œ ì‹¤í–‰ timesteps: {len(timesteps) - start_timestep_idx}ê°œ")
            
            # 4. init_image latentì™€ ë…¸ì´ì¦ˆ ì ìš©ëœ latent ë¹„êµ
            print(f"\nğŸ” 4ë‹¨ê³„: init_image latent ë¶„ì„")
            init_latent = self._encode_image(init_image)
            print(f"   - ì›ë³¸ ì´ë¯¸ì§€ latent shape: {init_latent.shape}")
            print(f"   - ì›ë³¸ ì´ë¯¸ì§€ latent ë²”ìœ„: [{init_latent.min().item():.3f}, {init_latent.max().item():.3f}]")
            print(f"   - ì›ë³¸ ì´ë¯¸ì§€ latent í‰ê· : {init_latent.mean().item():.3f}")
            print(f"   - ì›ë³¸ ì´ë¯¸ì§€ latent í‘œì¤€í¸ì°¨: {init_latent.std().item():.3f}")
            
            # íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì „ íŒŒë¼ë¯¸í„° ê²€ì¦
            print(f"\nğŸ” 2ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì‹œ ì „ë‹¬ë˜ëŠ” ì‹¤ì œ strength ê°’")
            print(f"   - ì „ë‹¬í•  strength: {strength}")
            print(f"   - ì „ë‹¬í•  steps: {params.steps}")
            print(f"   - ì „ë‹¬í•  cfg_scale: {params.cfg_scale}")
            print(f"   - ì „ë‹¬í•  ì´ë¯¸ì§€ í¬ê¸°: {init_image.size}")
            
            # íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ
            try:
                result = self.pipeline(
                    prompt=params.prompt,
                    negative_prompt=params.negative_prompt,
                    image=init_image,
                    strength=strength,
                    width=params.width,
                    height=params.height,
                    num_inference_steps=params.steps,
                    guidance_scale=params.cfg_scale,
                    generator=generator,
                    num_images_per_prompt=params.batch_size
                )
                
                print(f"   âœ… íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì„±ê³µ")
                
            except Exception as e:
                print(f"   âŒ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                return []
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ë°˜í™˜
            if hasattr(result, 'images'):
                return result.images
            else:
                return result if isinstance(result, list) else [result]
        
        # ìƒì„± ì‹¤í–‰
        generated_images = await asyncio.to_thread(_generate_with_strength_validation)
        
        if not generated_images:
            print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨")
            return []
        
        print(f"\nğŸ” 5ë‹¨ê³„: ê²°ê³¼ ì´ë¯¸ì§€ì™€ ì›ë³¸ ì´ë¯¸ì§€ì˜ ìœ ì‚¬ë„ ì¸¡ì •")
        
        # ê²°ê³¼ ì´ë¯¸ì§€ ë¶„ì„
        result_image = generated_images[0]
        print(f"   - ìƒì„±ëœ ì´ë¯¸ì§€ í¬ê¸°: {result_image.size}")
        print(f"   - ìƒì„±ëœ ì´ë¯¸ì§€ ëª¨ë“œ: {result_image.mode}")
        
        # ì´ë¯¸ì§€ í¬ê¸° í†µì¼ (ë¹„êµë¥¼ ìœ„í•´)
        if result_image.size != init_image.size:
            print(f"   - í¬ê¸° í†µì¼ì„ ìœ„í•´ ë¦¬ì‚¬ì´ì¦ˆ: {result_image.size} â†’ {init_image.size}")
            result_image = result_image.resize(init_image.size, Image.Resampling.LANCZOS)
        
        # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        init_array = np.array(init_image.convert('RGB'))
        result_array = np.array(result_image.convert('RGB'))
        
        # ì •ê·œí™” (0-255 â†’ 0-1)
        init_array_norm = init_array.astype(np.float32) / 255.0
        result_array_norm = result_array.astype(np.float32) / 255.0
        
        # SSIM ê³„ì‚° (ì´ë¯¸ì§€ í¬ê¸° ê²€ì¦ í›„)
        try:
            # ì´ë¯¸ì§€ í¬ê¸°ê°€ SSIM ê³„ì‚°ì— ì¶©ë¶„í•œì§€ í™•ì¸
            min_size = 7  # SSIMì˜ ìµœì†Œ ìœˆë„ìš° í¬ê¸°
            if init_array_norm.shape[0] < min_size or init_array_norm.shape[1] < min_size:
                print(f"   - ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ì•„ SSIM ê³„ì‚° ë¶ˆê°€: {init_array_norm.shape}")
                ssim_score = None
            else:
                # ìœˆë„ìš° í¬ê¸°ë¥¼ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
                win_size = min(7, min(init_array_norm.shape[0], init_array_norm.shape[1]))
                if win_size % 2 == 0:  # ì§ìˆ˜ì¸ ê²½ìš° í™€ìˆ˜ë¡œ ì¡°ì •
                    win_size -= 1
                
                ssim_score = ssim(init_array_norm, result_array_norm, 
                                 multichannel=True, data_range=1.0, 
                                 win_size=win_size)
                print(f"   - SSIM ìœ ì‚¬ë„: {ssim_score:.4f} (win_size={win_size})")
        except Exception as e:
            print(f"   - SSIM ê³„ì‚° ì‹¤íŒ¨: {e}")
            ssim_score = None
        
        # MSE ê³„ì‚°
        try:
            mse_score = mse(init_array_norm, result_array_norm)
            print(f"   - MSE ì˜¤ì°¨: {mse_score:.6f}")
        except Exception as e:
            print(f"   - MSE ê³„ì‚° ì‹¤íŒ¨: {e}")
            mse_score = None
        
        # ì˜ˆìƒ ìœ ì‚¬ë„ì™€ ë¹„êµ
        print(f"\nğŸ“Š Strength {strength} ì˜ˆìƒ vs ì‹¤ì œ ìœ ì‚¬ë„ ë¹„êµ:")
        if strength == 0.3:
            expected_ssim = 0.7
            print(f"   - ì˜ˆìƒ SSIM: {expected_ssim} (ì›ë³¸ê³¼ 70% ìœ ì‚¬)")
        elif strength == 0.8:
            expected_ssim = 0.2
            print(f"   - ì˜ˆìƒ SSIM: {expected_ssim} (ì›ë³¸ê³¼ 20% ìœ ì‚¬)")
        else:
            expected_ssim = 1.0 - strength
            print(f"   - ì˜ˆìƒ SSIM: {expected_ssim:.3f} (1 - strength)")
        
        if ssim_score is not None:
            print(f"   - ì‹¤ì œ SSIM: {ssim_score:.4f}")
            difference = abs(ssim_score - expected_ssim)
            print(f"   - ì°¨ì´: {difference:.4f}")
            
            if difference < 0.1:
                print(f"   âœ… Strengthê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•¨ (ì°¨ì´ < 0.1)")
            elif difference < 0.2:
                print(f"   âš ï¸ Strengthê°€ ë¶€ë¶„ì ìœ¼ë¡œ ì‘ë™í•¨ (ì°¨ì´ < 0.2)")
            else:
                print(f"   âŒ Strengthê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŒ (ì°¨ì´ >= 0.2)")
        
        print("=" * 100)
        print("ğŸ‰ Strength ê²€ì¦ ì™„ë£Œ")
        print("=" * 100)
        
        return generated_images
    
    async def upscale(self, image: Image.Image, scale_factor: float = 2.0) -> Image.Image:
        """ì´ë¯¸ì§€ ì—…ìŠ¤ì¼€ì¼ (ê°„ë‹¨í•œ êµ¬í˜„)"""
        if scale_factor <= 1.0:
            return image
        
        new_width = int(image.width * scale_factor)
        new_height = int(image.height * scale_factor)
        
        print(f"ğŸ”„ ì—…ìŠ¤ì¼€ì¼: {image.size} -> ({new_width}, {new_height})")
        
        # ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì¦ˆ
        upscaled_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        return upscaled_image
    
    async def inpaint(self, image: Image.Image, mask: Image.Image, prompt: str, 
                     negative_prompt: str = "", strength: float = 0.8) -> List[Any]:
        """ì¸í˜ì¸íŒ… (ë§ˆìŠ¤í¬ ê¸°ë°˜ ì´ë¯¸ì§€ ìˆ˜ì •)"""
        print(f"ğŸ¨ ì¸í˜ì¸íŒ… ì‹œì‘ - Strength: {strength}")
        
        # ë§ˆìŠ¤í¬ ê²€ì¦
        if mask.size != image.size:
            print(f"ğŸ”„ ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì •: {mask.size} -> {image.size}")
            mask = mask.resize(image.size, Image.Resampling.LANCZOS)
        
        # ìƒì„±ê¸° ì„¤ì •
        generator = torch.Generator(device=self.device)
        generator.manual_seed(int(torch.randint(0, 2**32 - 1, (1,)).item()))
        
        def _inpaint():
            """ì¸í˜ì¸íŒ… ìƒì„± ë¡œì§"""
            # ì¸í˜ì¸íŒ… íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (íŒŒì´í”„ë¼ì¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°)
            if hasattr(self.pipeline, 'inpaint'):
                result = self.pipeline.inpaint(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=image,
                    mask_image=mask,
                    strength=strength,
                    generator=generator,
                    num_inference_steps=20,
                    guidance_scale=7.0
                )
                return result.images if hasattr(result, 'images') else [result]
            else:
                # ì¸í˜ì¸íŒ…ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì¼ë°˜ img2imgë¡œ ëŒ€ì²´
                print("âš ï¸ ì¸í˜ì¸íŒ…ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤. ì¼ë°˜ img2imgë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                return self.pipeline(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=image,
                    strength=strength,
                    generator=generator,
                    num_inference_steps=20,
                    guidance_scale=7.0
                ).images
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ìˆ˜í–‰
        generated_images = await asyncio.to_thread(_inpaint)
        
        print(f"âœ… ì¸í˜ì¸íŒ… ì™„ë£Œ: {len(generated_images)}ê°œ ì´ë¯¸ì§€")
        return generated_images
