"""
ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ ë„ë©”ì¸ ë¡œì§
UIë‚˜ Servicesì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""

import torch
import asyncio
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from PIL import Image


@dataclass
class Img2ImgParams:
    """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± íŒŒë¼ë¯¸í„°"""
    prompt: str
    negative_prompt: str
    init_image: Image.Image
    strength: float  # 0.0 ~ 1.0 (0.0: ì›ë³¸ ìœ ì§€, 1.0: ì™„ì „íˆ ìƒˆë¡œ ìƒì„±)
    width: int
    height: int
    steps: int
    cfg_scale: float
    seed: int
    sampler: str
    scheduler: str
    batch_size: int
    model_type: str = 'SD15'
    clip_skip: int = 1  # CLIP Skip ì¶”ê°€


class Img2ImgMode:
    """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ (A1111 ìŠ¤íƒ€ì¼)"""
    
    def __init__(self, pipeline: Any, device: str):
        self.pipeline = pipeline
        self.device = device
    
    def _encode_image(self, image: Image.Image) -> torch.Tensor:
        """ì´ë¯¸ì§€ë¥¼ latent spaceë¡œ ì¸ì½”ë”© (A1111 ìŠ¤íƒ€ì¼)"""
        print(f"ğŸ” ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹œì‘: í¬ê¸°={image.size}, ëª¨ë“œ={image.mode}")
        
        with torch.no_grad():
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if hasattr(self.pipeline, 'image_processor'):
                # StableDiffusionImg2ImgPipelineì˜ ê²½ìš°
                image_tensor = self.pipeline.image_processor.preprocess(image)
            else:
                # ì¼ë°˜ StableDiffusionPipelineì˜ ê²½ìš°
                from diffusers.utils import PIL_INTERPOLATION
                image_tensor = self.pipeline.feature_extractor(
                    image, 
                    return_tensors="pt"
                ).pixel_values
            
            image_tensor = image_tensor.to(self.device)
            
            # VAE ì¸ì½”ë”©
            latent = self.pipeline.vae.encode(image_tensor).latent_dist.sample()
            latent = latent * self.pipeline.vae.config.scaling_factor
            
            print(f"âœ… ì´ë¯¸ì§€ ì¸ì½”ë”© ì™„ë£Œ: latent shape={latent.shape}")
            return latent
    
    def _validate_init_image(self, init_image: Image.Image, target_width: int, target_height: int) -> Image.Image:
        """ì´ˆê¸° ì´ë¯¸ì§€ ê²€ì¦ ë° ë¦¬ì‚¬ì´ì¦ˆ"""
        if init_image is None:
            raise ValueError("ì´ˆê¸° ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        if init_image.size != (target_width, target_height):
            print(f"ğŸ”„ ì´ˆê¸° ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •: {init_image.size} -> ({target_width}, {target_height})")
            init_image = init_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
        
        return init_image
    
    def _validate_strength(self, strength: float) -> float:
        """ê°•ë„ ê°’ ê²€ì¦"""
        if strength < 0.0 or strength > 1.0:
            raise ValueError("ê°•ë„ ê°’ì€ 0.0 ~ 1.0 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        return strength
    
    def _apply_sd15_optimizations(self, params: Img2ImgParams):
        """SD15 ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©"""
        if params.model_type != 'SD15':
            return
            
        # SD15ì—ì„œ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì„¤ì •
        if hasattr(self.pipeline.scheduler, 'config'):
            self.pipeline.scheduler.config.use_karras_sigmas = True
            self.pipeline.scheduler.config.karras_rho = 7.0
        
        # SD15ì—ì„œ ë” ë‚˜ì€ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ë§
        if hasattr(self.pipeline.scheduler, 'set_timesteps'):
            self.pipeline.scheduler.set_timesteps(params.steps, device=self.device)
        
        # SD15ì—ì„œ ë” ì•ˆì •ì ì¸ ìƒì„±
        if hasattr(self.pipeline.scheduler, 'beta_start'):
            self.pipeline.scheduler.beta_start = 0.00085
        if hasattr(self.pipeline.scheduler, 'beta_end'):
            self.pipeline.scheduler.beta_end = 0.012
        
        # SD15ì—ì„œ ë” ì„ ëª…í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì„¤ì •
        if hasattr(self.pipeline, 'enable_attention_slicing'):
            self.pipeline.enable_attention_slicing(1)
        if hasattr(self.pipeline, 'enable_vae_slicing'):
            self.pipeline.enable_vae_slicing()
        
        # xformersëŠ” ì„ íƒì  ê¸°ëŠ¥ì´ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            if hasattr(self.pipeline, 'enable_xformers_memory_efficient_attention'):
                self.pipeline.enable_xformers_memory_efficient_attention()
        except ModuleNotFoundError:
            pass  # ì¡°ìš©íˆ ë¬´ì‹œ
    
    async def generate(self, params: Img2ImgParams) -> List[Any]:
        """ì´ë¯¸ì§€-ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰ (A1111 ìŠ¤íƒ€ì¼)"""
        print(f"ğŸ¨ Img2Img ìƒì„± ì‹œì‘ - Seed: {params.seed}, Strength: {params.strength}")
        print(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ - Size: {params.width}x{params.height}, Batch: {params.batch_size}")
        
        # ë””ë²„ê·¸: ì´ˆê¸° ì´ë¯¸ì§€ í™•ì¸
        print(f"ğŸ” Img2Img ëª¨ë“œì—ì„œ init_image í™•ì¸: {params.init_image}")
        if params.init_image:
            print(f"ğŸ” Img2Img ëª¨ë“œì—ì„œ ì´ë¯¸ì§€ í¬ê¸°: {params.init_image.size}, ëª¨ë“œ: {params.init_image.mode}")
        else:
            print(f"âŒ Img2Img ëª¨ë“œì—ì„œ init_imageê°€ None!")
            return []
        
        # íŒŒë¼ë¯¸í„° ê²€ì¦
        init_image = self._validate_init_image(params.init_image, params.width, params.height)
        strength = self._validate_strength(params.strength)
        
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {params.prompt[:100]}...")
        print(f"ğŸš« ë¶€ì • í”„ë¡¬í”„íŠ¸: {params.negative_prompt[:100]}...")
        print(f"âš™ï¸ Steps: {params.steps}, CFG: {params.cfg_scale}, Strength: {strength}")
        
        # SD15 ìµœì í™” ì ìš©
        self._apply_sd15_optimizations(params)
        
        if params.model_type == 'SD15':
            print(f"ğŸ”§ SD15 ìƒì„± ìµœì í™” ì ìš©: Steps={params.steps}, CFG={params.cfg_scale}")
        
        # ìƒì„±ê¸° ì„¤ì •
        generator = torch.Generator(device=self.device)
        if params.seed > 0:
            generator.manual_seed(params.seed)
        
        def _generate():
            """ì‹¤ì œ ìƒì„± ë¡œì§ (A1111 ìŠ¤íƒ€ì¼)"""
            print(f"ğŸ” íŒŒì´í”„ë¼ì¸ íƒ€ì…: {type(self.pipeline)}")
            
            # 1. ì´ë¯¸ì§€ â†’ latent ë³€í™˜
            init_latent = self._encode_image(init_image)
            
            # 2. ë…¸ì´ì¦ˆ ì¶”ê°€ (strengthì— ë”°ë¼)
            noise = torch.randn_like(init_latent)
            
            # 3. timesteps ê³„ì‚° (strengthì— ë”°ë¼)
            timesteps = int(strength * params.steps)
            print(f"ğŸ” ë…¸ì´ì¦ˆ ì£¼ì…: strength={strength}, timesteps={timesteps}")
            
            # 4. ìŠ¤ì¼€ì¤„ëŸ¬ì— ë…¸ì´ì¦ˆ ì¶”ê°€
            noised_latent = self.pipeline.scheduler.add_noise(
                init_latent, 
                noise, 
                torch.tensor([timesteps], device=self.device)
            )
            
            # 5. íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (latents ì‚¬ìš©)
            try:
                # StableDiffusionImg2ImgPipeline ë˜ëŠ” latentsë¥¼ ì§€ì›í•˜ëŠ” íŒŒì´í”„ë¼ì¸
                result = self.pipeline(
                    prompt=params.prompt,
                    negative_prompt=params.negative_prompt,
                    latents=noised_latent,  # 'image' ëŒ€ì‹  'latents' ì‚¬ìš©
                    num_inference_steps=params.steps,
                    guidance_scale=params.cfg_scale,
                    generator=generator,
                    num_images_per_prompt=params.batch_size,
                    # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
                    **({"eta": 1.0} if params.model_type == 'SD15' else {})
                )
            except TypeError as e:
                # latents ì¸ìë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
                print(f"âš ï¸ latents ì¸ì ë¯¸ì§€ì›, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±: {e}")
                result = self.pipeline(
                    prompt=params.prompt,
                    negative_prompt=params.negative_prompt,
                    image=init_image,  # ê¸°ì¡´ ë°©ì‹
                    strength=strength,
                    num_inference_steps=params.steps,
                    guidance_scale=params.cfg_scale,
                    generator=generator,
                    num_images_per_prompt=params.batch_size,
                    **({"eta": 1.0} if params.model_type == 'SD15' else {})
                )
            
            # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ì—ì„œ images ë°˜í™˜
            if hasattr(result, 'images'):
                return result.images
            else:
                # result ìì²´ê°€ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                return result if isinstance(result, list) else [result]
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ìˆ˜í–‰
        generated_images = await asyncio.to_thread(_generate)
        
        print(f"âœ… ìƒì„±ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(generated_images)}")
        for i, image in enumerate(generated_images):
            print(f"âœ… ìƒì„±ëœ ì´ë¯¸ì§€ {i+1} í¬ê¸°: {image.size}")
        
        return generated_images
    
    async def upscale(self, image: Image.Image, scale_factor: float = 2.0) -> Image.Image:
        """ì´ë¯¸ì§€ ì—…ìŠ¤ì¼€ì¼ (ê°„ë‹¨í•œ êµ¬í˜„)"""
        if scale_factor <= 1.0:
            return image
        
        new_width = int(image.width * scale_factor)
        new_height = int(image.height * scale_factor)
        
        print(f"ğŸ”„ ì—…ìŠ¤ì¼€ì¼: {image.size} -> ({new_width}, {new_height})")
        
        # ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì¦ˆ
        upscaled_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        return upscaled_image
    
    async def inpaint(self, image: Image.Image, mask: Image.Image, prompt: str, 
                     negative_prompt: str = "", strength: float = 0.8) -> List[Any]:
        """ì¸í˜ì¸íŒ… (ë§ˆìŠ¤í¬ ê¸°ë°˜ ì´ë¯¸ì§€ ìˆ˜ì •)"""
        print(f"ğŸ¨ ì¸í˜ì¸íŒ… ì‹œì‘ - Strength: {strength}")
        
        # ë§ˆìŠ¤í¬ ê²€ì¦
        if mask.size != image.size:
            print(f"ğŸ”„ ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì •: {mask.size} -> {image.size}")
            mask = mask.resize(image.size, Image.Resampling.LANCZOS)
        
        # ìƒì„±ê¸° ì„¤ì •
        generator = torch.Generator(device=self.device)
        generator.manual_seed(int(torch.randint(0, 2**32 - 1, (1,)).item()))
        
        def _inpaint():
            """ì¸í˜ì¸íŒ… ìƒì„± ë¡œì§"""
            # ì¸í˜ì¸íŒ… íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (íŒŒì´í”„ë¼ì¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°)
            if hasattr(self.pipeline, 'inpaint'):
                result = self.pipeline.inpaint(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=image,
                    mask_image=mask,
                    strength=strength,
                    generator=generator,
                    num_inference_steps=20,
                    guidance_scale=7.0
                )
                return result.images if hasattr(result, 'images') else [result]
            else:
                # ì¸í˜ì¸íŒ…ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì¼ë°˜ img2imgë¡œ ëŒ€ì²´
                print("âš ï¸ ì¸í˜ì¸íŒ…ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤. ì¼ë°˜ img2imgë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                return self.pipeline(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=image,
                    strength=strength,
                    generator=generator,
                    num_inference_steps=20,
                    guidance_scale=7.0
                ).images
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ìˆ˜í–‰
        generated_images = await asyncio.to_thread(_inpaint)
        
        print(f"âœ… ì¸í˜ì¸íŒ… ì™„ë£Œ: {len(generated_images)}ê°œ ì´ë¯¸ì§€")
        return generated_images
