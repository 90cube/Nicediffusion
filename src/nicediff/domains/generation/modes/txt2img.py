from ....core.logger import (
    debug, info, warning, error, success, failure, warning_emoji, 
    info_emoji, debug_emoji, process_emoji, model_emoji, image_emoji, ui_emoji, canvas_emoji
)
"""
í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ ë„ë©”ì¸ ë¡œì§
UIë‚˜ Servicesì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""

import torch
import asyncio
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from typing import Union, TYPE_CHECKING, Any
if TYPE_CHECKING:
    from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipeline
    from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline

from ..services.scheduler_manager import SchedulerManager
from ..services.advanced_encoder import AdvancedTextEncoder


@dataclass
class Txt2ImgParams:
    """í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± íŒŒë¼ë¯¸í„°"""
    prompt: str
    negative_prompt: str
    width: int
    height: int
    steps: int
    cfg_scale: float
    seed: int
    sampler: str
    scheduler: str
    batch_size: int
    model_type: str = 'SD15'
    clip_skip: int = 1  # CLIP Skip ì¶”ê°€
    use_custom_tokenizer: bool = True  # ê³ ê¸‰ ì¸ì½”ë”© ì„¤ì •
    weight_interpretation: str = "A1111"  # ê°€ì¤‘ì¹˜ ì²˜ë¦¬ ë°©ì‹
    
    def __post_init__(self):
        """SD15 ëª¨ë¸ì˜ ê²½ìš° ê¸°ë³¸ê°’ ìµœì í™”"""
        if self.model_type == 'SD15':
            # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ê¸°ë³¸ê°’
            if self.steps < 20:
                self.steps = 20  # ìµœì†Œ steps ë³´ì¥
            if self.cfg_scale < 6.0:
                self.cfg_scale = 6.0  # ìµœì†Œ CFG ë³´ì¥
            if not self.scheduler or self.scheduler == "default":
                self.scheduler = "karras"  # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆ


class Txt2ImgMode:
    """í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ"""
    
    def __init__(self, pipeline: Any, device: str):
        self.pipeline = pipeline
        self.device = device
    
    def _truncate_prompt_with_tokenizer(self, text: str, max_tokens: int, tokenizer) -> str:
        """í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ"""
        if not text.strip():
            return text
            
        try:
            text_inputs = tokenizer(
                text,
                padding="longest",
                return_tensors="pt",
            )
            input_ids = text_inputs.input_ids[0]
            
            if len(input_ids) <= max_tokens:
                return text
            
            # í† í° ìˆ˜ë¥¼ ì œí•œí•˜ê³  ë‹¤ì‹œ ë””ì½”ë”©
            truncated_ids = input_ids[:max_tokens]
            truncated_text = tokenizer.decode(truncated_ids, skip_special_tokens=True)
            
            # ì˜ë¦° ë¶€ë¶„ í‘œì‹œ
            if len(input_ids) > max_tokens:
                warning_emoji(f"í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(input_ids)} > {max_tokens} í† í°). ìë™ìœ¼ë¡œ ì˜ë¦½ë‹ˆë‹¤.")
                info(f"   ì˜ë¦° ë¶€ë¶„: {text[len(truncated_text):].strip()}")
            
            return truncated_text
        except Exception as e:
            warning_emoji(f"í† í° ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê°„ë‹¨í•œ ì¶”ì •
            words = text.split()
            if len(words) > max_tokens:
                return ' '.join(words[:max_tokens])
            return text
    
    def _apply_sd15_optimizations(self, params: Txt2ImgParams):
        """SD15 ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©"""
        if params.model_type != 'SD15':
            return
            
        info(r"ğŸ”§ SD15 í’ˆì§ˆ ìµœì í™” ì ìš© ì¤‘...")
        
        # 1. ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™”
        if hasattr(self.pipeline.scheduler, 'config'):
            # Karras ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™” (ë” ë‚˜ì€ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ë§)
            self.pipeline.scheduler.config.use_karras_sigmas = True
            self.pipeline.scheduler.config.karras_rho = 7.0
            
            # SD15ì—ì„œ ë” ì•ˆì •ì ì¸ ìƒì„±
            if hasattr(self.pipeline.scheduler.config, 'beta_start'):
                self.pipeline.scheduler.config.beta_start = 0.00085
            if hasattr(self.pipeline.scheduler.config, 'beta_end'):
                self.pipeline.scheduler.config.beta_end = 0.012
        
        # 2. ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì„ìŠ¤í… ì„¤ì •
        if hasattr(self.pipeline.scheduler, 'set_timesteps'):
            self.pipeline.scheduler.set_timesteps(params.steps, device=self.device)
        
        # 3. ë©”ëª¨ë¦¬ ìµœì í™” (ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•´)
        if hasattr(self.pipeline, 'enable_attention_slicing'):
            self.pipeline.enable_attention_slicing(1)
        if hasattr(self.pipeline, 'enable_vae_slicing'):
            self.pipeline.enable_vae_slicing()
        
        # 4. PyTorch 2.0+ SDPA ìµœì í™”
        if hasattr(self.pipeline, 'enable_model_cpu_offload'):
            self.pipeline.enable_model_cpu_offload()
        
        # 5. xformers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ (ì„ íƒì )
        try:
            if hasattr(self.pipeline, 'enable_xformers_memory_efficient_attention'):
                self.pipeline.enable_xformers_memory_efficient_attention()
                success(r"xformers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ í™œì„±í™”")
        except (ModuleNotFoundError, AttributeError) as e:
            warning_emoji(f"xformers ë¯¸ì‚¬ìš©: {e}")
            success(r"PyTorch 2.0+ SDPA ì‚¬ìš© ì¤‘")
        
        # 6. ëª¨ë¸ ì •ë°€ë„ ìµœì í™”
        if hasattr(self.pipeline, 'text_encoder'):
            self.pipeline.text_encoder = self.pipeline.text_encoder.to(torch.float16)
        
        if hasattr(self.pipeline, 'vae'):
            self.pipeline.vae = self.pipeline.vae.to(torch.float16)
        
        # 7. SD15 íŠ¹í™” í’ˆì§ˆ ê°œì„  ì„¤ì •
        if hasattr(self.pipeline, 'unet'):
            # UNetì„ float16ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì†ë„ í–¥ìƒ
            self.pipeline.unet = self.pipeline.unet.to(torch.float16)
        
        # 8. ì¶”ê°€ í’ˆì§ˆ ê°œì„  ì„¤ì •
        if hasattr(self.pipeline.scheduler, 'config'):
            # ë” ì •ë°€í•œ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ë§
            if hasattr(self.pipeline.scheduler.config, 'timestep_spacing'):
                self.pipeline.scheduler.config.timestep_spacing = 'trailing'
            
            # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
            if hasattr(self.pipeline.scheduler.config, 'algorithm_type'):
                self.pipeline.scheduler.config.algorithm_type = 'dpmsolver++'
        
        success(r"SD15 í’ˆì§ˆ ìµœì í™” ì™„ë£Œ")
    
    def _validate_scheduler_application(self, expected_sampler: str, expected_scheduler: str):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© ê²€ì¦"""
        try:
            current_scheduler = self.pipeline.scheduler.__class__.__name__
            debug_emoji(f"í˜„ì¬ ì ìš©ëœ ìŠ¤ì¼€ì¤„ëŸ¬: {current_scheduler}")
            
            # ì„¤ì • í™•ì¸
            if hasattr(self.pipeline.scheduler, 'config'):
                config = self.pipeline.scheduler.config
                debug_emoji(r"ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •:")
                info(f"   - use_karras_sigmas: {getattr(config, 'use_karras_sigmas', 'N/A')}")
                info(f"   - algorithm_type: {getattr(config, 'algorithm_type', 'N/A')}")
                info(f"   - solver_order: {getattr(config, 'solver_order', 'N/A')}")
            
            return True
        except Exception as e:
            warning_emoji(f"ìŠ¤ì¼€ì¤„ëŸ¬ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    async def generate(self, params: Txt2ImgParams) -> List[Any]:
        """í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰"""
        canvas_emoji(r"Txt2Img ìƒì„± ì‹œì‘ - Seed: {params.seed}")
        info(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ - Size: {params.width}x{params.height}, Batch: {params.batch_size}")
        
        # 1. ìŠ¤ì¼€ì¤„ëŸ¬/ìƒ˜í”ŒëŸ¬ ì‹¤ì œ ì ìš©
        SchedulerManager.apply_scheduler_to_pipeline(
            self.pipeline, 
            params.sampler, 
            params.scheduler
        )
        
        # 2. CLIP Skip ì‹¤ì œ ì ìš©
        if params.clip_skip > 1:
            SchedulerManager.apply_clip_skip_to_pipeline(
                self.pipeline, 
                params.clip_skip
            )
        
        # 3. ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© ê²€ì¦
        self._validate_scheduler_application(params.sampler, params.scheduler)
        
        # ê³ ê¸‰ í…ìŠ¤íŠ¸ ì¸ì½”ë” ì‚¬ìš© (77í† í° ì œí•œ í•´ì œ)
        use_custom = getattr(params, 'use_custom_tokenizer', True)
        weight_mode = getattr(params, 'weight_interpretation', 'A1111')
        
        encoder = AdvancedTextEncoder(
            self.pipeline, 
            weight_mode=weight_mode,
            use_custom_tokenizer=use_custom
        )
        
        # í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”© (77í† í° ì œí•œ ì—†ìŒ, SDXL ì§€ì›)
        info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”© - ëª¨ë“œ: {weight_mode}, ì»¤ìŠ¤í…€: {use_custom}")
        prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, pooled_negative_prompt_embeds = encoder.encode_prompt_with_pooled(
            params.prompt, 
            params.negative_prompt
        )
        
        success(r"ì„ë² ë”© ìƒì„± ì™„ë£Œ:")
        if prompt_embeds is not None and hasattr(prompt_embeds, 'shape'):
            info(f"   - ê¸ì •: {prompt_embeds.shape}")
        if negative_prompt_embeds is not None and hasattr(negative_prompt_embeds, 'shape'):
            info(f"   - ë¶€ì •: {negative_prompt_embeds.shape}")
        if pooled_prompt_embeds is not None and hasattr(pooled_prompt_embeds, 'shape'):
            info(f"   - ê¸ì • pooled: {pooled_prompt_embeds.shape}")
        if pooled_negative_prompt_embeds is not None and hasattr(pooled_negative_prompt_embeds, 'shape'):
            info(f"   - ë¶€ì • pooled: {pooled_negative_prompt_embeds.shape}")
        else:
            info(r"   - SD15 ëª¨ë¸ (pooled ì„ë² ë”© ì—†ìŒ)")
        
        info(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {params.prompt[:100]}...")
        info(f"ğŸš« ë¶€ì • í”„ë¡¬í”„íŠ¸: {params.negative_prompt[:100]}...")
        info(f"âš™ï¸ Steps: {params.steps}, CFG: {params.cfg_scale}, Sampler: {params.sampler}, Scheduler: {params.scheduler}, CLIP Skip: {params.clip_skip}")
        
        # SD15 ìµœì í™” ì ìš©
        self._apply_sd15_optimizations(params)
        
        if params.model_type == 'SD15':
            info(f"ğŸ”§ SD15 ìƒì„± ìµœì í™” ì ìš©: Steps={params.steps}, CFG={params.cfg_scale}")
        
        def _generate():
            """ì‹¤ì œ ìƒì„± ë¡œì§"""
            import torch  # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ torch import
            
            # ìƒì„±ê¸° ì„¤ì • - íŒŒì´í”„ë¼ì¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ ì‚¬ìš© (SDXL í˜¸í™˜)
            try:
                # íŒŒì´í”„ë¼ì¸ ë””ë°”ì´ìŠ¤ ìƒíƒœ ìƒì„¸ í™•ì¸
                debug_emoji(r"íŒŒì´í”„ë¼ì¸ ë””ë°”ì´ìŠ¤ ìƒíƒœ í™•ì¸:")
                info(f"   - CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
                if torch.cuda.is_available():
                    info(f"   - CUDA ë””ë°”ì´ìŠ¤ ìˆ˜: {torch.cuda.device_count()}")
                    info(f"   - í˜„ì¬ CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.current_device()}")
                
                # íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸ë³„ ë””ë°”ì´ìŠ¤ í™•ì¸
                if hasattr(self.pipeline, 'unet'):
                    unet_device = next(self.pipeline.unet.parameters()).device
                    info(f"   - UNet ë””ë°”ì´ìŠ¤: {unet_device}")
                if hasattr(self.pipeline, 'text_encoder'):
                    text_encoder_device = next(self.pipeline.text_encoder.parameters()).device
                    info(f"   - Text Encoder ë””ë°”ì´ìŠ¤: {text_encoder_device}")
                if hasattr(self.pipeline, 'vae'):
                    vae_device = next(self.pipeline.vae.parameters()).device
                    info(f"   - VAE ë””ë°”ì´ìŠ¤: {vae_device}")
                
                # SDXL íŒŒì´í”„ë¼ì¸ì—ì„œëŠ” parameters()ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                if hasattr(self.pipeline, 'parameters'):
                    pipeline_device = next(self.pipeline.parameters()).device
                else:
                    # SDXL íŒŒì´í”„ë¼ì¸ì˜ ê²½ìš° text_encoderì—ì„œ ë””ë°”ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    if hasattr(self.pipeline, 'text_encoder'):
                        pipeline_device = next(self.pipeline.text_encoder.parameters()).device
                    else:
                        # GPU ê°•ì œ ì‚¬ìš© (RTX 4090 í™œìš©)
                        if torch.cuda.is_available():
                            pipeline_device = torch.device('cuda')
                            info(f"   - GPU ê°•ì œ ì‚¬ìš©: {pipeline_device}")
                        else:
                            pipeline_device = torch.device('cpu')
                            info(f"   - GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥, CPU ì‚¬ìš©: {pipeline_device}")
                
                info(f"   - ìµœì¢… íŒŒì´í”„ë¼ì¸ ë””ë°”ì´ìŠ¤: {pipeline_device}")
                
            except Exception as e:
                warning_emoji(f"íŒŒì´í”„ë¼ì¸ ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {e}")
                pipeline_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            
            generator = torch.Generator(device=pipeline_device)
            if params.seed > 0:
                generator.manual_seed(params.seed)
            
            info(f"ğŸ”§ Generator ì„¤ì •: device={pipeline_device}, seed={params.seed}")
            
            # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            extra_params: dict = {
                'output_type': 'pil',  # PIL ì´ë¯¸ì§€ë¡œ ì§ì ‘ ë°˜í™˜
            }
            
            # SD15ì—ì„œë§Œ íŠ¹ë³„í•œ ìµœì í™” ì ìš©
            if params.model_type == 'SD15':
                extra_params['eta'] = 1.0  # DDIM ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ì‚¬ìš©
                
                # guidance_rescaleì€ íŠ¹ì • ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œë§Œ ì‚¬ìš©
                if params.scheduler in ['karras', 'exponential']:
                    extra_params['guidance_rescale'] = 0.7
            
            # ì‹¤ì œ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ íŒŒë¼ë¯¸í„° ë¡œê¹… (ê³ ê¸‰ ì¸ì½”ë” ì‚¬ìš©, SDXL ì§€ì›)
            pipeline_params = {
                'prompt_embeds': prompt_embeds,
                'negative_prompt_embeds': negative_prompt_embeds,
                'height': params.height,
                'width': params.width,
                'num_inference_steps': params.steps,
                'guidance_scale': params.cfg_scale,
                'generator': generator,
                'num_images_per_prompt': params.batch_size,
                **extra_params
            }
            
            # SDXL ëª¨ë¸ì¸ ê²½ìš° pooled ì„ë² ë”© ì¶”ê°€
            if pooled_prompt_embeds is not None:
                pipeline_params['pooled_prompt_embeds'] = pooled_prompt_embeds
                pipeline_params['negative_pooled_prompt_embeds'] = pooled_negative_prompt_embeds
                info(r"   - SDXL ëª¨ë¸: pooled ì„ë² ë”© ì¶”ê°€ë¨")
            else:
                info(r"   - SD15 ëª¨ë¸: ê¸°ë³¸ ì„ë² ë”©ë§Œ ì‚¬ìš©")
            
            info(r"ğŸš€ ì‹¤ì œ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ íŒŒë¼ë¯¸í„°:")
            info(f"   - Steps: {pipeline_params['num_inference_steps']}")
            info(f"   - CFG: {pipeline_params['guidance_scale']}")
            info(f"   - Size: {pipeline_params['width']}x{pipeline_params['height']}")
            info(f"   - Batch: {pipeline_params['num_images_per_prompt']}")
            info(f"   - Generator Device: {generator.device}")
            info(f"   - Extra: {extra_params}")
            
            try:
                result = self.pipeline(**pipeline_params)
                
                # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ì—ì„œ images ë°˜í™˜
                if hasattr(result, 'images'):
                    return result.images
                else:
                    # result ìì²´ê°€ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                    return result if isinstance(result, list) else [result]
            except Exception as e:
                failure(f"íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                return []
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ìˆ˜í–‰
        generated_images = await asyncio.to_thread(_generate)
        
        # ê²°ê³¼ ê²€ì¦
        if generated_images is None:
            failure(r"íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
            return []
        
        if not isinstance(generated_images, list):
            generated_images = [generated_images]
        
        success(f"ìƒì„±ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(generated_images)}")
        for i, image in enumerate(generated_images):
            if hasattr(image, 'size'):
                success(f"ìƒì„±ëœ ì´ë¯¸ì§€ {i+1} í¬ê¸°: {image.size}")
            else:
                success(f"ìƒì„±ëœ ì´ë¯¸ì§€ {i+1}: {type(image)}")
        
        return generated_images
