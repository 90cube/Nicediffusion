"""
í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ ë„ë©”ì¸ ë¡œì§
UIë‚˜ Servicesì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""

import torch
import asyncio
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from typing import Union, TYPE_CHECKING, Any
if TYPE_CHECKING:
    from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipeline
    from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline

from ..services.scheduler_manager import SchedulerManager


@dataclass
class Txt2ImgParams:
    """í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± íŒŒë¼ë¯¸í„°"""
    prompt: str
    negative_prompt: str
    width: int
    height: int
    steps: int
    cfg_scale: float
    seed: int
    sampler: str
    scheduler: str
    batch_size: int
    model_type: str = 'SD15'
    clip_skip: int = 1  # CLIP Skip ì¶”ê°€
    
    def __post_init__(self):
        """SD15 ëª¨ë¸ì˜ ê²½ìš° ê¸°ë³¸ê°’ ìµœì í™”"""
        if self.model_type == 'SD15':
            # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ê¸°ë³¸ê°’
            if self.steps < 20:
                self.steps = 20  # ìµœì†Œ steps ë³´ì¥
            if self.cfg_scale < 6.0:
                self.cfg_scale = 6.0  # ìµœì†Œ CFG ë³´ì¥
            if not self.scheduler or self.scheduler == "default":
                self.scheduler = "karras"  # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆ


class Txt2ImgMode:
    """í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëª¨ë“œ"""
    
    def __init__(self, pipeline: Any, device: str):
        self.pipeline = pipeline
        self.device = device
    
    def _truncate_prompt_with_tokenizer(self, text: str, max_tokens: int, tokenizer) -> str:
        """í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ"""
        if not text.strip():
            return text
            
        try:
            text_inputs = tokenizer(
                text,
                padding="longest",
                return_tensors="pt",
            )
            input_ids = text_inputs.input_ids[0]
            
            if len(input_ids) <= max_tokens:
                return text
            
            # í† í° ìˆ˜ë¥¼ ì œí•œí•˜ê³  ë‹¤ì‹œ ë””ì½”ë”©
            truncated_ids = input_ids[:max_tokens]
            truncated_text = tokenizer.decode(truncated_ids, skip_special_tokens=True)
            
            # ì˜ë¦° ë¶€ë¶„ í‘œì‹œ
            if len(input_ids) > max_tokens:
                print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(input_ids)} > {max_tokens} í† í°). ìë™ìœ¼ë¡œ ì˜ë¦½ë‹ˆë‹¤.")
                print(f"   ì˜ë¦° ë¶€ë¶„: {text[len(truncated_text):].strip()}")
            
            return truncated_text
        except Exception as e:
            print(f"âš ï¸ í† í° ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê°„ë‹¨í•œ ì¶”ì •
            words = text.split()
            if len(words) > max_tokens:
                return ' '.join(words[:max_tokens])
            return text
    
    def _apply_sd15_optimizations(self, params: Txt2ImgParams):
        """SD15 ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©"""
        if params.model_type != 'SD15':
            return
            
        print("ğŸ”§ SD15 í’ˆì§ˆ ìµœì í™” ì ìš© ì¤‘...")
        
        # 1. ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™”
        if hasattr(self.pipeline.scheduler, 'config'):
            # Karras ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™” (ë” ë‚˜ì€ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ë§)
            self.pipeline.scheduler.config.use_karras_sigmas = True
            self.pipeline.scheduler.config.karras_rho = 7.0
            
            # SD15ì—ì„œ ë” ì•ˆì •ì ì¸ ìƒì„±
            if hasattr(self.pipeline.scheduler.config, 'beta_start'):
                self.pipeline.scheduler.config.beta_start = 0.00085
            if hasattr(self.pipeline.scheduler.config, 'beta_end'):
                self.pipeline.scheduler.config.beta_end = 0.012
        
        # 2. ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì„ìŠ¤í… ì„¤ì •
        if hasattr(self.pipeline.scheduler, 'set_timesteps'):
            self.pipeline.scheduler.set_timesteps(params.steps, device=self.device)
        
        # 3. ë©”ëª¨ë¦¬ ìµœì í™” (ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•´)
        if hasattr(self.pipeline, 'enable_attention_slicing'):
            self.pipeline.enable_attention_slicing(1)
        if hasattr(self.pipeline, 'enable_vae_slicing'):
            self.pipeline.enable_vae_slicing()
        
        # 4. PyTorch 2.0+ SDPA ìµœì í™”
        if hasattr(self.pipeline, 'enable_model_cpu_offload'):
            self.pipeline.enable_model_cpu_offload()
        
        # 5. xformers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ (ì„ íƒì )
        try:
            if hasattr(self.pipeline, 'enable_xformers_memory_efficient_attention'):
                self.pipeline.enable_xformers_memory_efficient_attention()
                print("âœ… xformers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ í™œì„±í™”")
        except (ModuleNotFoundError, AttributeError) as e:
            print(f"âš ï¸ xformers ë¯¸ì‚¬ìš©: {e}")
            print("âœ… PyTorch 2.0+ SDPA ì‚¬ìš© ì¤‘")
        
        # 6. ëª¨ë¸ ì •ë°€ë„ ìµœì í™”
        if hasattr(self.pipeline, 'text_encoder'):
            self.pipeline.text_encoder = self.pipeline.text_encoder.to(torch.float16)
        
        if hasattr(self.pipeline, 'vae'):
            self.pipeline.vae = self.pipeline.vae.to(torch.float16)
        
        # 7. SD15 íŠ¹í™” í’ˆì§ˆ ê°œì„  ì„¤ì •
        if hasattr(self.pipeline, 'unet'):
            # UNetì„ float16ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì†ë„ í–¥ìƒ
            self.pipeline.unet = self.pipeline.unet.to(torch.float16)
        
        # 8. ì¶”ê°€ í’ˆì§ˆ ê°œì„  ì„¤ì •
        if hasattr(self.pipeline.scheduler, 'config'):
            # ë” ì •ë°€í•œ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ë§
            if hasattr(self.pipeline.scheduler.config, 'timestep_spacing'):
                self.pipeline.scheduler.config.timestep_spacing = 'trailing'
            
            # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
            if hasattr(self.pipeline.scheduler.config, 'algorithm_type'):
                self.pipeline.scheduler.config.algorithm_type = 'dpmsolver++'
        
        print("âœ… SD15 í’ˆì§ˆ ìµœì í™” ì™„ë£Œ")
    
    async def generate(self, params: Txt2ImgParams) -> List[Any]:
        """í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰"""
        print(f"ğŸ¨ Txt2Img ìƒì„± ì‹œì‘ - Seed: {params.seed}")
        print(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ - Size: {params.width}x{params.height}, Batch: {params.batch_size}")
        
        # 1. ìŠ¤ì¼€ì¤„ëŸ¬/ìƒ˜í”ŒëŸ¬ ì‹¤ì œ ì ìš©
        SchedulerManager.apply_scheduler_to_pipeline(
            self.pipeline, 
            params.sampler, 
            params.scheduler
        )
        
        # 2. CLIP Skip ì‹¤ì œ ì ìš©
        if params.clip_skip > 1:
            SchedulerManager.apply_clip_skip_to_pipeline(
                self.pipeline, 
                params.clip_skip
            )
        
        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ (SD15: 77 í† í°, SDXL: 77 í† í°)
        max_tokens = 77
        
        # íŒŒì´í”„ë¼ì¸ì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©
        if hasattr(self.pipeline, 'tokenizer'):
            prompt = self._truncate_prompt_with_tokenizer(params.prompt, max_tokens, self.pipeline.tokenizer)
            negative_prompt = self._truncate_prompt_with_tokenizer(params.negative_prompt, max_tokens, self.pipeline.tokenizer)
        else:
            # í† í¬ë‚˜ì´ì €ê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            prompt = params.prompt
            negative_prompt = params.negative_prompt
            
            prompt_tokens = len(prompt.split())
            negative_tokens = len(negative_prompt.split())
            
            if prompt_tokens > max_tokens:
                print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({prompt_tokens} > {max_tokens} í† í°). ìë™ìœ¼ë¡œ ì˜ë¦½ë‹ˆë‹¤.")
                words = prompt.split()
                prompt = ' '.join(words[:max_tokens])
            
            if negative_tokens > max_tokens:
                print(f"âš ï¸ ë¶€ì • í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({negative_tokens} > {max_tokens} í† í°). ìë™ìœ¼ë¡œ ì˜ë¦½ë‹ˆë‹¤.")
                words = negative_prompt.split()
                negative_prompt = ' '.join(words[:max_tokens])
        
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {prompt[:100]}...")
        print(f"ğŸš« ë¶€ì • í”„ë¡¬í”„íŠ¸: {negative_prompt[:100]}...")
        print(f"âš™ï¸ Steps: {params.steps}, CFG: {params.cfg_scale}, Sampler: {params.sampler}, Scheduler: {params.scheduler}, CLIP Skip: {params.clip_skip}")
        
        # SD15 ìµœì í™” ì ìš©
        self._apply_sd15_optimizations(params)
        
        if params.model_type == 'SD15':
            print(f"ğŸ”§ SD15 ìƒì„± ìµœì í™” ì ìš©: Steps={params.steps}, CFG={params.cfg_scale}")
        
        # ìƒì„±ê¸° ì„¤ì •
        generator = torch.Generator(device=self.device)
        if params.seed > 0:
            generator.manual_seed(params.seed)
        
        def _generate():
            """ì‹¤ì œ ìƒì„± ë¡œì§"""
            # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
            extra_params = {}
            if params.model_type == 'SD15':
                extra_params.update({
                    'eta': 1.0,  # DDIM ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ì‚¬ìš©
                    'output_type': 'pil',  # PIL ì´ë¯¸ì§€ë¡œ ì§ì ‘ ë°˜í™˜
                    'guidance_rescale': 0.7,  # SD15ì—ì„œ ë” ì•ˆì •ì ì¸ ìƒì„±
                })
            
            # ì‹¤ì œ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ íŒŒë¼ë¯¸í„° ë¡œê¹…
            pipeline_params = {
                'prompt': prompt,
                'negative_prompt': negative_prompt,
                'height': params.height,
                'width': params.width,
                'num_inference_steps': params.steps,
                'guidance_scale': params.cfg_scale,
                'generator': generator,
                'num_images_per_prompt': params.batch_size,
                **extra_params
            }
            
            print(f"ğŸš€ ì‹¤ì œ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ íŒŒë¼ë¯¸í„°:")
            print(f"   - Steps: {pipeline_params['num_inference_steps']}")
            print(f"   - CFG: {pipeline_params['guidance_scale']}")
            print(f"   - Size: {pipeline_params['width']}x{pipeline_params['height']}")
            print(f"   - Batch: {pipeline_params['num_images_per_prompt']}")
            print(f"   - Extra: {extra_params}")
            
            result = self.pipeline(**pipeline_params)
            
            # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ì—ì„œ images ë°˜í™˜
            if hasattr(result, 'images'):
                return result.images
            else:
                # result ìì²´ê°€ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                return result if isinstance(result, list) else [result]
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ìˆ˜í–‰
        generated_images = await asyncio.to_thread(_generate)
        
        print(f"âœ… ìƒì„±ëœ ì´ë¯¸ì§€ ê°œìˆ˜: {len(generated_images)}")
        for i, image in enumerate(generated_images):
            print(f"âœ… ìƒì„±ëœ ì´ë¯¸ì§€ {i+1} í¬ê¸°: {image.size}")
        
        return generated_images
