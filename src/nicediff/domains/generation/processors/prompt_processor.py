"""
í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ë„ë©”ì¸ ë¡œì§
BREAK í‚¤ì›Œë“œ, ê°€ì¤‘ì¹˜ êµ¬ë¬¸, í† í° ìµœì í™”, ì²­í‚¹ ë“±ì„ ë‹´ë‹¹
"""

import re
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
import torch


@dataclass
class PromptAnalysis:
    """í”„ë¡¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼"""
    token_count: int
    segments: List[str]
    weights: Dict[str, float]
    suggestions: List[str]
    is_optimized: bool
    truncated_parts: List[str]  # ì˜ë¦° ë¶€ë¶„ ì¶”ì 


@dataclass
class WeightedPrompt:
    """ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ í”„ë¡¬í”„íŠ¸"""
    original: str
    processed: str
    weights: Dict[str, float]
    token_count: int
    chunks: List['PromptChunk']  # ì²­í‚¹ëœ í”„ë¡¬í”„íŠ¸


@dataclass
class PromptChunk:
    """í”„ë¡¬í”„íŠ¸ ì²­í¬"""
    text: str
    token_count: int
    importance_score: float
    chunk_type: str  # 'main', 'style', 'quality', 'detail'


class PromptProcessor:
    """ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, model_type='SD15'):
        # SDXLì€ 225 í† í°, SD15ëŠ” 77 í† í° ì§€ì›
        self.model_type = model_type
        self.max_tokens = 225 if model_type == 'SDXL' else 77
        self.warning_threshold = 200 if model_type == 'SDXL' else 70
        self.break_keyword = "BREAK"
        
        # ê°€ì¤‘ì¹˜ íŒ¨í„´ ì •ê·œì‹
        self.weight_pattern = r'\(([^:)]+)(?::([0-9]*\.?[0-9]+))?\)'
        self.negative_weight_pattern = r'\[([^\]]+)\]'
        
        # í‚¤ì›Œë“œ ì¤‘ìš”ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¤‘ìš”)
        self.importance_scores = {
            # ë©”ì¸ ì½˜ì…‰íŠ¸ (ê°€ì¥ ì¤‘ìš”)
            'portrait': 10, 'landscape': 10, 'character': 10, 'scene': 10,
            # ìŠ¤íƒ€ì¼ (ì¤‘ìš”)
            'anime': 8, 'realistic': 8, 'cartoon': 8, 'oil painting': 8,
            'watercolor': 8, 'digital art': 8, 'photography': 8,
            # í’ˆì§ˆ (ì¤‘ê°„)
            'high quality': 6, 'masterpiece': 6, 'best quality': 6,
            'detailed': 6, 'sharp focus': 6, 'professional': 6,
            # ì„¸ë¶€ì‚¬í•­ (ë‚®ìŒ)
            'lighting': 4, 'composition': 4, 'background': 4,
            'clothing': 3, 'accessories': 3, 'texture': 3,
        }
    
    def process_prompt(self, prompt: str, tokenizer=None) -> WeightedPrompt:
        """ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ (BREAK í‚¤ì›Œë“œ, ê°€ì¤‘ì¹˜ êµ¬ë¬¸, ì²­í‚¹)"""
        if not prompt.strip():
            return WeightedPrompt(
                original=prompt,
                processed=prompt,
                weights={},
                token_count=0,
                chunks=[]
            )
        
        # 1. BREAK í‚¤ì›Œë“œë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬
        segments = self._split_by_break(prompt)
        
        # 2. ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° ì²˜ë¦¬
        weights = self._extract_weights(prompt)
        
        # 3. ê°€ì¤‘ì¹˜ ì œê±°ëœ ê¹¨ë—í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        clean_prompt = self._remove_weight_syntax(prompt)
        
        # 4. í† í° ìˆ˜ ê³„ì‚°
        token_count = self._calculate_token_count(clean_prompt, tokenizer)
        
        # 5. í† í° ì œí•œ ì´ˆê³¼ ì‹œ ì§€ëŠ¥ì  ìµœì í™”
        if token_count > self.max_tokens:
            optimized_prompt = self._intelligent_truncation(clean_prompt, tokenizer)
            token_count = self._calculate_token_count(optimized_prompt, tokenizer)
            clean_prompt = optimized_prompt
        
        # 6. ì²­í‚¹ ì²˜ë¦¬
        chunks = self._create_chunks(clean_prompt, tokenizer)
        
        return WeightedPrompt(
            original=prompt,
            processed=clean_prompt,
            weights=weights,
            token_count=token_count,
            chunks=chunks
        )
    
    def _intelligent_truncation(self, prompt: str, tokenizer=None) -> str:
        """ì§€ëŠ¥ì  í”„ë¡¬í”„íŠ¸ ìë¥´ê¸° (ì¤‘ìš”ë„ ê¸°ë°˜)"""
        if not prompt.strip():
            return prompt
        
        # 1. í‚¤ì›Œë“œë³„ë¡œ ë¶„í• 
        keywords = [kw.strip() for kw in prompt.split(',')]
        
        # 2. ê° í‚¤ì›Œë“œì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        keyword_scores = []
        for keyword in keywords:
            score = 0
            for pattern, importance in self.importance_scores.items():
                if pattern.lower() in keyword.lower():
                    score = max(score, importance)
            keyword_scores.append((keyword, score))
        
        # 3. ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        keyword_scores.sort(key=lambda x: x[1], reverse=True)
        
        # 4. í† í° ì œí•œ ë‚´ì—ì„œ ìµœëŒ€í•œ ë§ì€ ì¤‘ìš” í‚¤ì›Œë“œ í¬í•¨
        selected_keywords = []
        current_tokens = 0
        
        for keyword, score in keyword_scores:
            keyword_tokens = self._calculate_token_count(keyword, tokenizer)
            if current_tokens + keyword_tokens <= self.max_tokens:
                selected_keywords.append(keyword)
                current_tokens += keyword_tokens
            else:
                break
        
        # 5. ì„ íƒëœ í‚¤ì›Œë“œë¡œ í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±
        optimized_prompt = ', '.join(selected_keywords)
        
        print(f"ğŸ”§ ì§€ëŠ¥ì  ìµœì í™”: {len(keywords)}ê°œ í‚¤ì›Œë“œ â†’ {len(selected_keywords)}ê°œ í‚¤ì›Œë“œ")
        print(f"   ì œê±°ëœ í‚¤ì›Œë“œ: {[kw for kw, _ in keyword_scores if kw not in selected_keywords]}")
        
        return optimized_prompt
    
    def _create_chunks(self, prompt: str, tokenizer=None) -> List[PromptChunk]:
        """í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ë¯¸ì  ì²­í¬ë¡œ ë¶„í• """
        if not prompt.strip():
            return []
        
        chunks = []
        keywords = [kw.strip() for kw in prompt.split(',')]
        
        # ì²­í¬ íƒ€ì… ë¶„ë¥˜
        main_keywords = []
        style_keywords = []
        quality_keywords = []
        detail_keywords = []
        
        for keyword in keywords:
            if any(pattern in keyword.lower() for pattern in ['portrait', 'landscape', 'character', 'scene']):
                main_keywords.append(keyword)
            elif any(pattern in keyword.lower() for pattern in ['anime', 'realistic', 'cartoon', 'painting', 'art']):
                style_keywords.append(keyword)
            elif any(pattern in keyword.lower() for pattern in ['quality', 'masterpiece', 'detailed', 'sharp']):
                quality_keywords.append(keyword)
            else:
                detail_keywords.append(keyword)
        
        # ì²­í¬ ìƒì„±
        if main_keywords:
            chunks.append(PromptChunk(
                text=', '.join(main_keywords),
                token_count=self._calculate_token_count(', '.join(main_keywords), tokenizer),
                importance_score=10,
                chunk_type='main'
            ))
        
        if style_keywords:
            chunks.append(PromptChunk(
                text=', '.join(style_keywords),
                token_count=self._calculate_token_count(', '.join(style_keywords), tokenizer),
                importance_score=8,
                chunk_type='style'
            ))
        
        if quality_keywords:
            chunks.append(PromptChunk(
                text=', '.join(quality_keywords),
                token_count=self._calculate_token_count(', '.join(quality_keywords), tokenizer),
                importance_score=6,
                chunk_type='quality'
            ))
        
        if detail_keywords:
            chunks.append(PromptChunk(
                text=', '.join(detail_keywords),
                token_count=self._calculate_token_count(', '.join(detail_keywords), tokenizer),
                importance_score=4,
                chunk_type='detail'
            ))
        
        return chunks
    
    def analyze_prompt(self, prompt: str, tokenizer=None) -> PromptAnalysis:
        """ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ë¶„ì„ ë° ìµœì í™” ì œì•ˆ"""
        processed = self.process_prompt(prompt, tokenizer)
        segments = self._split_by_break(processed.processed)
        
        suggestions = []
        is_optimized = True
        truncated_parts = []
        
        # í† í° ìˆ˜ ê¸°ë°˜ ì œì•ˆ
        if processed.token_count > self.max_tokens:
            suggestions.append(f"âš ï¸ í† í° ìˆ˜({processed.token_count})ê°€ ì œí•œ({self.max_tokens})ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤")
            suggestions.append("ğŸ’¡ ì§€ëŠ¥ì  ìµœì í™”ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤")
            suggestions.append("ğŸ’¡ BREAK í‚¤ì›Œë“œë¡œ ì˜ë¯¸ì  ê·¸ë£¹ì„ ë¶„ë¦¬í•˜ì„¸ìš”")
            is_optimized = False
        elif processed.token_count > self.warning_threshold:
            suggestions.append(f"âš ï¸ í† í° ìˆ˜({processed.token_count})ê°€ ê²½ê³„ì„ ì— ìˆìŠµë‹ˆë‹¤")
            suggestions.append("ğŸ’¡ BREAK í‚¤ì›Œë“œ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”")
        
        # ì²­í¬ ë¶„ì„
        if processed.chunks:
            suggestions.append(f"âœ… {len(processed.chunks)}ê°œì˜ ì˜ë¯¸ì  ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤")
            for chunk in processed.chunks:
                suggestions.append(f"   - {chunk.chunk_type}: {chunk.text[:30]}... ({chunk.token_count} í† í°)")
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
        if len(segments) > 1:
            suggestions.append("âœ… BREAK í‚¤ì›Œë“œë¡œ ì˜ êµ¬ì¡°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        else:
            suggestions.append("ğŸ’¡ BREAK í‚¤ì›Œë“œë¡œ ì˜ë¯¸ì  ê·¸ë£¹ì„ ë¶„ë¦¬í•˜ë©´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        # ê°€ì¤‘ì¹˜ ë¶„ì„
        if processed.weights:
            suggestions.append("âœ… ê°€ì¤‘ì¹˜ êµ¬ë¬¸ì´ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            for keyword, weight in processed.weights.items():
                if weight > 1.5:
                    suggestions.append(f"âš ï¸ '{keyword}'ì˜ ê°€ì¤‘ì¹˜({weight})ê°€ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤ (1.5 ì´í•˜ ê¶Œì¥)")
                elif weight < 0.5:
                    suggestions.append(f"âš ï¸ '{keyword}'ì˜ ê°€ì¤‘ì¹˜({weight})ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤ (0.5 ì´ìƒ ê¶Œì¥)")
        
        return PromptAnalysis(
            token_count=processed.token_count,
            segments=segments,
            weights=processed.weights,
            suggestions=suggestions,
            is_optimized=is_optimized,
            truncated_parts=truncated_parts
        )
    
    def _split_by_break(self, prompt: str) -> List[str]:
        """BREAK í‚¤ì›Œë“œë¡œ í”„ë¡¬í”„íŠ¸ ë¶„í• """
        if not prompt.strip():
            return []
        
        # BREAK í‚¤ì›Œë“œë¡œ ë¶„í•  (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
        segments = re.split(r'\bBREAK\b', prompt, flags=re.IGNORECASE)
        
        # ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸ ì œê±° ë° ê³µë°± ì •ë¦¬
        segments = [seg.strip() for seg in segments if seg.strip()]
        
        return segments
    
    def _extract_weights(self, prompt: str) -> Dict[str, float]:
        """ê°€ì¤‘ì¹˜ êµ¬ë¬¸ ì¶”ì¶œ"""
        weights = {}
        
        # ì–‘ì˜ ê°€ì¤‘ì¹˜: (í‚¤ì›Œë“œ:1.2) ë˜ëŠ” (í‚¤ì›Œë“œ)
        for match in re.finditer(self.weight_pattern, prompt):
            keyword = match.group(1).strip()
            weight_str = match.group(2)
            
            if weight_str:
                try:
                    weight = float(weight_str)
                except ValueError:
                    weight = 1.1  # ê¸°ë³¸ê°’
            else:
                weight = 1.1  # (í‚¤ì›Œë“œ)ëŠ” 1.1ê³¼ ë™ì¼
            
            weights[keyword] = weight
        
        # ìŒì˜ ê°€ì¤‘ì¹˜: [í‚¤ì›Œë“œ] (0.9ì™€ ë™ì¼)
        for match in re.finditer(self.negative_weight_pattern, prompt):
            keyword = match.group(1).strip()
            weights[keyword] = 0.9
        
        return weights
    
    def _remove_weight_syntax(self, prompt: str) -> str:
        """ê°€ì¤‘ì¹˜ êµ¬ë¬¸ ì œê±°"""
        # ì–‘ì˜ ê°€ì¤‘ì¹˜ ì œê±°
        clean_prompt = re.sub(self.weight_pattern, r'\1', prompt)
        
        # ìŒì˜ ê°€ì¤‘ì¹˜ ì œê±°
        clean_prompt = re.sub(self.negative_weight_pattern, r'\1', clean_prompt)
        
        return clean_prompt.strip()
    
    def _calculate_token_count(self, prompt: str, tokenizer=None) -> int:
        """ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°"""
        if not prompt.strip():
            return 0
        
        if tokenizer:
            try:
                # ì‹¤ì œ í† í¬ë‚˜ì´ì € ì‚¬ìš©
                text_inputs = tokenizer(
                    prompt,
                    padding="longest",
                    truncation=False,
                    return_tensors="pt",
                )
                input_ids = text_inputs.input_ids[0]
                return len(input_ids)
            except Exception as e:
                print(f"âš ï¸ í† í¬ë‚˜ì´ì € ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ ê°„ë‹¨í•œ ì¶”ì •ìœ¼ë¡œ í´ë°±
        
        # ê°„ë‹¨í•œ ì¶”ì • (ê³µë°±ìœ¼ë¡œ ë¶„í• )
        return len(prompt.split())
    
    def get_custom_tokenizer(self, state_manager=None):
        """ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°"""
        if state_manager and hasattr(state_manager, 'tokenizer_manager'):
            return state_manager.tokenizer_manager.get_current_tokenizer()
        return None
    
    def add_break_keyword(self, prompt: str, position: Optional[int] = None) -> str:
        """BREAK í‚¤ì›Œë“œ ì¶”ê°€"""
        if position is None:
            # ë¬¸ì¥ ëì— ì¶”ê°€
            return f"{prompt} {self.break_keyword}"
        else:
            # ì§€ì •ëœ ìœ„ì¹˜ì— ì¶”ê°€
            return prompt[:position] + f" {self.break_keyword} " + prompt[position:]
    
    def apply_weight(self, keyword: str, weight: float = 1.1) -> str:
        """ê°€ì¤‘ì¹˜ êµ¬ë¬¸ ìƒì„±"""
        if weight == 1.0:
            return keyword
        elif weight < 1.0:
            return f"[{keyword}]"  # ìŒì˜ ê°€ì¤‘ì¹˜ëŠ” ëŒ€ê´„í˜¸ ì‚¬ìš©
        else:
            return f"({keyword}:{weight})"
    
    def create_prompt_embeddings(self, prompt: str, tokenizer=None, text_encoder=None) -> Optional[torch.Tensor]:
        """í”„ë¡¬í”„íŠ¸ ì„ë² ë”© ì§ì ‘ ìƒì„±"""
        if not tokenizer or not text_encoder:
            return None
        
        try:
            # í† í°í™”
            text_inputs = tokenizer(
                prompt,
                padding="max_length",
                max_length=self.max_tokens,
                truncation=True,
                return_tensors="pt",
            )
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                prompt_embeds = text_encoder(text_inputs.input_ids)[0]
            
            return prompt_embeds
            
        except Exception as e:
            print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def optimize_prompt(self, prompt: str, target_tokens: int = 70) -> str:
        """í”„ë¡¬í”„íŠ¸ë¥¼ í† í° ì œí•œ ë‚´ì—ì„œ ìµœì í™”"""
        if not prompt.strip():
            return prompt
        
        # 1. ê¸°ë³¸ ì •ë¦¬
        optimized = self._clean_prompt(prompt)
        
        # 2. í† í° ìˆ˜ í™•ì¸
        token_count = self._calculate_token_count(optimized)
        
        if token_count <= target_tokens:
            return optimized
        
        # 3. í† í° ì œí•œ ì´ˆê³¼ ì‹œ ì§€ëŠ¥ì  ìµœì í™”
        print(f"ğŸ”§ í”„ë¡¬í”„íŠ¸ ìµœì í™”: {token_count} â†’ {target_tokens} í† í°")
        
        # 3-1. ë¶ˆí•„ìš”í•œ ì‰¼í‘œì™€ ê³µë°± ì œê±°
        optimized = self._remove_redundant_commas(optimized)
        
        # 3-2. ì¤‘ë³µ í‚¤ì›Œë“œ ì œê±°
        optimized = self._remove_duplicate_keywords(optimized)
        
        # 3-3. í† í° ìˆ˜ ì¬í™•ì¸
        token_count = self._calculate_token_count(optimized)
        
        if token_count <= target_tokens:
            return optimized
        
        # 3-4. ì§€ëŠ¥ì  ìë¥´ê¸° ì ìš©
        optimized = self._intelligent_truncation(optimized, None)
        
        # 3-5. ìµœì¢… í† í° ìˆ˜ í™•ì¸
        final_token_count = self._calculate_token_count(optimized)
        print(f"âœ… ìµœì í™” ì™„ë£Œ: {final_token_count} í† í°")
        
        return optimized
    
    def _clean_prompt(self, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ê¸°ë³¸ ì •ë¦¬"""
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        cleaned = ' '.join(prompt.split())
        # ì‰¼í‘œ ë’¤ ê³µë°± ì •ë¦¬
        cleaned = cleaned.replace(', ', ',')
        cleaned = cleaned.replace(',', ', ')
        # ë ì‰¼í‘œ ì œê±°
        cleaned = cleaned.rstrip(', ')
        return cleaned
    
    def _remove_redundant_commas(self, prompt: str) -> str:
        """ë¶ˆí•„ìš”í•œ ì‰¼í‘œ ì œê±°"""
        # ì—°ì†ëœ ì‰¼í‘œ ì œê±°
        cleaned = re.sub(r',\s*,+', ',', prompt)
        # ì‰¼í‘œ-ê³µë°±-ì‰¼í‘œ íŒ¨í„´ ì •ë¦¬
        cleaned = re.sub(r',\s+,', ',', cleaned)
        return cleaned
    
    def _remove_duplicate_keywords(self, prompt: str) -> str:
        """ì¤‘ë³µ í‚¤ì›Œë“œ ì œê±°"""
        keywords = [kw.strip() for kw in prompt.split(',')]
        unique_keywords = []
        seen = set()
        
        for keyword in keywords:
            if keyword.lower() not in seen:
                unique_keywords.append(keyword)
                seen.add(keyword.lower())
        
        return ', '.join(unique_keywords)
    
    def _remove_less_important_keywords(self, prompt: str, target_tokens: int) -> str:
        """ëœ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì œê±°"""
        keywords = [kw.strip() for kw in prompt.split(',')]
        
        # í‚¤ì›Œë“œë³„ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        keyword_scores = []
        for keyword in keywords:
            score = 0
            for pattern, importance in self.importance_scores.items():
                if pattern.lower() in keyword.lower():
                    score = max(score, importance)
            keyword_scores.append((keyword, score))
        
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        keyword_scores.sort(key=lambda x: x[1], reverse=True)
        
        # í† í° ì œí•œ ë‚´ì—ì„œ ìµœëŒ€í•œ ë§ì€ ì¤‘ìš” í‚¤ì›Œë“œ í¬í•¨
        selected_keywords = []
        current_tokens = 0
        
        for keyword, score in keyword_scores:
            keyword_tokens = len(keyword.split())  # ê°„ë‹¨í•œ í† í° ì¶”ì •
            if current_tokens + keyword_tokens <= target_tokens:
                selected_keywords.append(keyword)
                current_tokens += keyword_tokens
            else:
                break
        
        return ', '.join(selected_keywords) 