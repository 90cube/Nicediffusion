from ....core.logger import (
    debug, info, warning, error, success, failure, warning_emoji, 
    info_emoji, debug_emoji, process_emoji, model_emoji, image_emoji, ui_emoji
)
"""
ê³ ê¸‰ í…ìŠ¤íŠ¸ ì¸ì½”ë” ì„œë¹„ìŠ¤
ComfyUI ìŠ¤íƒ€ì¼ ê³ ê¸‰ ì¸ì½”ë”©ìœ¼ë¡œ 77í† í° ì œí•œ ì™„ì „ í•´ì œ
"""

import torch
import numpy as np
import itertools
import re
from typing import List, Tuple, Dict, Any, Optional
from math import gcd


class AdvancedTextEncoder:
    """ComfyUI ìŠ¤íƒ€ì¼ ê³ ê¸‰ í…ìŠ¤íŠ¸ ì¸ì½”ë”©"""
    
    def __init__(self, pipeline, weight_mode="A1111", use_custom_tokenizer=True):
        self.pipeline = pipeline
        self.weight_mode = weight_mode  # "A1111" ë˜ëŠ” "comfy++"
        self.use_custom_tokenizer = use_custom_tokenizer
        self.tokenizer = pipeline.tokenizer
        self.text_encoder = pipeline.text_encoder
        
    def parse_prompt_weights(self, text: str) -> List[Tuple[str, float, int]]:
        """A1111 ìŠ¤íƒ€ì¼ ê°€ì¤‘ì¹˜ íŒŒì‹±: (word:1.2), ((word)), [word]"""
        
        # ê°€ì¤‘ì¹˜ íŒ¨í„´ ì •ì˜
        patterns = [
            (r'\(\((.*?)\)\)', 1.21),  # ((word)) = 1.21
            (r'\((.*?):([0-9\.]+)\)', None),  # (word:1.2)
            (r'\((.*?)\)', 1.1),  # (word) = 1.1
            (r'\[(.*?)\]', 0.909),  # [word] = 1/1.1
        ]
        
        tokens = []
        word_id = 1
        
        remaining_text = text
        while remaining_text:
            matched = False
            
            # íŒ¨í„´ ë§¤ì¹­ ì‹œë„
            for pattern, default_weight in patterns:
                match = re.search(pattern, remaining_text)
                if match:
                    # ë§¤ì¹­ ì´ì „ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                    before = remaining_text[:match.start()].strip()
                    if before:
                        for word in before.split():
                            tokens.append((word, 1.0, word_id))
                            word_id += 1
                    
                    # ê°€ì¤‘ì¹˜ ì ìš©ëœ ë¶€ë¶„ ì²˜ë¦¬
                    if default_weight is None:  # (word:1.2) í˜•íƒœ
                        word, weight = match.groups()
                        weight = float(weight)
                    else:
                        word = match.group(1)
                        weight = default_weight
                    
                    for w in word.strip().split():
                        tokens.append((w, weight, word_id))
                        word_id += 1
                    
                    # ë§¤ì¹­ëœ ë¶€ë¶„ ì œê±°
                    remaining_text = remaining_text[match.end():].strip()
                    matched = True
                    break
            
            if not matched:
                # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ë‹¨ì–´ë¡œ ì²˜ë¦¬
                words = remaining_text.split()
                if words:
                    for word in words:
                        tokens.append((word, 1.0, word_id))
                        word_id += 1
                break
        
        return tokens
    
    def tokenize_with_weights(self, text: str) -> Dict[str, List]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ê°€ì¤‘ì¹˜ ì •ë³´ í¬í•¨"""
        
        if not self.use_custom_tokenizer:
            # ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            tokens = self.tokenizer.encode(text, add_special_tokens=True)
            weights = [1.0] * len(tokens)
            word_ids = list(range(len(tokens)))
            return {'tokens': [tokens], 'weights': [weights], 'word_ids': [word_ids]}
        
        # ê³ ê¸‰ ê°€ì¤‘ì¹˜ íŒŒì‹±
        parsed = self.parse_prompt_weights(text)
        
        tokens = []
        weights = []
        word_ids = []
        
        for word, weight, word_id in parsed:
            # ë‹¨ì–´ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
            word_tokens = self.tokenizer.encode(word, add_special_tokens=False)
            
            tokens.extend(word_tokens)
            weights.extend([weight] * len(word_tokens))
            word_ids.extend([word_id] * len(word_tokens))
        
        # íŠ¹ìˆ˜ í† í° ì¶”ê°€
        tokens = [self.tokenizer.bos_token_id] + tokens + [self.tokenizer.eos_token_id]
        weights = [1.0] + weights + [1.0]
        word_ids = [0] + word_ids + [0]
        
        return {'tokens': [tokens], 'weights': [weights], 'word_ids': [word_ids]}
    
    def _grouper(self, n, iterable):
        """ì²­í‚¹ í—¬í¼"""
        it = iter(iterable)
        while True:
            chunk = list(itertools.islice(it, n))
            if not chunk:
                return
            yield chunk
    
    def _norm_mag(self, w, n):
        """ê°€ì¤‘ì¹˜ ì •ê·œí™”"""
        d = w - 1
        return 1 + np.sign(d) * np.sqrt(np.abs(d)**2 / n)
    
    def divide_length(self, word_ids, weights):
        """ë‹¨ì–´ ê¸¸ì´ë³„ ê°€ì¤‘ì¹˜ ë¶„ë°°"""
        sums = dict(zip(*np.unique(word_ids, return_counts=True)))
        sums[0] = 1
        weights = [[self._norm_mag(w, sums[id]) if id != 0 else 1.0
                    for w, id in zip(x, y)] for x, y in zip(weights, word_ids)]
        return weights
    
    def from_zero(self, weights, base_emb):
        """ê°€ì¤‘ì¹˜ë¥¼ ì„ë² ë”©ì— ì ìš© (A1111 ë°©ì‹)"""
        weight_tensor = torch.tensor(weights, dtype=base_emb.dtype, device=base_emb.device)
        weight_tensor = weight_tensor.reshape(1, -1, 1).expand(base_emb.shape)
        return base_emb * weight_tensor
    
    def encode_prompt(self, prompt: str, negative_prompt: str = "") -> Tuple[torch.Tensor, torch.Tensor]:
        """í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”© (ë©”ì¸ í•¨ìˆ˜)"""
        
        # ê¸ì • í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
        pos_tokenized = self.tokenize_with_weights(prompt)
        pos_embeddings = self._encode_tokens(pos_tokenized)
        
        # ë¶€ì • í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
        if negative_prompt:
            neg_tokenized = self.tokenize_with_weights(negative_prompt)
            neg_embeddings = self._encode_tokens(neg_tokenized)
        else:
            # ë¹ˆ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©
            empty_tokens = self.tokenizer.encode("", add_special_tokens=True)
            empty_input = torch.tensor([empty_tokens], device=self.text_encoder.device)
            with torch.no_grad():
                neg_embeddings = self.text_encoder(empty_input)[0]
        
        return pos_embeddings, neg_embeddings
    
    def encode_prompt_with_pooled(self, prompt: str, negative_prompt: str = "") -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
        """SDXLìš© í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”© (pooled_prompt_embeds í¬í•¨)"""
        
        # SDXL ëª¨ë¸ì¸ì§€ í™•ì¸
        if not hasattr(self.pipeline, 'text_encoder_2'):
            # SD15 ëª¨ë¸ì¸ ê²½ìš° ê¸°ë³¸ ì¸ì½”ë”© ì‚¬ìš©
            pos_embeds, neg_embeds = self.encode_prompt(prompt, negative_prompt)
            return pos_embeds, neg_embeds, None, None
        
        # SDXL ëª¨ë¸ì¸ ê²½ìš° ë‘ ê°œì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë” ì‚¬ìš©
        info(r"ğŸ“ SDXL ëª¨ë¸ ê°ì§€ - ë‘ ê°œì˜ í…ìŠ¤íŠ¸ ì¸ì½”ë” ì‚¬ìš©")
        
        # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë” (OpenCLIP) - ê¸°ë³¸ ì„ë² ë”©
        pos_embeds_1, neg_embeds_1 = self.encode_prompt(prompt, negative_prompt)
        
        # ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë” (CLIP) - pooled_prompt_embeds ìƒì„±
        text_encoder_2 = self.pipeline.text_encoder_2
        tokenizer_2 = self.pipeline.tokenizer_2
        
        # ê¸ì • í”„ë¡¬í”„íŠ¸ ë‘ ë²ˆì§¸ ì¸ì½”ë” ì„ë² ë”©
        pos_tokens_2 = tokenizer_2(
            prompt,
            padding="max_length",
            max_length=tokenizer_2.model_max_length,
            truncation=True,
            return_tensors="pt"
        ).input_ids.to(text_encoder_2.device)
        
        with torch.no_grad():
            pos_output_2 = text_encoder_2(pos_tokens_2, output_hidden_states=True)
            pos_embeds_2 = pos_output_2.hidden_states[-2]  # ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ ë ˆì´ì–´
            pos_pooled = pos_output_2[0]  # pooled output
        
        # ë¶€ì • í”„ë¡¬í”„íŠ¸ ë‘ ë²ˆì§¸ ì¸ì½”ë” ì„ë² ë”©
        if negative_prompt:
            neg_tokens_2 = tokenizer_2(
                negative_prompt,
                padding="max_length",
                max_length=tokenizer_2.model_max_length,
                truncation=True,
                return_tensors="pt"
            ).input_ids.to(text_encoder_2.device)
        else:
            neg_tokens_2 = tokenizer_2(
                "",
                padding="max_length",
                max_length=tokenizer_2.model_max_length,
                truncation=True,
                return_tensors="pt"
            ).input_ids.to(text_encoder_2.device)
        
        with torch.no_grad():
            neg_output_2 = text_encoder_2(neg_tokens_2, output_hidden_states=True)
            neg_embeds_2 = neg_output_2.hidden_states[-2]  # ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ ë ˆì´ì–´
            neg_pooled = neg_output_2[0]  # pooled output
        
        # SDXLì—ì„œëŠ” ë‘ ì¸ì½”ë”ì˜ ì„ë² ë”©ì„ ì—°ê²°í•´ì•¼ í•¨
        # ì²« ë²ˆì§¸ ì¸ì½”ë”: 768 ì°¨ì›, ë‘ ë²ˆì§¸ ì¸ì½”ë”: 1280 ì°¨ì›
        # ì—°ê²°í•˜ë©´ 2048 ì°¨ì›ì´ ë¨
        pos_embeds = torch.cat([pos_embeds_1, pos_embeds_2], dim=-1)
        neg_embeds = torch.cat([neg_embeds_1, neg_embeds_2], dim=-1)
        
        success(r"SDXL ì„ë² ë”© ì—°ê²° ì™„ë£Œ:")
        info(f"   - ì²« ë²ˆì§¸ ì¸ì½”ë”: {pos_embeds_1.shape}")
        info(f"   - ë‘ ë²ˆì§¸ ì¸ì½”ë”: {pos_embeds_2.shape}")
        info(f"   - ì—°ê²° ê²°ê³¼: {pos_embeds.shape}")
        
        return pos_embeds, neg_embeds, pos_pooled, neg_pooled
    
    def _encode_tokens(self, tokenized_data: Dict[str, List]) -> torch.Tensor:
        """í† í°ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        
        tokens = tokenized_data['tokens'][0]
        weights = tokenized_data['weights'][0]
        word_ids = tokenized_data['word_ids'][0]
        
        # SDXL ëª¨ë¸ì¸ì§€ í™•ì¸í•˜ì—¬ í† í° ì œí•œ ê²°ì •
        is_sdxl = hasattr(self.pipeline, 'text_encoder_2')
        max_length = 77  # SDXLê³¼ SD15 ëª¨ë‘ ì²« ë²ˆì§¸ ì¸ì½”ë”ëŠ” 77í† í° ì œí•œ
        
        # í† í° ê¸¸ì´ë¥¼ ì •í™•íˆ 77ë¡œ ì œí•œ (SDXL ëª¨ë¸ì—ì„œ ì°¨ì› ë¶ˆì¼ì¹˜ ë°©ì§€)
        if len(tokens) > max_length:
            tokens = tokens[:max_length]
            weights = weights[:max_length]
            word_ids = word_ids[:max_length]
            warning_emoji(f"í† í° ê¸¸ì´ë¥¼ {max_length}ë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤ (ì›ë³¸: {len(tokenized_data['tokens'][0])})")
        
        # íŒ¨ë”©
        while len(tokens) < max_length:
            tokens.append(self.tokenizer.pad_token_id or 0)
            weights.append(1.0)
            word_ids.append(0)
        
        # ê¸°ë³¸ ì„ë² ë”© ìƒì„±
        token_tensor = torch.tensor([tokens], device=self.text_encoder.device)
        with torch.no_grad():
            base_embeddings = self.text_encoder(token_tensor)[0]
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        if self.weight_mode == "A1111":
            return self._apply_a1111_weights(base_embeddings, weights, word_ids)
        elif self.weight_mode == "comfy++":
            return self._apply_comfy_weights(base_embeddings, weights, word_ids)
        else:
            return base_embeddings
    
    def _encode_long_prompt(self, tokens: List[int], weights: List[float], word_ids: List[int], chunk_size: int) -> torch.Tensor:
        """ê¸´ í”„ë¡¬í”„íŠ¸ ì²­í‚¹ ì²˜ë¦¬"""
        
        chunks = []
        for i in range(0, len(tokens), chunk_size-2):  # íŠ¹ìˆ˜ í† í° ê³µê°„ í™•ë³´
            chunk_tokens = tokens[i:i+chunk_size-2]
            chunk_weights = weights[i:i+chunk_size-2]
            chunk_word_ids = word_ids[i:i+chunk_size-2]
            
            # íŠ¹ìˆ˜ í† í° ì¶”ê°€
            full_tokens = [self.tokenizer.bos_token_id] + chunk_tokens + [self.tokenizer.eos_token_id]
            full_weights = [1.0] + chunk_weights + [1.0]
            full_word_ids = [0] + chunk_word_ids + [0]
            
            # íŒ¨ë”©
            while len(full_tokens) < chunk_size:
                full_tokens.append(self.tokenizer.pad_token_id or 0)
                full_weights.append(1.0)
                full_word_ids.append(0)
            
            # ì¸ì½”ë”©
            token_tensor = torch.tensor([full_tokens], device=self.text_encoder.device)
            with torch.no_grad():
                chunk_emb = self.text_encoder(token_tensor)[0]
            
            # ê°€ì¤‘ì¹˜ ì ìš©
            if self.weight_mode == "A1111":
                chunk_emb = self._apply_a1111_weights(chunk_emb, full_weights, full_word_ids)
            elif self.weight_mode == "comfy++":
                chunk_emb = self._apply_comfy_weights(chunk_emb, full_weights, full_word_ids)
            
            chunks.append(chunk_emb)
        
        # ì²­í¬ ë³‘í•© (í‰ê· )
        if len(chunks) == 1:
            return chunks[0]
        else:
            return torch.mean(torch.stack(chunks), dim=0)
    
    def _apply_a1111_weights(self, embeddings: torch.Tensor, weights: List[float], word_ids: List[int]) -> torch.Tensor:
        """A1111 ìŠ¤íƒ€ì¼ ê°€ì¤‘ì¹˜ ì ìš©"""
        weighted_emb = self.from_zero([weights], embeddings)
        
        # A1111 ì •ê·œí™”
        norm_base = torch.linalg.norm(embeddings)
        norm_weighted = torch.linalg.norm(weighted_emb)
        
        if norm_weighted > 0:
            embeddings_final = (norm_base / norm_weighted) * weighted_emb
        else:
            embeddings_final = weighted_emb
        
        return embeddings_final
    
    def _apply_comfy_weights(self, embeddings: torch.Tensor, weights: List[float], word_ids: List[int]) -> torch.Tensor:
        """ComfyUI++ ìŠ¤íƒ€ì¼ ê°€ì¤‘ì¹˜ ì ìš© (ê³ ê¸‰)"""
        
        # ê¸¸ì´ë³„ ê°€ì¤‘ì¹˜ ë¶„ë°°
        weights_normalized = self.divide_length([word_ids], [weights])[0]
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        weighted_emb = self.from_zero([weights_normalized], embeddings)
        
        # ë¶„í¬ ë³µì›
        fixed_std = (embeddings.std() / weighted_emb.std()) * (weighted_emb - weighted_emb.mean())
        embeddings_final = fixed_std + (embeddings.mean() - fixed_std.mean())
        
        return embeddings_final 