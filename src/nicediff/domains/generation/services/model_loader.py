"""
ëª¨ë¸ ë¡œë”© ë„ë©”ì¸ ì„œë¹„ìŠ¤
UIë‚˜ StateManagerì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""

import asyncio
from typing import Dict, Any, Optional, Union

import torch
from diffusers import AutoencoderKL
from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipeline
from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline


class ModelLoader:
    """ëª¨ë¸ ë¡œë”© ì„œë¹„ìŠ¤"""
    
    def __init__(self, device: str = "cuda"):
        self.device = device
        self.current_pipeline: Optional[Union[StableDiffusionPipeline, StableDiffusionXLPipeline]] = None
    
    async def load_model(self, model_info: Dict[str, Any]) -> Union[StableDiffusionPipeline, StableDiffusionXLPipeline]:
        """ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìµœì í™” ì„¤ì •ì„ ì ìš©"""
        
        def _load():
            model_path = model_info['path']
            model_type = model_info.get('model_type', 'SD15')
            
            print(f"ğŸ” ëª¨ë¸ íƒ€ì…: {model_type}, ê²½ë¡œ: {model_path}")
            
            if model_type == 'SDXL':
                pipeline = StableDiffusionXLPipeline.from_single_file(
                    model_path,
                    torch_dtype=torch.float16,
                    use_safetensors=True
                )
            else:
                pipeline = StableDiffusionPipeline.from_single_file(
                    model_path,
                    torch_dtype=torch.float16,
                    use_safetensors=True
                )
            
            # GPUë¡œ ì´ë™
            pipeline = pipeline.to(self.device)
            
            # ìµœì í™” ì„¤ì • ì ìš©
            self._apply_optimizations(pipeline, model_type)
            
            return pipeline
        
        self.current_pipeline = await asyncio.to_thread(_load)
        return self.current_pipeline
    
    def _apply_optimizations(self, pipeline: Union[StableDiffusionPipeline, StableDiffusionXLPipeline], model_type: str):
        """ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©"""
        
        # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ì„¤ì •
        if hasattr(pipeline, 'enable_attention_slicing'):
            pipeline.enable_attention_slicing(1)
        if hasattr(pipeline, 'enable_vae_slicing'):
            pipeline.enable_vae_slicing()
        
        # PyTorch 2.0+ SDPA ìµœì í™” (xformers ëŒ€ì‹  ì‚¬ìš©)
        if hasattr(pipeline, 'enable_model_cpu_offload'):
            pipeline.enable_model_cpu_offload()
        
        # xformersëŠ” ì„ íƒì  ê¸°ëŠ¥ì´ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            if hasattr(pipeline, 'enable_xformers_memory_efficient_attention'):
                pipeline.enable_xformers_memory_efficient_attention()
                print("âœ… xformers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ í™œì„±í™”")
        except (ModuleNotFoundError, AttributeError) as e:
            print(f"â„¹ï¸ xformers ë¯¸ì‚¬ìš©: {e}")
        
        # SD15 ì „ìš© ìµœì í™”
        if model_type == 'SD15':
            self._apply_sd15_optimizations(pipeline)
    
    def _apply_sd15_optimizations(self, pipeline: StableDiffusionPipeline):
        """SD15 ëª¨ë¸ ì „ìš© ìµœì í™”"""
        # Text Encoderë¥¼ float16ìœ¼ë¡œ ë³€í™˜
        if hasattr(pipeline, 'text_encoder') and pipeline.text_encoder is not None:
            pipeline.text_encoder = pipeline.text_encoder.to(torch.float16)
        
        # VAEë¥¼ float16ìœ¼ë¡œ ë³€í™˜
        if hasattr(pipeline, 'vae') and pipeline.vae is not None:
            pipeline.vae = pipeline.vae.to(torch.float16)
        
        # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        if hasattr(pipeline.scheduler, 'config'):
            pipeline.scheduler.config.use_karras_sigmas = True
            pipeline.scheduler.config.karras_rho = 7.0
        
        print("ğŸ”§ SD15 ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")
    
    async def load_vae(self, vae_path: str) -> bool:
        """VAE ë¡œë“œ"""
        if not self.current_pipeline:
            return False
        
        def _load_vae():
            vae = AutoencoderKL.from_single_file(
                vae_path,
                torch_dtype=torch.float16
            )
            self.current_pipeline.vae = vae.to(self.device)
            return True
        
        return await asyncio.to_thread(_load_vae)
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.current_pipeline:
            # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
            del self.current_pipeline
            self.current_pipeline = None
            
            # CUDA ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print("âœ… ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
    
    def get_current_pipeline(self) -> Optional[Union[StableDiffusionPipeline, StableDiffusionXLPipeline]]:
        """í˜„ì¬ ë¡œë“œëœ íŒŒì´í”„ë¼ì¸ ë°˜í™˜"""
        return self.current_pipeline 