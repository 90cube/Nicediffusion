"""
ëª¨ë¸ ë¡œë”© ë„ë©”ì¸ ì„œë¹„ìŠ¤
UIë‚˜ StateManagerì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ìˆœìˆ˜í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""

import asyncio
from typing import Dict, Any, Optional, Union, List
from pathlib import Path

import torch
from diffusers import AutoencoderKL
from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipeline
from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline


class ModelLoader:
    """ëª¨ë¸ ë¡œë”© ì„œë¹„ìŠ¤"""
    
    def __init__(self, device: str = "cuda"):
        self.device = device
        self.current_pipeline: Optional[Union[StableDiffusionPipeline, StableDiffusionXLPipeline]] = None
        self.loaded_loras: List[Dict[str, Any]] = []  # ë¡œë“œëœ LoRA ëª©ë¡
    
    async def load_model(self, model_info: Dict[str, Any]) -> Union[StableDiffusionPipeline, StableDiffusionXLPipeline]:
        """ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìµœì í™” ì„¤ì •ì„ ì ìš©"""
        
        def _load():
            model_path = model_info['path']
            model_type = model_info.get('model_type', 'SD15')
            
            print(f"ğŸ” ëª¨ë¸ íƒ€ì…: {model_type}, ê²½ë¡œ: {model_path}")
            
            if model_type == 'SDXL':
                pipeline = StableDiffusionXLPipeline.from_single_file(
                    model_path,
                    torch_dtype=torch.float16,
                    use_safetensors=True
                )
            else:
                pipeline = StableDiffusionPipeline.from_single_file(
                    model_path,
                    torch_dtype=torch.float16,
                    use_safetensors=True
                )
            
            # GPUë¡œ ì´ë™
            pipeline = pipeline.to(self.device)
            
            # ìµœì í™” ì„¤ì • ì ìš©
            self._apply_optimizations(pipeline, model_type)
            
            return pipeline
        
        self.current_pipeline = await asyncio.to_thread(_load)
        # ëª¨ë¸ ë¡œë“œ ì‹œ ê¸°ì¡´ LoRA ëª©ë¡ ì´ˆê¸°í™”
        self.loaded_loras = []
        return self.current_pipeline
    
    def _apply_optimizations(self, pipeline: Union[StableDiffusionPipeline, StableDiffusionXLPipeline], model_type: str):
        """ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©"""
        
        # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ì„¤ì •
        if hasattr(pipeline, 'enable_attention_slicing'):
            pipeline.enable_attention_slicing(1)
        if hasattr(pipeline, 'enable_vae_slicing'):
            pipeline.enable_vae_slicing()
        
        # PyTorch 2.0+ SDPA ìµœì í™” (xformers ëŒ€ì‹  ì‚¬ìš©)
        if hasattr(pipeline, 'enable_model_cpu_offload'):
            pipeline.enable_model_cpu_offload()
        
        # xformersëŠ” ì„ íƒì  ê¸°ëŠ¥ì´ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            if hasattr(pipeline, 'enable_xformers_memory_efficient_attention'):
                pipeline.enable_xformers_memory_efficient_attention()
                print("âœ… xformers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜ í™œì„±í™”")
        except (ModuleNotFoundError, AttributeError) as e:
            print(f"â„¹ï¸ xformers ë¯¸ì‚¬ìš©: {e}")
        
        # SD15 ì „ìš© ìµœì í™”
        if model_type == 'SD15':
            self._apply_sd15_optimizations(pipeline)
    
    def _apply_sd15_optimizations(self, pipeline: StableDiffusionPipeline):
        """SD15 ëª¨ë¸ ì „ìš© ìµœì í™”"""
        # Text Encoderë¥¼ float16ìœ¼ë¡œ ë³€í™˜
        if hasattr(pipeline, 'text_encoder') and pipeline.text_encoder is not None:
            pipeline.text_encoder = pipeline.text_encoder.to(torch.float16)
        
        # VAEë¥¼ float16ìœ¼ë¡œ ë³€í™˜
        if hasattr(pipeline, 'vae') and pipeline.vae is not None:
            pipeline.vae = pipeline.vae.to(torch.float16)
        
        # SD15ì—ì„œ ë” ë‚˜ì€ í’ˆì§ˆì„ ìœ„í•œ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        if hasattr(pipeline.scheduler, 'config'):
            pipeline.scheduler.config.use_karras_sigmas = True
            pipeline.scheduler.config.karras_rho = 7.0
        
        print("ğŸ”§ SD15 ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ")
    
    async def load_vae(self, vae_path: str) -> bool:
        """VAE ë¡œë“œ"""
        if not self.current_pipeline:
            return False
        
        def _load_vae():
            vae = AutoencoderKL.from_single_file(
                vae_path,
                torch_dtype=torch.float16
            )
            self.current_pipeline.vae = vae.to(self.device)
            return True
        
        return await asyncio.to_thread(_load_vae)
    
    async def load_lora(self, lora_info: Dict[str, Any], weight: float = 1.0) -> bool:
        """LoRA ë¡œë“œ"""
        if not self.current_pipeline:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            lora_path = lora_info['path']
            lora_name = Path(lora_path).stem
            
            def _load_lora():
                # LoRA ë¡œë“œ
                self.current_pipeline.load_lora_weights(
                    lora_path,
                    adapter_name=lora_name,
                    weight=weight
                )
                return True
            
            success = await asyncio.to_thread(_load_lora)
            
            if success:
                # ë¡œë“œëœ LoRA ì •ë³´ ì €ì¥
                loaded_lora = {
                    'name': lora_name,
                    'path': lora_path,
                    'weight': weight,
                    'info': lora_info
                }
                self.loaded_loras.append(loaded_lora)
                print(f"âœ… LoRA ë¡œë“œ ì™„ë£Œ: {lora_name} (weight: {weight})")
                return True
            else:
                print(f"âŒ LoRA ë¡œë“œ ì‹¤íŒ¨: {lora_name}")
                return False
                
        except Exception as e:
            print(f"âŒ LoRA ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    async def unload_lora(self, lora_name: str) -> bool:
        """íŠ¹ì • LoRA ì–¸ë¡œë“œ"""
        if not self.current_pipeline:
            return False
        
        try:
            def _unload_lora():
                # LoRA ì–¸ë¡œë“œ
                self.current_pipeline.unload_lora_weights()
                return True
            
            success = await asyncio.to_thread(_unload_lora)
            
            if success:
                # ë¡œë“œëœ LoRA ëª©ë¡ì—ì„œ ì œê±°
                self.loaded_loras = [lora for lora in self.loaded_loras if lora['name'] != lora_name]
                print(f"âœ… LoRA ì–¸ë¡œë“œ ì™„ë£Œ: {lora_name}")
                return True
            else:
                print(f"âŒ LoRA ì–¸ë¡œë“œ ì‹¤íŒ¨: {lora_name}")
                return False
                
        except Exception as e:
            print(f"âŒ LoRA ì–¸ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    async def unload_all_loras(self) -> bool:
        """ëª¨ë“  LoRA ì–¸ë¡œë“œ"""
        if not self.current_pipeline:
            return False
        
        try:
            def _unload_all_loras():
                # ëª¨ë“  LoRA ì–¸ë¡œë“œ
                self.current_pipeline.unload_lora_weights()
                return True
            
            success = await asyncio.to_thread(_unload_all_loras)
            
            if success:
                self.loaded_loras = []
                print("âœ… ëª¨ë“  LoRA ì–¸ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                print("âŒ ëª¨ë“  LoRA ì–¸ë¡œë“œ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ëª¨ë“  LoRA ì–¸ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def get_loaded_loras(self) -> List[Dict[str, Any]]:
        """ë¡œë“œëœ LoRA ëª©ë¡ ë°˜í™˜"""
        return self.loaded_loras.copy()
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.current_pipeline:
            # GPU ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
            del self.current_pipeline
            self.current_pipeline = None
            
            # LoRA ëª©ë¡ ì´ˆê¸°í™”
            self.loaded_loras = []
            
            # CUDA ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print("âœ… ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
    
    def get_current_pipeline(self) -> Optional[Union[StableDiffusionPipeline, StableDiffusionXLPipeline]]:
        """í˜„ì¬ ë¡œë“œëœ íŒŒì´í”„ë¼ì¸ ë°˜í™˜"""
        return self.current_pipeline 